@MISC{Altenkirch2020-rm,
  title     = "Big Step Normalisation for Type Theory",
  author    = "Altenkirch, Thorsten and Geniet, Colin",
  publisher = "Schloss Dagstuhl - Leibniz-Zentrum für Informatik",
  abstract  = "Big step normalisation is a normalisation method for typed
               lambda-calculi which relies on a purely syntactic recursive
               evaluator. Termination of that evaluator is proven using a
               predicate called strong computability, similar to the techniques
               used to prove strong normalisation of β-reduction for typed
               lambda-calculi. We generalise big step normalisation to a
               minimalist dependent type theory. Compared to previous
               presentations of big step normalisation for e.g. the simply-typed
               lambda-calculus, we use a quotiented syntax of type theory, which
               crucially reduces the syntactic complexity introduced by
               dependent types. Most of the proof has been formalised using
               Agda.",
  month     =  sep,
  year      =  2020,
  url       = "https://drops.dagstuhl.de/storage/00lipics/lipics-vol175-types2019/LIPIcs.TYPES.2019.4/LIPIcs.TYPES.2019.4.pdf"
}

@PHDTHESIS{Kovacs2023-gq,
  title     = "Type-theoretic signatures for algebraic theories and inductive
               types",
  author    = "Kovács, András",
  publisher = "Eötvös Loránd University",
  year      =  2023,
  url       = "https://andraskovacs.github.io/pdfs/phdthesis_compact.pdf"
}

@ARTICLE{Kaposi2019-gw,
  title         = "Shallow embedding of type theory is morally correct",
  author        = "Kaposi, Ambrus and Kovács, András and Kraus, Nicolai",
  journal       = "arXiv [cs.LO]",
  abstract      = "There are multiple ways to formalise the metatheory of type
                   theory. For some purposes, it is enough to consider specific
                   models of a type theory, but sometimes it is necessary to
                   refer to the syntax, for example in proofs of canonicity and
                   normalisation. One option is to embed the syntax deeply, by
                   using inductive definitions in a proof assistant. However, in
                   this case the handling of definitional equalities becomes
                   technically challenging. Alternatively, we can reuse
                   conversion checking in the metatheory by shallowly embedding
                   the object theory. In this paper, we consider the standard
                   model of a type theoretic object theory in Agda. This model
                   has the property that all of its equalities hold
                   definitionally, and we can use it as a shallow embedding by
                   building expressions from the components of this model.
                   However, if we are to reason soundly about the syntax with
                   this setup, we must ensure that distinguishable syntactic
                   constructs do not become provably equal when shallowly
                   embedded. First, we prove that shallow embedding is injective
                   up to definitional equality, by modelling the embedding as a
                   syntactic translation targeting the metatheory. Second, we
                   use an implementation hiding trick to disallow illegal
                   propositional equality proofs and constructions which do not
                   come from the syntax. We showcase our technique with very
                   short formalisations of canonicity and parametricity for
                   Martin-Lof type theory. Our technique only requires
                   features which are available in all major proof assistants
                   based on dependent type theory.",
  month         =  jul,
  year          =  2019,
  url           = "http://arxiv.org/abs/1907.07562",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LO"
}

@INCOLLECTION{Atkey2011-yi,
  title     = "When is a type refinement an inductive type?",
  author    = "Atkey, Robert and Johann, Patricia and Ghani, Neil",
  booktitle = "Foundations of Software Science and Computational Structures",
  publisher = "Springer Berlin Heidelberg",
  address   = "Berlin, Heidelberg",
  pages     = "72--87",
  series    = "Lecture notes in computer science",
  year      =  2011,
  url       = "https://bentnib.org/inductive-refinement.pdf"
}

@INCOLLECTION{Atkey2011-ex,
  title     = "When is a type refinement an inductive type?",
  author    = "Atkey, Robert and Johann, Patricia and Ghani, Neil",
  booktitle = "Foundations of Software Science and Computational Structures",
  publisher = "Springer Berlin Heidelberg",
  address   = "Berlin, Heidelberg",
  pages     = "72--87",
  series    = "Lecture notes in computer science",
  year      =  2011,
  url       = "https://bentnib.org/inductive-refinement.pdf"
}

@INCOLLECTION{Atkey2011-tw,
  title     = "When is a type refinement an inductive type?",
  author    = "Atkey, Robert and Johann, Patricia and Ghani, Neil",
  booktitle = "Foundations of Software Science and Computational Structures",
  publisher = "Springer Berlin Heidelberg",
  address   = "Berlin, Heidelberg",
  pages     = "72--87",
  series    = "Lecture notes in computer science",
  year      =  2011,
  url       = "https://bentnib.org/inductive-refinement.pdf"
}

@INCOLLECTION{Altenkirch2010-gd,
  title     = "ΠΣ: Dependent types without the sugar",
  author    = "Altenkirch, Thorsten and Danielsson, Nils Anders and Löh, Andres
               and Oury, Nicolas",
  booktitle = "Functional and Logic Programming",
  publisher = "Springer Berlin Heidelberg",
  address   = "Berlin, Heidelberg",
  pages     = "40--55",
  series    = "Lecture notes in computer science",
  year      =  2010,
  url       = "https://people.cs.nott.ac.uk/psztxa/publ/pisigma-new.pdf"
}

@INPROCEEDINGS{Winterhalter2019-zw,
  title     = "Eliminating reflection from type theory",
  author    = "Winterhalter, Théo and Sozeau, Matthieu and Tabareau, Nicolas",
  booktitle = "Proceedings of the 8th ACM SIGPLAN International Conference on
               Certified Programs and Proofs",
  publisher = "ACM",
  address   = "New York, NY, USA",
  abstract  = "Type theories with equality reflection, such as extensional type
               theory (ETT), are convenient theories in which to formalise
               mathematics, as they make it possible to consider provably equal
               terms as convertible. Although type-checking is undecidable in
               this context, variants of ETT have been implemented, for example
               in NuPRL and more recently in Andromeda. The actual objects that
               can be checked are not proof-terms, but derivations of
               proof-terms. This suggests that any derivation of ETT can be
               translated into a typecheckable proof term of intensional type
               theory (ITT). However, this result, investigated categorically by
               Hofmann in 1995, and 10 years later more syntactically by Oury,
               has never given rise to an effective translation. In this paper,
               we provide the first effective syntactical translation from ETT
               to ITT with uniqueness of identity proofs and functional
               extensionality. This translation has been defined and proven
               correct in Coq and yields an executable plugin that translates a
               derivation in ETT into an actual Coq typing judgment.
               Additionally, we show how this result is extended in the context
               of homotopy type theory to a two-level type theory.",
  month     =  jan,
  year      =  2019,
  url       = "https://dl.acm.org/doi/10.1145/3293880.3294095",
  language  = "en"
}

@ARTICLE{Gratzer2020-kf,
  title         = "Multimodal Dependent Type Theory",
  author        = "Gratzer, Daniel and Kavvos, G A and Nuyts, Andreas and
                   Birkedal, Lars",
  journal       = "arXiv [cs.LO]",
  abstract      = "We introduce MTT, a dependent type theory which supports
                   multiple modalities. MTT is parametrized by a mode theory
                   which specifies a collection of modes, modalities, and
                   transformations between them. We show that different choices
                   of mode theory allow us to use the same type theory to
                   compute and reason in many modal situations, including
                   guarded recursion, axiomatic cohesion, and parametric
                   quantification. We reproduce examples from prior work in
                   guarded recursion and axiomatic cohesion, thereby
                   demonstrating that MTT constitutes a simple and usable syntax
                   whose instantiations intuitively correspond to previous
                   handcrafted modal type theories. In some cases, instantiating
                   MTT to a particular situation unearths a previously unknown
                   type theory that improves upon prior systems. Finally, we
                   investigate the metatheory of MTT. We prove the consistency
                   of MTT and establish canonicity through an extension of
                   recent type-theoretic gluing techniques. These results hold
                   irrespective of the choice of mode theory, and thus apply to
                   a wide variety of modal situations.",
  month         =  nov,
  year          =  2020,
  url           = "http://dx.doi.org/10.46298/lmcs-17(3:11)2021",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LO"
}

@MISC{UnknownUnknown-hl,
  title        = "The {GNU} {MP} Bignum Library",
  howpublished = "\url{https://gmplib.org/}",
  note         = "Accessed: 2024-12-8",
  language     = "en"
}

@INCOLLECTION{McBride2006-tz,
  title     = "A Few Constructions on Constructors",
  author    = "McBride, Conor and Goguen, Healfdene and McKinna, James",
  booktitle = "Lecture Notes in Computer Science",
  publisher = "Springer Berlin Heidelberg",
  address   = "Berlin, Heidelberg",
  pages     = "186--200",
  series    = "Lecture notes in computer science",
  year      =  2006,
  url       = "http://www.e-pig.org/downloads/concon.pdf"
}

@INCOLLECTION{McBride2006-fp,
  title     = "A Few Constructions on Constructors",
  author    = "McBride, Conor and Goguen, Healfdene and McKinna, James",
  booktitle = "Lecture Notes in Computer Science",
  publisher = "Springer Berlin Heidelberg",
  address   = "Berlin, Heidelberg",
  pages     = "186--200",
  series    = "Lecture notes in computer science",
  year      =  2006,
  url       = "http://www.e-pig.org/downloads/concon.pdf"
}

@BOOK{Adamek2010-ls,
  title     = "Cambridge tracts in mathematics: Algebraic theories: A
               categorical introduction to general algebra series number 184",
  author    = "Adamek, Jiri and Rosicky, J and Vitale, E M",
  publisher = "Cambridge University Press",
  address   = "Cambridge, England",
  abstract  = "Algebraic theories, introduced as a concept in the 1960s, have
               been a fundamental step towards a categorical view of general
               algebra. Moreover, they have proved very useful in various areas
               of mathematics and computer science. This carefully developed
               book gives a systematic introduction to algebra based on
               algebraic theories that is accessible to both graduate students
               and researchers. It will facilitate interactions of general
               algebra, category theory and computer science. A central concept
               is that of sifted colimits – that is, those commuting with finite
               products in sets. The authors prove the duality between algebraic
               categories and algebraic theories and discuss Morita equivalence
               between algebraic theories. They also pay special attention to
               one-sorted algebraic theories and the corresponding concrete
               algebraic categories over sets, and to S-sorted algebraic
               theories, which are important in program semantics. The final
               chapter is devoted to finitary localizations of algebraic
               categories, a recent research area.",
  month     =  nov,
  year      =  2010,
  url       = "https://www.cambridge.org/academic/subjects/mathematics/logic-categories-and-sets/algebraic-theories-categorical-introduction-general-algebra?format=HB&isbn=9780521119221"
}

@BOOK{Hofmann1997-py,
  title     = "Extensional constructs in intensional type theory",
  author    = "Hofmann, Martin",
  publisher = "Springer London",
  address   = "London",
  year      =  1997,
  url       = "https://link.springer.com/book/10.1007/978-1-4471-0963-1"
}

@ARTICLE{Winterhalter2024-pw,
  title     = "Dependent ghosts have a reflection for free",
  author    = "Winterhalter, Théo",
  journal   = "Proc. ACM Program. Lang.",
  publisher = "Association for Computing Machinery (ACM)",
  volume    =  8,
  number    = "ICFP",
  pages     = "630--658",
  abstract  = "We introduce ghost type theory (GTT) a dependent type theory
               extended with a new universe for ghost data that can safely be
               erased when running a program but which is not proof irrelevant
               like with a universe of (strict) propositions. Instead, ghost
               data carry information that can be used in proofs or to discard
               impossible cases in relevant computations. Casts can be used to
               replace ghost values by others that are propositionally equal,
               but crucially these casts can be ignored for conversion without
               compromising soundness. We provide a type-preserving erasure
               procedure which gets rid of all ghost data and proofs, a step
               which may be used as a first step to program extraction. We give
               a syntactical model of GTT using a program translation akin to
               the parametricity translation and thus show consistency of the
               theory. Because it is a parametricity model, it can also be used
               to derive free theorems about programs using ghost code. We
               further extend GTT to support equality reflection and show that
               we can eliminate its use without the need for the usual extra
               axioms of function extensionality and uniqueness of identity
               proofs. In particular we validate the intuition that indices of
               inductive type—such as the length index of vectors—do not matter
               for computation and can safely be considered modulo theory. The
               results of the paper have been formalised in Coq.",
  month     =  aug,
  year      =  2024,
  url       = "https://hal.science/hal-04163836/file/icfp24main-p74-p-dba0e9df86-78716-final.pdf",
  language  = "en"
}

@MISC{Mc_Bride_undated-bb,
  title  = "Dependently Typed Functional Programs and their Proofs",
  author = "Mc Bride, Conor",
  url    = "http://strictlypositive.org/thesis.pdf"
}

@ARTICLE{Pitts2015-vj,
  title     = "A dependent type theory with abstractable names",
  author    = "Pitts, Andrew M and Matthiesen, Justus and Derikx, Jasper",
  journal   = "Electron. Notes Theor. Comput. Sci.",
  publisher = "Elsevier BV",
  volume    =  312,
  pages     = "19--50",
  abstract  = "This paper describes a version of Martin-Löf's dependent type
               theory extended with names and constructs for freshness and
               name-abstraction derived from the theory of nominal sets. We aim
               for a type theory for computing and proving (via a Curry-Howard
               correspondence) with syntactic structures which captures
               familiar, but informal, ‘nameful’ practices when dealing with
               binders.",
  month     =  apr,
  year      =  2015,
  url       = "https://www.cl.cam.ac.uk/~amp12/papers/deptta/deptta.pdf"
}

@ARTICLE{Schopp2004-vg,
  title   = "A dependent type theory with names and binding",
  author  = "Schöpp, Ulrich and Stark, I",
  journal = "CSL",
  volume  =  235,
  pages   = "235--249",
  month   =  sep,
  year    =  2004,
  url     = "https://homepages.inf.ed.ac.uk/stark/names+binding.pdf"
}

@INCOLLECTION{Goguen2006-sy,
  title     = "Eliminating dependent pattern matching",
  author    = "Goguen, Healfdene and McBride, Conor and McKinna, James",
  booktitle = "Algebra, Meaning, and Computation",
  publisher = "Springer Berlin Heidelberg",
  address   = "Berlin, Heidelberg",
  pages     = "521--540",
  series    = "Lecture notes in computer science",
  year      =  2006,
  url       = "https://research.google.com/pubs/archive/99.pdf"
}

@ARTICLE{Cockx2018-bv,
  title     = "Proof-relevant unification: Dependent pattern matching with only
               the axioms of your type theory",
  author    = "Cockx, Jesper and Devriese, Dominique",
  journal   = "J. Funct. Prog.",
  publisher = "Cambridge University Press (CUP)",
  volume    =  28,
  number    = "e12",
  pages     = "e12",
  abstract  = "AbstractDependently typed languages such as Agda, Coq, and Idris
               use a syntactic first-order unification algorithm to check
               definitions by dependent pattern matching. However, standard
               unification algorithms implicitly rely on principles such
               asuniqueness of identity proofsandinjectivity of type
               constructors. These principles are inadmissible in many type
               theories, particularly in the new and promising branch known as
               homotopy type theory. As a result, programs and proofs in these
               new theories cannot make use of dependent pattern matching or
               other techniques relying on unification, and are as a result much
               harder to write, modify, and understand. This paper proposes a
               proof-relevant framework for reasoning formally about unification
               in a dependently typed setting. In this framework, unification
               rules compute not just a unifier but also a corresponding
               soundness proof in the form of anequivalencebetween two sets of
               equations. By rephrasing the standard unification rules in a
               proof-relevant manner, they are guaranteed to preserve soundness
               of the theory. In addition, it enables us to safely add new rules
               that can exploit the dependencies between the types of equations,
               such as rules for eta-equality of record types and higher
               dimensional unification rules for solving equations between
               equality proofs. Using our framework, we implemented a complete
               overhaul of the unification algorithm used by Agda. As a result,
               we were able to replace previousad-hocrestrictions with formally
               verified unification rules, fixing a substantial number of bugs
               in the process. In the future, we may also want to integrate new
               principles with pattern matching, for example, the higher
               inductive types introduced by homotopy type theory. Our framework
               also provides a solid basis for such extensions to be built on.",
  month     =  jan,
  year      =  2018,
  url       = "https://www.cambridge.org/core/services/aop-cambridge-core/content/view/E54D56DC3F5D5361CCDECA824030C38E/S095679681800014Xa.pdf/div-class-title-proof-relevant-unification-dependent-pattern-matching-with-only-the-axioms-of-your-type-theory-div.pdf",
  language  = "en"
}

@ARTICLE{Levy2006-qd,
  title     = "Call-by-push-value: Decomposing call-by-value and call-by-name",
  author    = "Levy, Paul Blain",
  journal   = "High.-order Symb. Comput.",
  publisher = "Springer Science and Business Media LLC",
  volume    =  19,
  number    =  4,
  pages     = "377--414",
  month     =  dec,
  year      =  2006,
  url       = "https://link.springer.com/article/10.1007/s10990-006-0480-6",
  language  = "en"
}

@ARTICLE{Loow2024-td,
  title         = "Compositional symbolic execution for correctness and
                   incorrectness reasoning (extended version)",
  author        = "Lööw, Andreas and Nantes-Sobrinho, Daniele and Ayoun,
                   Sacha-Élie and Cronjäger, Caroline and Maksimović, Petar and
                   Gardner, Philippa",
  journal       = "arXiv [cs.PL]",
  abstract      = "The introduction of separation logic has led to the
                   development of symbolic-execution techniques and tools that
                   are (functionally) compositional with function specifications
                   that can be used in broader calling contexts. Many of the
                   compositional symbolic-execution tools developed in academia
                   and industry have been grounded on a formal foundation, but
                   either the function specifications are not validated
                   concerning the underlying separation logic of the theory, or
                   there is a large gulf between the theory and the tool
                   implementation. We introduce a formal compositional
                   symbolic-execution engine which creates and uses function
                   specifications from an underlying separation logic and
                   provides a sound theoretical foundation partially inspired by
                   the Gillian symbolic-execution platform. This is achieved by
                   providing an axiomatic interface which describes the
                   properties of the consume and produce operations used in the
                   engine to compositionally update the symbolic state,
                   including, when calling function specifications -- a
                   technique used by VeriFast, Viper, and Gillian but not
                   previously characterised independently of the tool. Our
                   result consume and produce operations inspired by the Gillian
                   implementation that satisfy the properties described by our
                   axiomatic interface. A surprising property of our engine
                   semantics is its ability to underpin both correctness and
                   incorrectness reasoning, with the primary distinction being
                   the choice between satisfiability and validity. We use this
                   property to extend the Gillian platform, which previously
                   only supported correctness reasoning, with incorrectness
                   reasoning and automatic true bug-finding using incorrectness
                   bi-abduction. We evaluate our new Gillian platform through
                   instantiation to C. This instantiation is the first tool
                   grounded on a common formal compositional symbolic-execution
                   engine to support both correctness and incorrectness
                   reasoning.",
  month         =  jul,
  year          =  2024,
  url           = "http://arxiv.org/abs/2407.10838",
  archivePrefix = "arXiv",
  primaryClass  = "cs.PL"
}

@ARTICLE{Ghorbani2024-va,
  title         = "Compressing Structured Tensor Algebra",
  author        = "Ghorbani, Mahdi and Bauer, Emilien and Grosser, Tobias and
                   Shaikhha, Amir",
  journal       = "arXiv [cs.PL]",
  abstract      = "Tensor algebra is a crucial component for data-intensive
                   workloads such as machine learning and scientific computing.
                   As the complexity of data grows, scientists often encounter a
                   dilemma between the highly specialized dense tensor algebra
                   and efficient structure-aware algorithms provided by sparse
                   tensor algebra. In this paper, we introduce DASTAC, a
                   framework to propagate the tensors's captured high-level
                   structure down to low-level code generation by incorporating
                   techniques such as automatic data layout compression,
                   polyhedral analysis, and affine code generation. Our
                   methodology reduces memory footprint by automatically
                   detecting the best data layout, heavily benefits from
                   polyhedral optimizations, leverages further optimizations,
                   and enables parallelization through MLIR. Through extensive
                   experimentation, we show that DASTAC achieves 1 to 2 orders
                   of magnitude speedup over TACO, a state-of-the-art sparse
                   tensor compiler, and StructTensor, a state-of-the-art
                   structured tensor algebra compiler, with a significantly
                   lower memory footprint.",
  month         =  jul,
  year          =  2024,
  url           = "http://arxiv.org/abs/2407.13726",
  archivePrefix = "arXiv",
  primaryClass  = "cs.PL"
}

@ARTICLE{Tang2024-ab,
  title         = "Modal Effect Types",
  author        = "Tang, Wenhao and White, Leo and Dolan, Stephen and
                   Hillerström, Daniel and Lindley, Sam and Lorenzen, Anton",
  journal       = "arXiv [cs.PL]",
  abstract      = "We propose a novel type system for effects and handlers using
                   modal types. Conventional effect systems attach effects to
                   function types, which can lead to verbose effect-polymorphic
                   types, especially for higher-order functions. Our modal
                   effect system provides succinct types for higher-order
                   first-class functions without losing modularity and
                   reusability. The core idea is to decouple effects from
                   function types and instead to track effects through relative
                   and absolute modalities, which represent transformations on
                   the ambient effects provided by the context. We formalise the
                   idea of modal effect types in a multimodal System F-style
                   core calculus Met with effects and handlers. Met supports
                   modular effectful programming via modalities without relying
                   on effect variables. We encode a practical fragment of a
                   conventional row-based effect system with effect
                   polymorphism, which captures most common use-cases, into Met
                   in order to formally demonstrate the expressive power of
                   modal effect types. To recover the full power of conventional
                   effect systems beyond this fragment, we seamlessly extend Met
                   to Mete with effect variables. We propose a surface language
                   Metel for Mete with a sound and complete type inference
                   algorithm inspired by FreezeML.",
  month         =  jul,
  year          =  2024,
  url           = "http://arxiv.org/abs/2407.11816",
  archivePrefix = "arXiv",
  primaryClass  = "cs.PL"
}

@ARTICLE{Zwaan2024-dv,
  title         = "Defining name accessibility using scope graphs (extended
                   edition)",
  author        = "Zwaan, Aron and Poulsen, Casper Bach",
  journal       = "arXiv [cs.PL]",
  abstract      = "Many programming languages allow programmers to regulate
                   accessibility; i.e., annotating a declaration with keywords
                   such as export and private to indicate where it can be
                   accessed. Despite the importance of name accessibility for,
                   e.g., compilers, editor auto-completion and tooling, and
                   automated refactorings, few existing type systems provide a
                   formal account of name accessibility. We present a
                   declarative, executable, and language-parametric model for
                   name accessibility, which provides a formal specification of
                   name accessibility in Java, C\#, C++, Rust, and Eiffel. We
                   achieve this by defining name accessibility as a predicate on
                   resolution paths through scope graphs. Since scope graphs are
                   a language-independent model of name resolution, our model
                   provides a uniform approach to defining different
                   accessibility policies for different languages. Our model is
                   implemented in Statix, a logic language for executable type
                   system specification using scope graphs. We evaluate its
                   correctness on a test suite that compares it with the C\#,
                   Java, and Rust compilers, and show we can synthesize access
                   modifiers in programs with holes accurately.",
  month         =  jul,
  year          =  2024,
  url           = "http://arxiv.org/abs/2407.09320",
  archivePrefix = "arXiv",
  primaryClass  = "cs.PL"
}

@ARTICLE{Koronkevich2024-mj,
  title         = "Type Universes as Allocation Effects",
  author        = "Koronkevich, Paulette and Bowman, William J",
  journal       = "arXiv [cs.PL]",
  abstract      = "In this paper, we explore a connection between type universes
                   and memory allocation. Type universe hierarchies are used in
                   dependent type theories to ensure consistency, by forbidding
                   a type from quantifying over all types. Instead, the types of
                   types (universes) form a hierarchy, and a type can only
                   quantify over types in other universes (with some
                   exceptions), restricting cyclic reasoning in proofs. We
                   present a perspective where universes also describe
                   \emph{where} values are allocated in the heap, and the choice
                   of universe algebra imposes a structure on the heap overall.
                   The resulting type system provides a simple declarative
                   system for reasoning about and restricting memory allocation,
                   without reasoning about reads or writes. We present a
                   theoretical framework for equipping a type system with
                   higher-order references restricted by a universe hierarchy,
                   and conjecture that many existing universe algebras give rise
                   to interesting systems for reasoning about allocation. We
                   present 3 instantiations of this approach to enable reasoning
                   about allocation in the simply typed $\lambda$-calculus: (1)
                   the standard ramified universe hierarchy, which we prove
                   guarantees termination of the language extended with
                   higher-order references by restricting cycles in the heap;
                   (2) an extension with an \emph{impredicative} base universe,
                   which we conjecture enables full-ground references (with
                   terminating computation but cyclic ground data structures);
                   (3) an extension with \emph{universe polymorphism}, which
                   divides the heap into fine-grained regions.",
  month         =  jul,
  year          =  2024,
  url           = "http://arxiv.org/abs/2407.06473",
  archivePrefix = "arXiv",
  primaryClass  = "cs.PL"
}

@INPROCEEDINGS{Cockx2016-nw,
  title     = "Unifiers as equivalences: proof-relevant unification of
               dependently typed data",
  author    = "Cockx, Jesper and Devriese, Dominique and Piessens, Frank",
  booktitle = "Proceedings of the 21st ACM SIGPLAN International Conference on
               Functional Programming",
  publisher = "ACM",
  address   = "New York, NY, USA",
  abstract  = "Dependently typed languages such as Agda, Coq and Idris use a
               syntactic first-order unification algorithm to check definitions
               by dependent pattern matching. However, these algorithms don’t
               adequately consider the types of the terms being unified, leading
               to various unintended results. As a consequence, they require ad
               hoc restrictions to preserve soundness, but this makes them very
               hard to prove correct, modify, or extend. This paper proposes a
               framework for reasoning formally about unification in a
               dependently typed setting. In this framework, unification rules
               compute not just a unifier but also a corresponding correctness
               proof in the form of an equivalence between two sets of
               equations. By rephrasing the standard unification rules in a
               proof-relevant manner, they are guaranteed to preserve soundness
               of the theory. In addition, it enables us to safely add new rules
               that can exploit the dependencies between the types of equations.
               Using our framework, we reimplemented the unification algorithm
               used by Agda. As a result, we were able to replace previous ad
               hoc restrictions with formally verified unification rules, fixing
               a number of bugs in the process. We are convinced this will also
               enable the addition of new and interesting unification rules in
               the future, without compromising soundness along the way.",
  month     =  sep,
  year      =  2016,
  url       = "https://dl.acm.org/doi/10.1145/2951913.2951917",
  language  = "en"
}

@ARTICLE{Sinkarovs2024-hb,
  title         = "Correctness is Demanding, Performance is Frustrating",
  author        = "Sinkarovs, Artjoms and Koopman, Thomas and Scholz, Sven-Bodo",
  journal       = "arXiv [cs.PL]",
  abstract      = "In this paper we demonstrate a technique for developing high
                   performance applications with strong correctness guarantees.
                   We use a theorem prover to derive a high-level specification
                   of the application that includes correctness invariants of
                   our choice. After that, within the same theorem prover, we
                   implement an extraction of the specified application into a
                   high-performance language of our choice. Concretely, we are
                   using Agda to specify a framework for automatic
                   differentiation (reverse mode) that is focused on index-safe
                   tensors. This framework comes with an optimiser for tensor
                   expressions and the ability to translate these expressions
                   into SaC and C. We specify a canonical convolutional neural
                   network within the proposed framework, compute the
                   derivatives needed for the training phase and then
                   demonstrate that the generated code matches the performance
                   of hand-written code when running on a multi-core machine.",
  month         =  jun,
  year          =  2024,
  url           = "http://arxiv.org/abs/2406.10405",
  archivePrefix = "arXiv",
  primaryClass  = "cs.PL"
}

@ARTICLE{Dagand2017-nj,
  title     = "The essence of ornaments",
  author    = "Dagand, Pierre-Evariste",
  journal   = "J. Funct. Programming",
  publisher = "Cambridge University Press",
  volume    =  27,
  pages     = "e9",
  abstract  = "The essence of ornaments - Volume 27",
  month     =  jan,
  year      =  2017,
  url       = "https://www.cambridge.org/core/services/aop-cambridge-core/content/view/4D2DF6F4FE23599C8C1FEA6C921A3748/S0956796816000356a.pdf/div-class-title-the-essence-of-ornaments-div.pdf"
}

@BOOK{2013-yr,
  title    = "``Τα κακομαθημένα παιδιά της Ιστορίας'': Η διαμόρφωση του
              νεοελληνικού κράτους 18ος-21ος αιώνας",
  author   = "Κωστής, Κώστας",
  year     =  2013,
  language = "el"
}

@ARTICLE{Kellison2024-of,
  title         = "Numerical fuzz: A type system for rounding error analysis",
  author        = "Kellison, Ariel E and Hsu, Justin",
  journal       = "arXiv [cs.PL]",
  abstract      = "Algorithms operating on real numbers are implemented as
                   floating-point computations in practice, but floating-point
                   operations introduce roundoff errors that can degrade the
                   accuracy of the result. We propose $\Lambda_{num}$, a
                   functional programming language with a type system that can
                   express quantitative bounds on roundoff error. Our type
                   system combines a sensitivity analysis, enforced through a
                   linear typing discipline, with a novel graded monad to track
                   the accumulation of roundoff errors. We prove that our type
                   system is sound by relating the denotational semantics of our
                   language to the exact and floating-point operational
                   semantics. To demonstrate our system, we instantiate
                   $\Lambda_{num}$ with error metrics proposed in the numerical
                   analysis literature and we show how to incorporate rounding
                   operations that faithfully model aspects of the IEEE 754
                   floating-point standard. To show that $\Lambda_{num}$ can be
                   a useful tool for automated error analysis, we develop a
                   prototype implementation for $\Lambda_{num}$ that infers
                   error bounds that are competitive with existing tools, while
                   often running significantly faster. Finally, we consider
                   semantic extensions of our graded monad to bound error under
                   more complex rounding behaviors, such as non-deterministic
                   and randomized rounding.",
  month         =  may,
  year          =  2024,
  url           = "http://arxiv.org/abs/2405.04612",
  archivePrefix = "arXiv",
  primaryClass  = "cs.PL"
}

@ARTICLE{Faisal_Al_Ameen2024-jq,
  title    = "Asynchronous unfold/fold transformation for fixpoint logic",
  author   = "Faisal Al Ameen, Mahmudul and Kobayashi, Naoki and Sato, Ryosuke",
  journal  = "Science of Computer Programming",
  volume   =  231,
  pages    =  103014,
  abstract = "Various program verification problems for functional programs can
              be reduced to the validity checking problem for formulas of a
              fixpoint logic. Recently, Kobayashi et al. have shown that the
              unfold/fold transformation originally developed for logic
              programming can be extended and applied to prove the validity of
              fixpoint logic formulas. In the present paper, we refine their
              unfold/fold transformation, so that each predicate can be unfolded
              a different number of times in an asynchronous manner. Inspired by
              the work of Lee et al. on size change termination, we use a
              variant of size change graphs to find an appropriate number of
              unfoldings of each predicate. We have implemented an unfold/fold
              transformation tool based on the proposed method, and evaluated
              its effectiveness.",
  month    =  jan,
  year     =  2024,
  url      = "https://www.sciencedirect.com/science/article/pii/S0167642323000965",
  keywords = "Program transformation; Program verification; Fixpoint logic"
}

@INPROCEEDINGS{De_Francesco1996-pn,
  title     = "Unfold/Fold transformations of concurrent processes",
  author    = "De Francesco, Nicoletta and Santone, Antonella",
  booktitle = "Programming Languages: Implementations, Logics, and Programs",
  publisher = "Springer Berlin Heidelberg",
  pages     = "167--181",
  abstract  = "Program transformation is a technique for obtaining, starting
               from a program P, a semantically equivalent one, which is
               ”better” than P with respect to a particular goal. Traditionally,
               the main goal of program transformation was obtaining more
               efficient programs, but, in general, this technique can be used
               to produce programs written in a syntactic form satisfying some
               properties. Program transformation techniques have been
               extensively studied in the framework of functional and logic
               languages, where they were applied mainly to obtain more
               efficient and readable programs. All these works are based on the
               Unfold/Fold program transformation method developed by Burstall
               and Darlington in the context of their recursive equational
               language. The use of Unfold/Fold based transformations for
               concurrent languages is a relevant issue that has not yet
               received an adequate attention. In fact the existing proposals of
               transformations of concurrent programs are not based on a general
               Unfold/Fold transformation theory. The aim of this paper is to
               define such a theory for the concurrent calculus CCS and to prove
               it correct.",
  year      =  1996,
  url       = "http://dx.doi.org/10.1007/3-540-61756-6_84"
}

@ARTICLE{Burstall1977-fg,
  title     = "A Transformation System for Developing Recursive Programs",
  author    = "Burstall, R M and Darlington, John",
  journal   = "J. ACM",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  volume    =  24,
  number    =  1,
  pages     = "44--67",
  abstract  = "A system of rules for transforming programs is described, with
               the programs in the form of recursion equations. An initially
               very simple, lucid, and hopefully correct program is transformed
               into a more efficient one by altering the recursion structure.
               Illustrative examples of program transformations are given, and a
               tentative implementation is described. Alternative structures for
               programs are shown, and a possible initial phase for an automatic
               or semiautomatic program-manipulation system is indicated.",
  month     =  jan,
  year      =  1977,
  url       = "https://doi.org/10.1145/321992.321996"
}

@ARTICLE{Coutts2007-nv,
  title     = "Stream fusion: from lists to streams to nothing at all",
  author    = "Coutts, Duncan and Leshchinskiy, Roman and Stewart, Don",
  journal   = "SIGPLAN Not.",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  volume    =  42,
  number    =  9,
  pages     = "315--326",
  abstract  = "This paper presents an automatic deforestation system, stream
               fusion, based on equational transformations, that fuses a wider
               range of functions than existing short-cut fusion systems. In
               particular, stream fusion is able to fuse zips, left folds and
               functions over nested lists, including list comprehensions. A
               distinguishing feature of the framework is its simplicity: by
               transforming list functions to expose their structure,
               intermediate values are eliminated by general purpose compiler
               optimisations.We have reimplemented the Haskell standard List
               library on top of our framework, providing stream fusion for
               Haskell lists. By allowing a wider range of functions to fuse, we
               see an increase in the number of occurrences of fusion in typical
               Haskell programs. We present benchmarks documenting time and
               space improvements.",
  month     =  oct,
  year      =  2007,
  url       = "https://doi.org/10.1145/1291220.1291199",
  keywords  = "program transformation, program optimisation, program fusion,
               functional programming, deforestation"
}

@MISC{noauthor_undated-wh,
  title        = "The Agda Wiki",
  howpublished = "\url{https://wiki.portal.chalmers.se/agda/pmwiki.php}",
  note         = "Accessed: 2024-5-3",
  language     = "en"
}

@MISC{noauthor_undated-is,
  title        = "Lean: Programming Language and Theorem Prover",
  howpublished = "\url{https://lean-lang.org/}",
  note         = "Accessed: 2024-5-3"
}

@MISC{noauthor_undated-tx,
  title        = "Idris: A Language for Type-Driven Development",
  howpublished = "\url{https://www.idris-lang.org/}",
  note         = "Accessed: 2024-5-3",
  language     = "en"
}

@MISC{noauthor_undated-fr,
  title        = "{OCaml}",
  booktitle    = "OCaml",
  abstract     = "OCaml is a general-purpose, industrial-strength programming
                  language with an emphasis on expressiveness and safety.",
  howpublished = "\url{https://ocaml.org/}",
  note         = "Accessed: 2024-5-3",
  language     = "en"
}

@MISC{noauthor_undated-pn,
  title        = "Haskell Language",
  abstract     = "The Haskell purely functional programming language home page.",
  howpublished = "\url{https://www.haskell.org/}",
  note         = "Accessed: 2024-5-3",
  language     = "en"
}

@ARTICLE{Kaposi2019-pj,
  title     = "Constructing quotient inductive-inductive types",
  author    = "Kaposi, Ambrus and Kovács, András and Altenkirch, Thorsten",
  journal   = "Proc. ACM Program. Lang.",
  publisher = "Association for Computing Machinery (ACM)",
  volume    =  3,
  number    = "POPL",
  pages     = "1--24",
  abstract  = "Quotient inductive-inductive types (QIITs) generalise inductive
               types in two ways: a QIIT can have more than one sort and the
               later sorts can be indexed over the previous ones. In addition,
               equality constructors are also allowed. We work in a setting with
               uniqueness of identity proofs, hence we use the term QIIT instead
               of higher inductive-inductive type. An example of a QIIT is the
               well-typed (intrinsic) syntax of type theory quotiented by
               conversion. In this paper first we specify finitary QIITs using a
               domain-specific type theory which we call the theory of
               signatures. The syntax of the theory of signatures is given by a
               QIIT as well. Then, using this syntax we show that all specified
               QIITs exist and they have a dependent elimination principle. We
               also show that algebras of a signature form a category with
               families (CwF) and use the internal language of this CwF to show
               that dependent elimination is equivalent to initiality.",
  month     =  jan,
  year      =  2019,
  url       = "http://dx.doi.org/10.1145/3290315",
  language  = "en"
}

@ARTICLE{Stump2019-tl,
  title         = "A weakly initial algebra for higher-order abstract syntax in
                   Cedille",
  author        = "Stump, Aaron",
  journal       = "arXiv [cs.LO]",
  abstract      = "Cedille is a relatively recent tool based on a Curry-style
                   pure type theory, without a primitive datatype system. Using
                   novel techniques based on dependent intersection types,
                   inductive datatypes with their induction principles are
                   derived. One benefit of this approach is that it allows
                   exploration of new or advanced forms of inductive datatypes.
                   This paper reports work in progress on one such form, namely
                   higher-order abstract syntax (HOAS). We consider the nature
                   of HOAS in the setting of pure type theory, comparing with
                   the traditional concept of environment models for lambda
                   calculus. We see an alternative, based on what we term Kripke
                   function-spaces, for which we can derive a weakly initial
                   algebra in Cedille. Several examples are given using the
                   encoding.",
  month         =  oct,
  year          =  2019,
  url           = "http://arxiv.org/abs/1910.10851",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LO"
}

@INPROCEEDINGS{Kopylov2003-ek,
  title     = "Dependent intersection: a new way of defining records in type
               theory",
  author    = "Kopylov, A",
  booktitle = "18th Annual IEEE Symposium of Logic in Computer Science, 2003.
               Proceedings",
  publisher = "IEEE Comput. Soc",
  pages     = "86--95",
  abstract  = "Records and dependent records are a powerful tool for
               programming, representing mathematical concepts, and program
               verification. In this last decade several type systems with
               records as primitive types were proposed. The question is arisen
               whether it is possible to define record type in existent type
               theories using standard types without introducing new primitives.
               It was known that independent records can be defined in type
               theories with dependent functions or intersection. On the other
               hand dependent records cannot be formed using standard types.
               Hickey introduced a complex notion of very dependent functions to
               represent dependent records. In the current paper we extend
               Martin-Lof's type theory with a simpler type constructor
               dependent intersection, i.e., the intersection of two types,
               where the second type may depend on elements of the first one
               (not to be confused with the intersection of a family of types).
               This new type constructor allows us to define dependent records
               in a very simple way. It also allows us to define the set type
               constructor.",
  year      =  2003,
  url       = "https://ieeexplore.ieee.org/document/1210048/citations?tabFilter=papers"
}

@INPROCEEDINGS{Partain1993-rj,
  title     = "The nofib Benchmark Suite of Haskell Programs",
  author    = "Partain, Will",
  booktitle = "Functional Programming, Glasgow 1992",
  publisher = "Springer London",
  pages     = "195--202",
  abstract  = "This position paper describes the need for, make-up of, and
               “rules of the game” for a benchmark suite of Haskell programs.
               (It does not include results from running the suite.) Those of us
               working on the Glasgow Haskell compiler hope this suite will
               encourage sound, quantitative assessment of lazy functional
               programming systems. This version of this paper reflects the
               state of play at the initial pre-release of the suite.",
  year      =  1993,
  url       = "http://dx.doi.org/10.1007/978-1-4471-3215-8_17"
}

@MISC{noauthor_undated-dx,
  title        = "{QuickCheck}: Automatic testing of Haskell programs",
  booktitle    = "Hackage",
  abstract     = "Automatic testing of Haskell programs",
  howpublished = "\url{https://hackage.haskell.org/package/QuickCheck}",
  note         = "Accessed: 2024-4-30"
}

@ARTICLE{Damato2023-yr,
  title   = "Specifying {QIITs} using Containers",
  author  = "Damato, Stefania and Altenkirch, Thorsten",
  journal = "HoTT/UF 2023",
  year    =  2023,
  url     = "https://hott-uf.github.io/2023/HoTTUF_2023_paper_8931.pdf"
}

@ARTICLE{Kaposi2017-ct,
  title    = "Codes for Quotient Inductive Inductive Types",
  author   = "Kaposi, A and Kovács, A",
  abstract = "A type of codes for QIITs along with interpretation functions
              which specify constructors, eliminators and computation rules for
              the encoded types is presented, analogous to containers which act
              as codes for W-types. Quotient inductive inductive types (QIITs)
              are generalisations of inductive types in type theory. QIITs may
              consist of multiple sorts which can be indexed over each other.
              They also support equality constructors. In this paper we present
              a type of codes for QIITs along with interpretation functions
              which specify constructors, eliminators and computation rules for
              the encoded types. This is analogous to containers which act as
              codes for W-types. We present an internal syntax of a type theory
              with a universe and restricted function types. A code for a QIIT
              is a context in this type theory. The internal syntax is
              formalised as a QIIT itself. We consider a metatheory with unique
              identity proofs, hence we interpret codes as quotient instead of
              general higher inductive types. Some of the contents of this paper
              were formalised in the proof assistant Agda. Showing that the
              given QIITs exist is left as future work.",
  year     =  2017,
  url      = "https://www.semanticscholar.org/paper/4199cd0314f4127b25dc8157b906c29619091c8a",
  language = "en"
}

@MISC{noauthor_undated-pe,
  title        = "Cubical — Agda 2.6.4.3 documentation",
  howpublished = "\url{https://agda.readthedocs.io/en/v2.6.4.3/language/cubical.html}",
  note         = "Accessed: 2024-4-30",
  language     = "en"
}

@INPROCEEDINGS{Altenkirch2018-nd,
  title     = "Quotient Inductive-Inductive Types",
  author    = "Altenkirch, Thorsten and Capriotti, Paolo and Dijkstra, Gabe and
               Kraus, Nicolai and Nordvall Forsberg, Fredrik",
  booktitle = "Foundations of Software Science and Computation Structures",
  publisher = "Springer International Publishing",
  pages     = "293--310",
  abstract  = "Higher inductive types (HITs) in Homotopy Type Theory allow the
               definition of datatypes which have constructors for equalities
               over the defined type. HITs generalise quotient types, and allow
               to define types with non-trivial higher equality types, such as
               spheres, suspensions and the torus. However, there are also
               interesting uses of HITs to define types satisfying uniqueness of
               equality proofs, such as the Cauchy reals, the partiality monad,
               and the well-typed syntax of type theory. In each of these
               examples we define several types that depend on each other
               mutually, i.e. they are inductive-inductive definitions. We call
               those HITs quotient inductive-inductive types (QIITs). Although
               there has been recent progress on a general theory of HITs, there
               is not yet a theoretical foundation for the combination of
               equality constructors and induction-induction, despite many
               interesting applications. In the present paper we present a first
               step towards a semantic definition of QIITs. In particular, we
               give an initial-algebra semantics. We further derive a section
               induction principle, stating that every algebra morphism into the
               algebra in question has a section, which is close to the
               intuitively expected elimination rules.",
  year      =  2018,
  url       = "http://dx.doi.org/10.1007/978-3-319-89366-2_16"
}

@MISC{noauthor_2017-yg,
  title        = "Quotient Inductive Types \& Their Dual",
  booktitle    = "Crunchy Type",
  abstract     = "A Quotient Inductive Type (QIT) is an inductive type that
                  includes some additional equality rules. For example, the type
                  Int of integers can be defined as a QIT: data Int Zero : Int
                  Succ : Int -> Int Pred : Int -> Int SuccPred : (z : Int) ->
                  Succ (Pred z) = z PredSucc…",
  month        =  apr,
  year         =  2017,
  howpublished = "\url{https://crunchytype.wordpress.com/2017/04/27/quotient-inductive-types-their-dual/}",
  note         = "Accessed: 2024-4-30",
  language     = "en"
}

@ARTICLE{Mcbride2004-fd,
  title     = "The view from the left",
  author    = "Mcbride, Conor and Mckinna, James",
  journal   = "J. Funct. Programming",
  publisher = "Cambridge University Press",
  volume    =  14,
  number    =  1,
  pages     = "69--111",
  abstract  = "The view from the left - Volume 14 Issue 1",
  month     =  jan,
  year      =  2004,
  url       = "https://www.cambridge.org/core/services/aop-cambridge-core/content/view/F8A44CAC27CCA178AF69DD84BC585A2D/S0956796803004829a.pdf/div-class-title-the-view-from-the-left-div.pdf"
}

@INPROCEEDINGS{Wadler1987-zp,
  title     = "Views: a way for pattern matching to cohabit with data
               abstraction",
  author    = "Wadler, P",
  booktitle = "Proceedings of the 14th ACM SIGACT-SIGPLAN symposium on
               Principles of programming languages",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "307--313",
  abstract  = "Pattern matching and data abstraction are important concepts in
               designing programs, but they do not fit well together. Pattern
               matching depends on making public a free data type
               representation, while data abstraction depends on hiding the
               representation. This paper proposes the views mechanism as a
               means of reconciling this conflict. A view allows any type to be
               viewed as a free data type, thus combining the clarity of pattern
               matching with the efficiency of data abstraction.",
  series    = "POPL '87",
  month     =  oct,
  year      =  1987,
  url       = "https://doi.org/10.1145/41625.41653"
}

@INPROCEEDINGS{Brady2004-ay,
  title     = "Inductive Families Need Not Store Their Indices",
  author    = "Brady, Edwin and McBride, Conor and McKinna, James",
  booktitle = "Types for Proofs and Programs",
  publisher = "Springer Berlin Heidelberg",
  pages     = "115--129",
  abstract  = "We consider the problem of efficient representation of
               dependently typed data. In particular, we consider a language TT
               based on Dybjer’s notion of inductive families [10] and reanalyse
               their general form with a view to optimising the storage
               associated with their use. We introduce an execution language,
               ExTT, which allows the commenting out of computationally
               irrelevant subterms and show how to use properties of elimination
               rules to elide constructor arguments and tags in ExTT. We further
               show how some types can be collapsed entirely at run-time.
               Several examples are given, including a representation of the
               simply typed λ-calculus for which our analysis yields an 80\%
               reduction in run-time storage requirements.",
  year      =  2004,
  url       = "http://dx.doi.org/10.1007/978-3-540-24849-1_8"
}

@PHDTHESIS{Brady2005-jq,
  title    = "Practical implementation of a dependently typed functional
              programming language",
  author   = "Brady, Edwin С",
  abstract = "Types express a program's meaning, and checking types ensures that
              a program has the intended meaning. In a dependently typed
              programming language types are predicated on values, leading to
              the possibility of expressing invariants of a program's behaviour
              in its type. Dependent types allow us to give more detailed
              meanings to programs, and hence be more confident of their
              correctness. This thesis considers the practical implementation of
              a dependently typed programming language, using the Epigram
              notation defined by McBride and McKinna. Epigram is a high level
              notation for dependently typed functional programming elaborating
              to a core type theory based on Lu๙s UTT, using Dybjer's inductive
              families and elimination rules to implement pattern matching. This
              gives us a rich framework for reasoning about programs. However, a
              naive implementation introduces several run-time overheads since
              the type system blurs the distinction between types and values;
              these overheads include the duplication of values, and the storage
              of redundant information and explicit proofs. A practical
              implementation of any programming language should be as efficient
              as possible; in this thesis we see how the apparent efficiency
              problems of dependently typed programming can be overcome and that
              in many cases the richer type information allows us to apply
              optimisations which are not directly available in traditional
              languages. I introduce three storage optimisations on inductive
              families; forcing, detagging and collapsing. I further introduce a
              compilation scheme from the core type theory to G-machine code,
              including a pattern matching compiler for elimination rules and a
              compilation scheme for efficient run-time implementation of
              Peano's natural numbers. We also see some low level optimisations
              for removal of identity functions, unused arguments and impossible
              case branches. As a result, we see that a dependent type theory is
              an effective base on which to build a feasible programming
              language.",
  year     =  2005,
  url      = "https://etheses.dur.ac.uk/2800/",
  school   = "Durham University"
}

@INPROCEEDINGS{Sasse2013-ef,
  title     = "{IBOS}: A Correct-By-Construction Modular Browser",
  author    = "Sasse, Ralf and King, Samuel T and Meseguer, José and Tang, Shuo",
  booktitle = "Formal Aspects of Component Software",
  publisher = "Springer Berlin Heidelberg",
  pages     = "224--241",
  abstract  = "Current web browsers are complex, have enormous trusted computing
               bases, and provide attackers with easy access to computer
               systems. This makes web browser security a difficult issue that
               increases in importance as more and more applications move to the
               web. Our approach for this challenge is to design and build a
               correct-by-construction web browser, called IBOS, that consists
               of multiple concurrent components, with a small required trusted
               computing base. We give a formal specification of the design of
               this secure-by-construction web browser in rewriting logic. We
               use formal verification of that specification to prove the
               desired security properties of the IBOS design, including the
               address bar correctness and the same-origin policy.",
  year      =  2013,
  url       = "http://dx.doi.org/10.1007/978-3-642-35861-6_14"
}

@ARTICLE{Bahr2015-uy,
  title     = "Calculating correct compilers",
  author    = "Bahr, Patrick and Hutton, Graham",
  journal   = "J. Funct. Programming",
  publisher = "Cambridge University Press",
  volume    =  25,
  pages     = "e14",
  abstract  = "In this article, we present a new approach to the problem of
               calculating compilers. In particular, we develop a simple but
               general technique that allows us to derive correct compilers from
               high-level semantics by systematic calculation, with all details
               of the implementation of the compilers falling naturally out of
               the calculation process. Our approach is based upon the use of
               standard equational reasoning techniques, and has been applied to
               calculate compilers for a wide range of language features and
               their combination, including arithmetic expressions, exceptions,
               state, various forms of lambda calculi, bounded and unbounded
               loops, non-determinism and interrupts. All the calculations in
               the article have been formalised using the Coq proof assistant,
               which serves as a convenient interactive tool for developing and
               verifying the calculations.",
  month     =  jan,
  year      =  2015,
  url       = "https://www.cambridge.org/core/services/aop-cambridge-core/content/view/70AA17724EBCA4182B1B2B522362A9AF/S0956796815000180a.pdf/div-class-title-calculating-correct-compilers-div.pdf"
}

@ARTICLE{Yallop2018-zw,
  title     = "Partially-static data as free extension of algebras",
  author    = "Yallop, Jeremy and von Glehn, Tamara and Kammar, Ohad",
  journal   = "Proc. ACM Program. Lang.",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  volume    =  2,
  number    = "ICFP",
  pages     = "1--30",
  abstract  = "Partially-static data structures are a well-known technique for
               improving binding times. However, they are often defined in an
               ad-hoc manner, without a unifying framework to ensure full use of
               the equations associated with each operation. We present a
               foundational view of partially-static data structures as free
               extensions of algebras for suitable equational theories, i.e. the
               coproduct of an algebra and a free algebra in the category of
               algebras and their homomorphisms. By precalculating these free
               extensions, we construct a high-level library of partially-static
               data representations for common algebraic structures. We
               demonstrate our library with common use-cases from the
               literature: string and list manipulation, linear algebra, and
               numerical simplification.",
  month     =  jul,
  year      =  2018,
  url       = "https://doi.org/10.1145/3236795",
  keywords  = "universal algebra, partially-static data, partial evaluation,
               multi-stage compilation, metaprogramming"
}

@INPROCEEDINGS{Bulychev2010-sq,
  title     = "Anti-unification Algorithms and Their Applications in Program
               Analysis",
  author    = "Bulychev, Peter E and Kostylev, Egor V and Zakharov, Vladimir A",
  booktitle = "Perspectives of Systems Informatics",
  publisher = "Springer Berlin Heidelberg",
  pages     = "413--423",
  abstract  = "A term t is called a template of terms t1 and t2 iff t1 = tη1 and
               t2 = tη2, for some substitutions η1 and η2. A template t of t1
               and t2 is called the most specific iff for any template t′ of t1
               and t2 there exists a substitution ξ such that t = t′ξ. The
               anti-unification problem is that of computing the most specific
               template of two given terms. This problem is dual to the
               well-known unification problem, which is the computing of the
               most general instance of terms. Unification is used extensively
               in automatic theorem proving and logic programming. We believe
               that anti-unification algorithms may have wide applications in
               program analysis. In this paper we present an efficient algorithm
               for computing the most specific templates of terms represented by
               labelled directed acyclic graphs and estimate the complexity of
               the anti-unification problem. We also describe techniques for
               invariant generation and software clone detection based on the
               concepts of the most specific templates and anti-unification.",
  year      =  2010,
  url       = "http://dx.doi.org/10.1007/978-3-642-11486-1_35"
}

@MISC{noauthor_undated-ka,
  title        = "The {GNU} {MP} Bignum Library",
  howpublished = "\url{https://gmplib.org/}",
  note         = "Accessed: 2024-4-30",
  language     = "en"
}

@MISC{noauthor_undated-mf,
  title       = "{Idris2}",
  institution = "Github",
  abstract    = "The optimization hack for compare probably isn't ideal, but I'm
                 unsure of a better way.",
  url         = "https://github.com/idris-lang/Idris2/pull/1580",
  language    = "en"
}

@MISC{Stewart_undated-mc,
  title        = "dlist: {DIfference} lists",
  author       = "Stewart, Don",
  booktitle    = "Hackage",
  abstract     = "Difference lists",
  howpublished = "\url{https://hackage.haskell.org/package/dlist}",
  note         = "Accessed: 2024-4-30"
}

@MISC{Haskell_undated-vt,
  title        = "containers: Assorted concrete container types",
  author       = "{Haskell}",
  booktitle    = "Hackage",
  abstract     = "Assorted concrete container types",
  howpublished = "\url{https://hackage.haskell.org/package/containers-0.7}",
  note         = "Accessed: 2024-4-30"
}

@MISC{Haskell_undated-uh,
  title        = "array: Mutable and immutable arrays",
  author       = "{Haskell}",
  booktitle    = "Hackage",
  abstract     = "Mutable and immutable arrays",
  howpublished = "\url{https://hackage.haskell.org/package/array}",
  note         = "Accessed: 2024-4-30"
}

@MISC{Leshchinskiy_undated-nt,
  title        = "vector: Efficient Arrays",
  author       = "Leshchinskiy, Roman",
  booktitle    = "Hackage",
  howpublished = "\url{https://hackage.haskell.org/package/vector}",
  note         = "Accessed: 2024-4-30"
}

@INPROCEEDINGS{Meijer1991-xz,
  title     = "Functional programming with bananas, lenses, envelopes and barbed
               wire",
  author    = "Meijer, Erik and Fokkinga, Maarten and Paterson, Ross",
  booktitle = "Functional Programming Languages and Computer Architecture",
  publisher = "Springer Berlin Heidelberg",
  pages     = "124--144",
  abstract  = "We develop a calculus for lazy functional programming based on
               recursion operators associated with data type definitions. For
               these operators we derive various algebraic laws that are useful
               in deriving and manipulating programs. We shall show that all
               example functions in Bird and Wadler's “Introduction to
               Functional Programming” can be expressed using these operators.",
  year      =  1991,
  url       = "http://dx.doi.org/10.1007/3540543961_7"
}

@INPROCEEDINGS{Atkey2018-pj,
  title     = "Syntax and Semantics of Quantitative Type Theory",
  author    = "Atkey, Robert",
  booktitle = "Proceedings of the 33rd Annual ACM/IEEE Symposium on Logic in
               Computer Science",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "56--65",
  abstract  = "We present Quantitative Type Theory, a Type Theory that records
               usage information for each variable in a judgement, based on a
               previous system by McBride. The usage information is used to give
               a realizability semantics using a variant of Linear Combinatory
               Algebras, refining the usual realizability semantics of Type
               Theory by accurately tracking resource behaviour. We define the
               semantics in terms of Quantitative Categories with Families, a
               novel extension of Categories with Families for modelling
               resource sensitive type theories.",
  series    = "LICS '18",
  month     =  jul,
  year      =  2018,
  url       = "https://doi.org/10.1145/3209108.3209189",
  keywords  = "Linear Logic, Type Theory"
}

@INPROCEEDINGS{Hughes2024-di,
  title     = "Program Synthesis from Graded Types",
  author    = "Hughes, Jack and Orchard, Dominic",
  booktitle = "Programming Languages and Systems",
  publisher = "Springer Nature Switzerland",
  pages     = "83--112",
  abstract  = "Graded type systems are a class of type system for fine-grained
               quantitative reasoning about data-flow in programs. Through the
               use of resource annotations (or grades), a programmer can express
               various program properties at the type level, reducing the number
               of typeable programs. These additional constraints on types lend
               themselves naturally to type-directed program synthesis, where
               this information can be exploited to constrain the search space
               of programs. We present a synthesis algorithm for a graded type
               system, where grades form an arbitrary pre-ordered semiring.
               Harnessing this grade information in synthesis is non-trivial,
               and we explore some of the issues involved in designing and
               implementing a resource-aware program synthesis tool. In our
               evaluation we show that by harnessing grades in synthesis, the
               majority of our benchmark programs (many of which involve
               recursive functions over recursive ADTs) require less exploration
               of the synthesis search space than a purely type-driven approach
               and with fewer needed input-output examples. This
               type-and-graded-directed approach is demonstrated for the
               research language Granule but we also adapt it for synthesising
               Haskell programs that use GHC’s linear types extension.",
  year      =  2024,
  url       = "http://dx.doi.org/10.1007/978-3-031-57262-3_4"
}

@ARTICLE{Pickering2021-ri,
  title         = "A Specification for Typed Template Haskell",
  author        = "Pickering, Matthew and Löh, Andres and Wu, Nicolas",
  journal       = "arXiv [cs.PL]",
  abstract      = "Multi-stage programming is a proven technique that provides
                   predictable performance characteristics by controlling code
                   generation. We propose a core semantics for Typed Template
                   Haskell, an extension of Haskell that supports multi staged
                   programming that interacts well with polymorphism and
                   qualified types. Our semantics relates a declarative source
                   language with qualified types to a core language based on the
                   the polymorphic lambda calculus augmented with multi-stage
                   constructs.",
  month         =  dec,
  year          =  2021,
  url           = "http://arxiv.org/abs/2112.03653",
  archivePrefix = "arXiv",
  primaryClass  = "cs.PL"
}

@INPROCEEDINGS{Kiselyov2014-bg,
  title     = "The Design and Implementation of {BER} {MetaOCaml}",
  author    = "Kiselyov, Oleg",
  booktitle = "Functional and Logic Programming",
  publisher = "Springer International Publishing",
  pages     = "86--102",
  abstract  = "MetaOCaml is a superset of OCaml extending it with the data type
               for program code and operations for constructing and executing
               such typed code values. It has been used for compiling
               domain-specific languages and automating tedious and error-prone
               specializations of high-performance computational kernels. By
               statically ensuring that the generated code compiles and letting
               us quickly run it, MetaOCaml makes writing generators less
               daunting and more productive.",
  year      =  2014,
  url       = "http://dx.doi.org/10.1007/978-3-319-07151-0_6"
}

@MISC{noauthor_undated-xh,
  title        = "{BER} {MetaOCaml}",
  abstract     = "Introduction to BER MetaOCaml: how to use, how to install, the
                  current status and the future development",
  howpublished = "\url{https://okmij.org/ftp/ML/MetaOCaml.html}",
  note         = "Accessed: 2024-4-27"
}

@ARTICLE{Nanevski2005-ea,
  title     = "Staged computation with names and necessity",
  author    = "Nanevski, Aleksandar and Pfenning, Frank",
  journal   = "J. Funct. Programming",
  publisher = "Cambridge University Press",
  volume    =  15,
  number    =  6,
  pages     = "893--939",
  abstract  = "Staging is a programming technique for dividing the computation
               in order to exploit the early availability of some arguments. In
               the early stages the program uses the available arguments to
               generate, at run time, the code for the late stages. A type
               system for staging should ensure that only well-typed expressions
               are generated, and that only expressions with no free variables
               are permitted for evaluation. In this paper, we present a
               calculus for staged computation in which code from the late
               stages is composed by splicing smaller code fragments into a
               larger context, possibly incurring capture of free variables. The
               type system ensures safety by tracking the names of free
               variables for each code fragment. The type system is based on the
               necessity operator □ from constructive modal logic, which we
               index with a set of names C. Our type □CA classifies expressions
               of type A that belong to the late stage, and whose free names are
               in the set C.",
  month     =  nov,
  year      =  2005,
  url       = "https://www.cambridge.org/core/services/aop-cambridge-core/content/view/13E2C64B0C6D91711C2DE0F3CD03C002/S095679680500568Xa.pdf/div-class-title-staged-computation-with-names-and-necessity-div.pdf"
}

@INPROCEEDINGS{Kameyama2008-nc,
  title     = "Closing the stage: from staged code to typed closures",
  author    = "Kameyama, Yukiyoshi and Kiselyov, Oleg and Shan, Chung-Chieh",
  booktitle = "Proceedings of the 2008 ACM SIGPLAN symposium on Partial
               evaluation and semantics-based program manipulation",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "147--157",
  abstract  = "Code generation lets us write well-abstracted programs without
               performance penalty. Writing a correct code generator is easier
               than building a full-scale compiler but still hard. Typed
               multistage languages such as MetaOCaml help in two ways: they
               provide simple annotations to express code generation, and they
               assure that the generated code is well-typed and well-scoped.
               Unfortunately, the assurance only holds without side effects such
               as state and control. Without effects, generators often have to
               be written in a continuation-passing or monadic style that has
               proved inconvenient. It is thus a pressing open problem to
               combine effects with staging in a sound type system.This paper
               takes a first step towards solving the problem, by translating
               the staging away. Our source language models MetaOCaml restricted
               to one future stage. It is a call-by-value language, with a sound
               type system and a small-step operational semantics, that supports
               building open code, running closed code, cross-stage persistence,
               and non-termination effects. We translate each typing derivation
               from this source language to the unstaged System F with
               constants. Our translation represents future-stage code using
               closures, yet preserves the typing, α-equivalence (hygiene), and
               (we conjecture) termination and evaluation order of the staged
               program.To decouple evaluation from scope (a defining
               characteristic of staging), our translation weakens the typing
               environment of open code using a term coercion reminiscent of
               Goedel's translation from intuitionistic to modal logic. By
               converting open code to closures with typed environments, our
               translation establishes a framework in which to study staging
               with effects and to prototype staged languages. It already makes
               scope extrusion a type error.",
  series    = "PEPM '08",
  month     =  jan,
  year      =  2008,
  url       = "https://doi.org/10.1145/1328408.1328430",
  keywords  = "closures, multistage programming, mutable state and control
               effects, parametric polymorphism, type abstraction"
}

@ARTICLE{Davies2001-lq,
  title     = "A modal analysis of staged computation",
  author    = "Davies, Rowan and Pfenning, Frank",
  journal   = "J. ACM",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  volume    =  48,
  number    =  3,
  pages     = "555--604",
  abstract  = "We show that a type system based on the intuitionistic modal
               logic S4 provides an expressive framework for specifying and
               analyzing computation stages in the context of typed λ-calculi
               and functional languages. We directly demonstrate the sense in
               which our λe→□-calculus captures staging, and also give a
               conservative embeddng of Nielson and Nielson's two-level
               functional language in our functional language Mini-ML□, thus
               proving that binding-time correctness is equivalent to modal
               correctness on this fragment. In addition, Mini-ML□ can also
               express immediate evaluation and sharing of code across multiple
               stages, thus supporting run-time code generation as well as
               partial evaluation.",
  month     =  may,
  year      =  2001,
  url       = "https://doi.org/10.1145/382780.382785",
  keywords  = "binding times, run-time code generation, staged computation"
}

@INPROCEEDINGS{Taha2003-we,
  title     = "Staged Notational Definitions",
  author    = "Taha, Walid and Johann, Patricia",
  booktitle = "Generative Programming and Component Engineering",
  publisher = "Springer Berlin Heidelberg",
  pages     = "97--116",
  abstract  = "Recent work proposed defining type-safe macros via interpretation
               into a multi-stage language. The utility of this approach was
               illustrated with a language called MacroML, in which all type
               checking is carried out before macro expansion. Building on this
               work, the goal of this paper is to develop a macro language that
               makes it easy for programmers to reason about terms locally. We
               show that defining the semantics of macros in this manner helps
               in developing and verifying not only type systems for macro
               languages but also equational reasoning principles. Because the
               MacroML calculus is sensetive to renaming of (what appear locally
               to be) bound variables, we present a calculus of staged
               notational definitions (SND) that eliminates the renaming problem
               but retains MacroML’s phase distinction. Additionally, SND
               incorporates the generality of Griffin’s account of notational
               definitions. We exhibit a formal equational theory for SND and
               prove its soundness.",
  year      =  2003,
  url       = "http://dx.doi.org/10.1007/978-3-540-39815-8_6"
}

@ARTICLE{Martin-Lof1984-pz,
  title    = "Intuitionistic type theory",
  author   = "Martin-Löf, P",
  volume   =  1,
  pages    = "1--91",
  abstract = "Preface These lectures were given in Padova at the Laboratorio per
              Ricerche di Di-namica dei Sistemi e di Elettronica Biomedica of
              the Consiglio Nazionale delle Ricerche during the month of June
              1980. I am indebted to Dr. Enrico Pagello of that laboratory for
              the opportunity of so doing. The audience was made up by
              philosophers, mathematicians and computer scientists. Accordingly,
              I tried to say something which might be of interest to each of
              these three categories. Essentially the same lectures, albeit in a
              somewhat improved and more advanced form, were given later in the
              same year as part of the meeting on Konstruktive Mengenlehre und
              Typentheorie which was organized in Munich by Prof. Dr. Helmut
              Schwichtenberg, to whom I am indebted for the invitation, during
              the week 29 September – 3 October 1980. The main improvement of
              the Munich lectures, as compared with those given in Padova, was
              the adoption of a systematic higher level (Ger. Stufe) notation
              which allows me to write simply respectively. Moreover, the use of
              higher level variables and constants makes it possible to
              formulate the elimination and equality rules for the cartesian
              product in such a way that they follow the same pattern as the
              elimination and equality rules for all the other type forming
              operations. In their new formulation, these rules read
              Π-elimination c ∈ Π(A, B) (y(x) ∈ B(x) (x ∈ A)) d(y) ∈ C(λ(y))
              F(c, d) ∈ C(c) and Π-equality (x ∈ A) b(x) ∈ B(x) (y(x) ∈ B(x) (x
              ∈ A)) d(y) ∈ C(λ(y)) F(λ(b), d) = d(b) ∈ C(λ(b)) respectively.
              Here y is a bound function variable, F is a new non-canonical
              (eliminatory) operator by means of which the binary application
              operation can be defined, putting Ap(c, a) ≡ F(c, (y) y(a)), and
              y(x) ∈ B(x) (x ∈ A) is an assumption, itself hypothetical, which
              has been put within parentheses to indicate that it is being
              discharged. A program of the new form F(c, d) has value e provided
              c has value λ(b) and d(b) has value e. This rule for evaluating
              F(c, d) reduces to the lazy evaluation rule for Ap(c, a) when the
              above definition is being made. Choosing C(z) to be B(a), thus
              independent of z, and d(y) to be y(a), the new elimination rule
              reduces to the old one and the new equality rule to the first of
              the two old equality rules. Moreover, the second …",
  year     =  1984,
  url      = "http://people.csail.mit.edu/jgross/personal-website/papers/academic-papers-local/Martin-Lof80.pdf"
}

@ARTICLE{Adams1993-rh,
  title     = "Functional Pearls Efficient sets—a balancing act",
  author    = "Adams, Stephen",
  journal   = "J. Funct. Programming",
  publisher = "Cambridge University Press",
  volume    =  3,
  number    =  4,
  pages     = "553--561",
  abstract  = "//static.cambridge.org/content/id/urn\%3Acambridge.org\%3Aid\%3Aarticle\%3AS0956796800000885/resource/name/firstPage-S0956796800000885a.jpg",
  month     =  oct,
  year      =  1993,
  url       = "https://www.cambridge.org/core/services/aop-cambridge-core/content/view/0CAA1C189B4F7C15CE9B8C02D0D4B54E/S0956796800000885a.pdf/div-class-title-span-class-italic-functional-pearls-span-efficient-sets-a-balancing-act-div.pdf"
}

@ARTICLE{Hinze2006-sy,
  title    = "Finger trees: a simple general−purpose data structure",
  author   = "Hinze, Ralf",
  journal  = "J. Fam. Psychol.",
  abstract = "We introduce 2-3 finger trees, a functional representation of
              persistent sequences supporting access to the ends in amortized
              constant time, and concatenation and splitting in time logarithmic
              in the size of the smaller piece. Representations achieving these
              bounds have appeared previously, but 2-3 finger trees are much
              simpler, as are the operations on them. Further, by defining the
              split operation in a general form, we obtain a general purpose
              data structure that can serve as a sequence, priority queue,
              search tree, priority search queue and more.",
  year     =  2006,
  url      = "http://www.cs.ox.ac.uk/publications/publication3989.bib"
}

@BOOK{Nipkow2024-ja,
  title  = "Functional Data Structures and Algorithms. A Proof Assistant
            Approach",
  author = "Nipkow, Tobias",
  month  =  feb,
  year   =  2024,
  url    = "https://functional-algorithms-verified.org/"
}

@ARTICLE{Blelloch2015-rm,
  title     = "Cache efficient functional algorithms",
  author    = "Blelloch, Guy E and Harper, Robert",
  journal   = "Commun. ACM",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  volume    =  58,
  number    =  7,
  pages     = "101--108",
  abstract  = "The widely studied I/O and ideal-cache models were developed to
               account for the large difference in costs to access memory at
               different levels of the memory hierarchy. Both models are based
               on a two level memory hierarchy with a fixed size fast memory
               (cache) of size M, and an unbounded slow memory organized in
               blocks of size B. The cost measure is based purely on the number
               of block transfers between the primary and secondary memory. All
               other operations are free. Many algorithms have been analyzed in
               these models and indeed these models predict the relative
               performance of algorithms much more accurately than the standard
               Random Access Machine (RAM) model. The models, however, require
               specifying algorithms at a very low level, requiring the user to
               carefully lay out their data in arrays in memory and manage their
               own memory allocation.We present a cost model for analyzing the
               memory efficiency of algorithms expressed in a simple functional
               language. We show how some algorithms written in standard forms
               using just lists and trees (no arrays) and requiring no explicit
               memory layout or memory management are efficient in the model. We
               then describe an implementation of the language and show provable
               bounds for mapping the cost in our model to the cost in the
               ideal-cache model. These bounds imply that purely functional
               programs based on lists and trees with no special attention to
               any details of memory layout can be asymptotically as efficient
               as the carefully designed imperative I/O efficient algorithms.
               For example we describe an o(n/BlogM/Bn/B) cost sorting
               algorithm, which is optimal in the ideal cache and I/O models.",
  month     =  jun,
  year      =  2015,
  url       = "https://doi.org/10.1145/2776825"
}

@BOOK{Okasaki1999-yi,
  title     = "Purely Functional Data Structures",
  author    = "Okasaki, Chris",
  publisher = "Cambridge University Press",
  abstract  = "Most books on data structures assume an imperative language such
               as C or C++. However, data structures for these languages do not
               always translate well to functional languages such as Standard
               ML, Haskell, or Scheme. This book describes data structures from
               the point of view of functional languages, with examples, and
               presents design techniques that allow programmers to develop
               their own functional data structures. The author includes both
               classical data structures, such as red-black trees and binomial
               queues, and a host of new data structures developed exclusively
               for functional languages. All source code is given in Standard ML
               and Haskell, and most of the programs are easily adaptable to
               other functional languages. This handy reference for professional
               programmers working with functional languages can also be used as
               a tutorial or for self-study.",
  month     =  jun,
  year      =  1999,
  url       = "https://play.google.com/store/books/details?id=SxPzSTcTalAC",
  language  = "en"
}

@INPROCEEDINGS{Xu2013-td,
  title     = "{CoCo}: Sound and Adaptive Replacement of Java Collections",
  author    = "Xu, Guoqing",
  booktitle = "ECOOP 2013 – Object-Oriented Programming",
  publisher = "Springer Berlin Heidelberg",
  pages     = "1--26",
  abstract  = "Inefficient use of Java containers is an important source of
               run-time inefficiencies in large applications. This paper
               presents an application-level dynamic optimization technique
               called CoCo, that exploits algorithmic advantages of Java
               collections to improve performance. CoCo dynamically identifies
               optimal Java collection objects and safely performs run-time
               collection replacement, both using pure Java code. At the heart
               of this technique is a framework that abstracts container
               elements to achieve efficiency and that concretizes abstractions
               to achieve soundness. We have implemented part of the Java
               collection framework as instances of this framework, and
               developed a static CoCo compiler to generate Java code that
               performs optimizations. This work is the first step towards
               achieving the ultimate goal of automatically optimizing away
               semantic inefficiencies.",
  year      =  2013,
  url       = "http://dx.doi.org/10.1007/978-3-642-39038-8_1"
}

@ARTICLE{Jung2011-tv,
  title     = "Brainy: effective selection of data structures",
  author    = "Jung, Changhee and Rus, Silvius and Railing, Brian P and Clark,
               Nathan and Pande, Santosh",
  journal   = "SIGPLAN Not.",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  volume    =  46,
  number    =  6,
  pages     = "86--97",
  abstract  = "Data structure selection is one of the most critical aspects of
               developing effective applications. By analyzing data structures'
               behavior and their interaction with the rest of the application
               on the underlying architecture, tools can make suggestions for
               alternative data structures better suited for the program input
               on which the application runs. Consequently, developers can
               optimize their data structure usage to make the application
               conscious of an underlying architecture and a particular program
               input.This paper presents the design and evaluation of Brainy, a
               new program analysis tool that automatically selects the best
               data structure for a given program and its input on a specific
               microarchitecture. The data structure's interface functions are
               instrumented to dynamically monitor how the data structure
               interacts with the application for a given input. The
               instrumentation records traces of various runtime characteristics
               including underlying architecture-specific events. These
               generated traces are analyzed and fed into an offline model,
               constructed using machine learning, to select the best data
               structure. That is, Brainy exploits runtime feedback of data
               structures to model the situation an application runs on, and
               selects the best data structure for a given
               application/input/architecture combination based on the
               constructed model. The empirical evaluation shows that this
               technique is highly accurate across several real-world
               applications with various program input sets on two different
               state-of-the-art microarchitectures. Consequently, Brainy
               achieved an average performance improvement of 27\% and 33\% on
               both microarchitectures, respectively.",
  month     =  jun,
  year      =  2011,
  url       = "https://doi.org/10.1145/1993316.1993509",
  keywords  = "application generator, data structure selection, performance
               counters, training framework"
}

@INPROCEEDINGS{Costa2018-pe,
  title     = "{CollectionSwitch}: a framework for efficient and dynamic
               collection selection",
  author    = "Costa, Diego and Andrzejak, Artur",
  booktitle = "Proceedings of the 2018 International Symposium on Code
               Generation and Optimization",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "16--26",
  abstract  = "Selecting collection data structures for a given application is a
               crucial aspect of the software development. Inefficient usage of
               collections has been credited as a major cause of performance
               bloat in applications written in Java, C++ and C\#. Furthermore,
               a single implementation might not be optimal throughout the
               entire program execution. This demands an adaptive solution that
               adjusts at runtime the collection implementations to varying
               workloads. We present CollectionSwitch, an application-level
               framework for efficient collection adaptation. It selects at
               runtime collection implementations in order to optimize the
               execution and memory performance of an application. Unlike
               previous works, we use workload data on the level of collection
               allocation sites to guide the optimization process. Our framework
               identifies allocation sites which instantiate suboptimal
               collection variants, and selects optimized variants for future
               instantiations. As a further contribution we propose adaptive
               collection implementations which switch their underlying data
               structures according to the size of the collection. We implement
               this framework in Java, and demonstrate the improvements in terms
               of time and memory behavior across a range of benchmarks. To our
               knowledge, it is the first approach which is capable of runtime
               performance optimization of Java collections with very low
               overhead.",
  series    = "CGO 2018",
  month     =  feb,
  year      =  2018,
  url       = "https://doi.org/10.1145/3168825",
  keywords  = "adaptive algorithms, data structure, optimization, performance"
}

@MISC{noauthor_undated-tm,
  title        = "The Koka Programming Language",
  abstract     = "Provides an automatically expanding table-of-contents in a
                  side-panel next to the main content. The TOC must be in a
                  `SidePanel` block, while all the other content must be in a
                  `MainPanel`.",
  howpublished = "\url{https://koka-lang.github.io/koka/doc/book.html}",
  note         = "Accessed: 2024-4-27",
  language     = "en"
}

@ARTICLE{Lorenzen2023-gw,
  title     = "{FP²}: Fully in-Place Functional Programming",
  author    = "Lorenzen, Anton and Leijen, Daan and Swierstra, Wouter",
  journal   = "Proc. ACM Program. Lang.",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  volume    =  7,
  number    = "ICFP",
  pages     = "275--304",
  abstract  = "As functional programmers we always face a dilemma: should we
               write purely functional code, or sacrifice purity for efficiency
               and resort to in-place updates? This paper identifies precisely
               when we can have the best of both worlds: a wide class of purely
               functional programs can be executed safely using in-place updates
               without requiring allocation, provided their arguments are not
               shared elsewhere. We describe a linear \_fully in-place\_ (FIP)
               calculus where we prove that we can always execute such functions
               in a way that requires no (de)allocation and uses constant stack
               space. Of course, such a calculus is only relevant if we can
               express interesting algorithms; we provide numerous examples of
               in-place functions on datastructures such as splay trees or
               finger trees, together with in-place versions of merge sort and
               quick sort. We also show how we can generically derive a map
               function over \_any\_ polynomial data type that is fully
               in-place. Finally, we have implemented the rules of the FIP
               calculus in the Koka language. Using the Perceus reference
               counting garbage collection, this implementation dynamically
               executes FIP functions in-place whenever possible.",
  month     =  aug,
  year      =  2023,
  url       = "https://doi.org/10.1145/3607840",
  keywords  = "FBIP, Tail Recursion Modulo Cons"
}

@MISC{noauthor_2020-vr,
  title        = "The Swift Runtime: Enums",
  booktitle    = "-dealloc",
  abstract     = "How is it that 'Optional' is the same size as 'AnyObject'?",
  month        =  oct,
  year         =  2020,
  howpublished = "\url{https://belkadan.com/blog/2020/10/Swift-Runtime-Enums/}",
  note         = "Accessed: 2024-4-26"
}

@MISC{Bartell-Mangel2022-tv,
  title        = "Filling a Niche: Using Spare Bits to Optimize Data
                  Representations",
  author       = "Bartell-Mangel, Noah Lev",
  month        =  jan,
  year         =  2022,
  url          = "https://www.noahlev.org/papers/popl22src-filling-a-niche.pdf",
  howpublished = "POPL 2022 Student Research Competition"
}

@MISC{noauthor_undated-nz,
  title        = "Enums - Unsafe Code Guidelines Reference",
  howpublished = "\url{https://rust-lang.github.io/unsafe-code-guidelines/layout/enums.html}",
  note         = "Accessed: 2024-4-26",
  language     = "en"
}

@BOOK{Bergman2011-mf,
  title     = "Universal Algebra: Fundamentals and Selected Topics",
  author    = "Bergman, Clifford",
  publisher = "CRC Press",
  abstract  = "Starting with the most basic notions, Universal Algebra:
               Fundamentals and Selected Topics introduces all the key elements
               needed to read and understand current research in this field.
               Based on the author's two-semester course, the text prepares
               students for research work by providing a solid grounding in the
               fundamental constructions and concepts o",
  month     =  sep,
  year      =  2011,
  url       = "https://play.google.com/store/books/details?id=snvRBQAAQBAJ",
  language  = "en"
}

@MISC{noauthor_undated-or,
  title        = "generated code · Wiki · Glasgow Haskell Compiler / {GHC}",
  booktitle    = "GitLab",
  abstract     = "The Glorious Glasgow Haskell Compiler.",
  howpublished = "\url{https://gitlab.haskell.org/ghc/ghc/-/wikis/commentary/compiler/generated-code}",
  note         = "Accessed: 2024-4-25",
  language     = "en"
}

@BOOK{Appel1991-ff,
  title     = "Compiling with Continuations",
  author    = "Appel, Andrew W",
  publisher = "Cambridge University Press",
  abstract  = "The control and data flow of a program can be represented using
               continuations, a concept from denotational semantics that has
               practical application in real compilers. This book shows how
               continuation-passing style is used as an intermediate
               representation on which to perform optimisations and program
               transformations. Continuations can be used to compile most
               programming languages. The method is illustrated in a compiler
               for the programming language Standard ML. However, prior
               knowledge of ML is not necessary, as the author carefully
               explains each concept as it arises. This is the first book to
               show how concepts from the theory of programming languages can be
               applied to the producton of practical optimising compilers for
               modern languages like ML. This book will be essential reading for
               compiler writers in both industry and academe, as well as for
               students and researchers in programming language theory.",
  month     =  nov,
  year      =  1991,
  url       = "https://www.cambridge.org/core/books/compiling-with-continuations/7CA9C36DCE78AD82218E745F43A4E740"
}

@INPROCEEDINGS{Peyton_Jones1989-hm,
  title     = "The spineless tagless {G}-machine",
  author    = "Peyton Jones, Simon L and Salkild, Jon",
  booktitle = "Proceedings of the fourth international conference on Functional
               programming languages and computer architecture",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "184--201",
  series    = "FPCA '89",
  month     =  nov,
  year      =  1989,
  url       = "https://doi.org/10.1145/99370.99385"
}

@INPROCEEDINGS{Kennedy2007-ll,
  title     = "Compiling with continuations, continued",
  author    = "Kennedy, Andrew",
  booktitle = "Proceedings of the 12th ACM SIGPLAN international conference on
               Functional programming",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "177--190",
  abstract  = "We present a series of CPS-based intermediate languages suitable
               for functional language compilation, arguing that they have
               practical benefits over direct-style languages based on A-normal
               form (ANF) or monads. Inlining of functions demonstrates the
               benefits most clearly: in ANF-based languages, inlining involves
               a re-normalization step that rearranges let expressions and
               possibly introduces a new 'join point' function, and in monadic
               languages, commuting conversions must be applied; in contrast,
               inlining in our CPS language is a simple substitution of
               variables for variables.We present a contification transformation
               implemented by simple rewrites on the intermediate language.
               Exceptions are modelled using so-called 'double-barrelled' CPS.
               Subtyping on exception constructors then gives a very
               straightforward effect analysis for exceptions. We also show how
               a graph-based representation of CPS terms can be implemented
               extremely efficiently, with linear-time term simplification.",
  series    = "ICFP '07",
  month     =  oct,
  year      =  2007,
  url       = "https://doi.org/10.1145/1291151.1291179",
  keywords  = "continuation passing style, continuations, functional programming
               languages, monads, optimizing compilation"
}

@ARTICLE{Wadler1990-yo,
  title    = "Deforestation: transforming programs to eliminate trees",
  author   = "Wadler, Philip",
  journal  = "Theor. Comput. Sci.",
  volume   =  73,
  number   =  2,
  pages    = "231--248",
  abstract = "An algorithm that transforms programs to eliminate intermediate
              trees is presented. The algorithm applies to any term containing
              only functions with definitions in a given syntactic form, and is
              suitable for incorporation in an optimizing compiler.",
  month    =  jun,
  year     =  1990,
  url      = "https://www.sciencedirect.com/science/article/pii/030439759090147A"
}

@PHDTHESIS{Lindley2008-rv,
  title    = "Normalisation by Evaluation in the Compilation of Typed Functional
              Programming Languages",
  author   = "Lindley, Sam",
  abstract = "This thesis presents a critical analysis of normalisation by
              evaluation as a technique for speeding up compilation of typed
              functional programming languages. Our investiga-tion focuses on
              the SML.NET compiler and its typed intermediate language MIL. We
              implement and measure the performance of normalisation by
              evaluation for MIL across a range of benchmarks. Taking a
              different approach, we also implement and measure the performance
              of a graph-based shrinking reductions algorithm for SML.NET. MIL
              is based on Moggi’s computational metalanguage. As a stepping
              stone to normalisation by evaluation, we investigate strong
              normalisation of the computational metalanguage by introducing an
              extension of Girard-Tait reducibility. Inspired by pre-vious work
              on local state and parametric polymorphism, we define reducibility
              for continuations and more generally reducibility for frame
              stacks. First we prove strong normalistion for the computational
              metalanguage. Then we extend that proof to in-clude features of
              MIL such as sums and exceptions. Taking an incremental approach,
              we construct a collection of increasingly sophisti-",
  month    =  dec,
  year     =  2008,
  url      = "https://www.researchgate.net/publication/239722678_Normalisation_by_Evaluation_in_the_Compilation_of_Typed_Functional_Programming_Languages"
}

@INPROCEEDINGS{Bell1997-xy,
  title     = "Type-driven defunctionalization",
  author    = "Bell, Jeffrey M and Bellegarde, Françoise and Hook, James",
  booktitle = "Proceedings of the second ACM SIGPLAN international conference on
               Functional programming",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "25--37",
  abstract  = "In 1972, Reynolds outlined a general method for eliminating
               functional arguments known as defunctionalization. The idea
               underlying defunctionalization is encoding a functional value as
               first-order data, and then realizing the applications of the
               encoded function via an apply function. Although this process is
               simple enough, problems arise when defunctionalization is used in
               a polymorphic language. In such a language, a functional argument
               of a higher-order function can take different type instances in
               different applications. As a consequence, its associated apply
               function can be untypable in the source language. In the paper we
               present a defunctionalization transformation which preserves
               typability. Moreover, the transformation imposes no restriction
               on functional arguments of recursive functions, and it handles
               functions as results as well as functions encapsulated in
               constructors. The key to this success is the use of type
               information in the defunctionalization transformation. Run-time
               characteristics are preserved by defunctionalization; hence,
               there is no performance improvement coming from the
               transformation itself. However closures need not be implemented
               to compile the transformed program.",
  series    = "ICFP '97",
  month     =  aug,
  year      =  1997,
  url       = "https://doi.org/10.1145/258948.258953"
}

@INPROCEEDINGS{Reynolds1972-kz,
  title     = "Definitional interpreters for higher-order programming languages",
  author    = "Reynolds, John C",
  booktitle = "Proceedings of the ACM annual conference - Volume 2",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "717--740",
  abstract  = "Higher-order programming languages (i.e., languages in which
               procedures or labels can occur as values) are usually defined by
               interpreters which are themselves written in a programming
               language based on the lambda calculus (i.e., an applicative
               language such as pure LISP). Examples include McCarthy's
               definition of LISP, Landin's SECD machine, the Vienna definition
               of PL/I, Reynolds' definitions of GEDANKEN, and recent
               unpublished work by L. Morris and C. Wadsworth. Such definitions
               can be classified according to whether the interpreter contains
               higher-order functions, and whether the order of application
               (i.e., call-by-value versus call-by-name) in the defined language
               depends upon the order of application in the defining language.
               As an example, we consider the definition of a simple applicative
               programming language by means of an interpreter written in a
               similar language. Definitions in each of the above
               classifications are derived from one another by informal but
               constructive methods. The treatment of imperative features such
               as jumps and assignment is also discussed.",
  series    = "ACM '72",
  month     =  aug,
  year      =  1972,
  url       = "https://doi.org/10.1145/800194.805852",
  keywords  = "Applicative language, Closure, Continuation, GEDANKEN,
               Higher-order function, Interpreter, J-operator, LISP, Lambda
               calculus, Language definition, Order of application, PAL,
               Programming language, Reference, SECD machine"
}

@ARTICLE{Cockx2018-fk,
  title     = "Elaborating dependent (co)pattern matching",
  author    = "Cockx, Jesper and Abel, Andreas",
  journal   = "Proc. ACM Program. Lang.",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  volume    =  2,
  number    = "ICFP",
  pages     = "1--30",
  abstract  = "In a dependently typed language, we can guarantee correctness of
               our programs by providing formal proofs. To check them, the
               typechecker elaborates these programs and proofs into a low level
               core language. However, this core language is by nature hard to
               understand by mere humans, so how can we know we proved the right
               thing? This question occurs in particular for dependent copattern
               matching, a powerful language construct for writing programs and
               proofs by dependent case analysis and mixed
               induction/coinduction. A definition by copattern matching
               consists of a list of clauses that are elaborated to a case tree,
               which can be further translated to primitive eliminators. In
               previous work this second step has received a lot of attention,
               but the first step has been mostly ignored so far. We present an
               algorithm elaborating definitions by dependent copattern matching
               to a core language with inductive datatypes, coinductive record
               types, an identity type, and constants defined by well-typed case
               trees. To ensure correctness, we prove that elaboration preserves
               the first-match semantics of the user clauses. Based on this
               theoretical work, we reimplement the algorithm used by Agda to
               check left-hand sides of definitions by pattern matching. The new
               implementation is at the same time more general and less complex,
               and fixes a number of bugs and usability issues with the old
               version. Thus we take another step towards the formally verified
               implementation of a practical dependently typed language.",
  month     =  jul,
  year      =  2018,
  url       = "https://doi.org/10.1145/3236770",
  keywords  = "Agda, Copatterns, Dependent pattern matching, Dependent types"
}

@BOOK{Frankel2004-yk,
  title     = "The geometry of physics: An introduction",
  author    = "Frankel, Theodore",
  publisher = "Cambridge University Press",
  address   = "Cambridge, England",
  edition   =  2,
  abstract  = "This book provides a working knowledge of those parts of exterior
               differential forms, differential geometry, algebraic and
               differential topology, Lie groups, vector bundles and Chern forms
               that are essential for a deeper understanding of both classical
               and modern physics and engineering. Included are discussions of
               analytical and fluid dynamics, electromagnetism (in flat and
               curved space), thermodynamics, the deformation tensors of
               elasticity, soap films, special and general relativity, the Dirac
               operator and spinors, and gauge fields, including Yang-Mills, the
               Aharonov-Bohm effect, Berry phase, and instanton winding numbers,
               quarks, and quark model for mesons. Before discussing abstract
               notions of differential geometry, geometric intuition is
               developed through a rather extensive introduction to the study of
               surfaces in ordinary space; consequently, the book should be of
               interest also to mathematics students. Ideal for graduate and
               advanced undergraduate students of physics, engineering and
               mathematics as a course text or for self study.",
  year      =  2004,
  url       = "http://dx.doi.org/10.1017/cbo9780511817977",
  language  = "en"
}

@ARTICLE{De_Jong_undated-yh,
  title  = "Categorical Realisability",
  author = "de Jong, Tom"
}

@ARTICLE{Barwell2018-pg,
  title     = "Finding parallel functional pearls: Automatic parallel recursion
               scheme detection in Haskell functions via anti-unification",
  author    = "Barwell, Adam D and Brown, Christopher and Hammond, Kevin",
  journal   = "Future Gener. Comput. Syst.",
  publisher = "Elsevier BV",
  volume    =  79,
  pages     = "669--686",
  month     =  feb,
  year      =  2018,
  url       = "http://dx.doi.org/10.1016/j.future.2017.07.024",
  language  = "en"
}

@ARTICLE{Villani2024-xj,
  title         = "The Topos of Transformer Networks",
  author        = "Villani, Mattia Jacopo and McBurney, Peter",
  journal       = "arXiv [cs.LG]",
  abstract      = "The transformer neural network has significantly out-shined
                   all other neural network architectures as the engine behind
                   large language models. We provide a theoretical analysis of
                   the expressivity of the transformer architecture through the
                   lens of topos theory. From this viewpoint, we show that many
                   common neural network architectures, such as the
                   convolutional, recurrent and graph convolutional networks,
                   can be embedded in a pretopos of piecewise-linear functions,
                   but that the transformer necessarily lives in its topos
                   completion. In particular, this suggests that the two network
                   families instantiate different fragments of logic: the former
                   are first order, whereas transformers are higher-order
                   reasoners. Furthermore, we draw parallels with architecture
                   search and gradient descent, integrating our analysis in the
                   framework of cybernetic agents.",
  month         =  mar,
  year          =  2024,
  url           = "http://arxiv.org/abs/2403.18415",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG"
}

@ARTICLE{Cruttwell2021-se,
  title         = "Categorical Foundations of Gradient-Based Learning",
  author        = "Cruttwell, G S H and Gavranović, Bruno and Ghani, Neil and
                   Wilson, Paul and Zanasi, Fabio",
  journal       = "arXiv [cs.LG]",
  abstract      = "We propose a categorical semantics of gradient-based machine
                   learning algorithms in terms of lenses, parametrised maps,
                   and reverse derivative categories. This foundation provides a
                   powerful explanatory and unifying framework: it encompasses a
                   variety of gradient descent algorithms such as ADAM, AdaGrad,
                   and Nesterov momentum, as well as a variety of loss functions
                   such as as MSE and Softmax cross-entropy, shedding new light
                   on their similarities and differences. Our approach to
                   gradient-based learning has examples generalising beyond the
                   familiar continuous domains (modelled in categories of smooth
                   maps) and can be realized in the discrete setting of boolean
                   circuits. Finally, we demonstrate the practical significance
                   of our framework with an implementation in Python.",
  month         =  mar,
  year          =  2021,
  url           = "http://arxiv.org/abs/2103.01931",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG"
}

@ARTICLE{Fong2017-bn,
  title         = "Backprop as Functor: A compositional perspective on
                   supervised learning",
  author        = "Fong, Brendan and Spivak, David I and Tuyéras, Rémy",
  journal       = "arXiv [math.CT]",
  abstract      = "A supervised learning algorithm searches over a set of
                   functions $A \to B$ parametrised by a space $P$ to find the
                   best approximation to some ideal function $f\colon A \to B$.
                   It does this by taking examples $(a,f(a)) \in A\times B$, and
                   updating the parameter according to some rule. We define a
                   category where these update rules may be composed, and show
                   that gradient descent---with respect to a fixed step size and
                   an error function satisfying a certain property---defines a
                   monoidal functor from a category of parametrised functions to
                   this category of update rules. This provides a structural
                   perspective on backpropagation, as well as a broad
                   generalisation of neural networks.",
  month         =  nov,
  year          =  2017,
  url           = "http://arxiv.org/abs/1711.10455",
  archivePrefix = "arXiv",
  primaryClass  = "math.CT"
}

@ARTICLE{Gavranovic2024-dc,
  title         = "Categorical Deep Learning: An Algebraic Theory of
                   Architectures",
  author        = "Gavranović, Bruno and Lessard, Paul and Dudzik, Andrew and
                   von Glehn, Tamara and Araújo, João G M and Veličković, Petar",
  journal       = "arXiv [cs.LG]",
  abstract      = "We present our position on the elusive quest for a
                   general-purpose framework for specifying and studying deep
                   learning architectures. Our opinion is that the key attempts
                   made so far lack a coherent bridge between specifying
                   constraints which models must satisfy and specifying their
                   implementations. Focusing on building a such a bridge, we
                   propose to apply category theory -- precisely, the universal
                   algebra of monads valued in a 2-category of parametric maps
                   -- as a single theory elegantly subsuming both of these
                   flavours of neural network design. To defend our position, we
                   show how this theory recovers constraints induced by
                   geometric deep learning, as well as implementations of many
                   architectures drawn from the diverse landscape of neural
                   networks, such as RNNs. We also illustrate how the theory
                   naturally encodes many standard constructs in computer
                   science and automata theory.",
  month         =  feb,
  year          =  2024,
  url           = "http://arxiv.org/abs/2402.15332",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG"
}

@ARTICLE{Dybjer1994-zx,
  title     = "Inductive families",
  author    = "Dybjer, Peter",
  journal   = "Form. Asp. Comput.",
  publisher = "Association for Computing Machinery",
  volume    =  6,
  number    =  4,
  pages     = "440--465",
  abstract  = "A general formulation of inductive and recursive definitions in
               Martin-Löf's type theory is presented. It extends Backhouse's
               ‘Do-It-Yourself Type Theory’ to include inductive definitions of
               families of sets and definitions of functions by recursion on the
               way elements of such sets are generated. The formulation is in
               natural deduction and is intended to be a natural generalisation
               to type theory of Martin-Löf's theory of iterated inductive
               definitions in predicate logic. Formal criteria are given for
               correct formation and introduction rules of a new set former
               capturing definition by strictly positive, iterated, generalised
               induction. Moreover, there is an inversion principle for deriving
               elimination and equality rules from the formation and
               introduction rules. Finally, there is an alternative schematic
               presentation of definition by recursion. The resulting theory is
               a flexible and powerful language for programming and constructive
               mathematics. We hint at the wealth of possible applications by
               showing several basic examples: predicate logic, generalised
               induction, and a formalisation of the untyped lambda calculus.",
  month     =  jan,
  year      =  1994,
  url       = "https://www.researchgate.net/publication/226035566_Inductive_families"
}

@ARTICLE{Allais2023-zq,
  title         = "Seamless, Correct, and Generic Programming over Serialised
                   Data",
  author        = "Allais, Guillaume",
  journal       = "arXiv [cs.PL]",
  abstract      = "In typed functional languages, one can typically only
                   manipulate data in a type-safe manner if it first has been
                   deserialised into an in-memory tree represented as a graph of
                   nodes-as-structs and subterms-as-pointers. We demonstrate how
                   we can use QTT as implemented in \idris{} to define a small
                   universe of serialised datatypes, and provide generic
                   programs allowing users to process values stored contiguously
                   in buffers. Our approach allows implementors to prove the
                   full functional correctness by construction of the IO
                   functions processing the data stored in the buffer.",
  month         =  oct,
  year          =  2023,
  url           = "http://arxiv.org/abs/2310.13441",
  archivePrefix = "arXiv",
  primaryClass  = "cs.PL"
}

@INPROCEEDINGS{Allais2023-pf,
  title     = "Builtin Types Viewed as Inductive Families",
  author    = "Allais, Guillaume",
  booktitle = "Programming Languages and Systems",
  publisher = "Springer Nature Switzerland",
  pages     = "113--139",
  abstract  = "State of the art optimisation passes for dependently typed
               languages can help erase the redundant information typical of
               invariant-rich data structures and programs. These automated
               processes do not dramatically change the structure of the data,
               even though more efficient representations could be available.",
  year      =  2023,
  url       = "http://dx.doi.org/10.1007/978-3-031-30044-8_5"
}

@MISC{Kaposi2020-is,
  title     = "A syntax for mutual inductive families",
  author    = "Kaposi, Ambrus and von Raumer, Jakob",
  publisher = "Schloss Dagstuhl - Leibniz-Zentrum für Informatik",
  abstract  = "Inductive families of types are a feature of most languages based
               on dependent types. They are usually described either by
               syntactic schemes or by encodings of strictly positive functors
               such as combinator languages or containers. The former approaches
               are informal and give only external signatures, the latter
               approaches suffer from encoding overheads and do not directly
               represent mutual types. In this paper we propose a direct method
               for describing signatures for mutual inductive families using a
               domain-specific type theory. A signature is a context (roughly
               speaking, a list of types) in this small type theory. Algebras,
               displayed algebras and sections are defined by models of this
               type theory: the standard model, the logical predicate and a
               logical relation interpretation, respectively. We reduce the
               existence of initial algebras for these signatures to the
               existence of the syntax of our domain-specific type theory. As
               this theory is very simple, its normal syntax can be encoded
               using indexed W-types. To the best of our knowledge, this is the
               first formalisation of the folklore fact that mutual inductive
               types can be reduced to indexed W-types. The contents of this
               paper were formalised in the proof assistant Agda.",
  month     =  jun,
  year      =  2020,
  url       = "http://dx.doi.org/10.4230/LIPIcs.FSCD.2020.23"
}

@ARTICLE{Baudon2023-cy,
  title     = "Bit-Stealing Made Legal: Compilation for Custom Memory
               Representations of Algebraic Data Types",
  author    = "Baudon, Thaïs and Radanne, Gabriel and Gonnord, Laure",
  journal   = "Proc. ACM Program. Lang.",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  volume    =  7,
  number    = "ICFP",
  pages     = "813--846",
  abstract  = "Initially present only in functional languages such as OCaml and
               Haskell, Algebraic Data Types (ADTs) have now become pervasive in
               mainstream languages, providing nice data abstractions and an
               elegant way to express functions through pattern matching.
               Unfortunately, ADTs remain seldom used in low-level programming.
               One reason is that their increased convenience comes at the cost
               of abstracting away the exact memory layout of values. Even Rust,
               which tries to optimize data layout, severely limits control over
               memory representation. In this article, we present a new approach
               to specify the data layout of rich data types based on a dual
               view: a source type, providing a high-level description available
               in the rest of the code, along with a memory type, providing full
               control over the memory layout. This dual view allows for better
               reasoning about memory layout, both for correctness, with
               dedicated validity criteria linking the two views, and for
               optimizations that manipulate the memory view. We then provide
               algorithms to compile constructors and destructors, including
               pattern matching, to their low-level memory representation. We
               prove our compilation algorithms correct, implement them in a
               tool called ribbit that compiles to LLVM IR, and show some early
               experimental results.",
  month     =  aug,
  year      =  2023,
  url       = "https://doi.org/10.1145/3607858",
  keywords  = "Algebraic Data Types, Compilation, Data Layouts, Pattern Matching"
}

@INPROCEEDINGS{Hinze2011-jx,
  title     = "Type Fusion",
  author    = "Hinze, Ralf",
  booktitle = "Algebraic Methodology and Software Technology",
  publisher = "Springer Berlin Heidelberg",
  pages     = "92--110",
  abstract  = "Fusion is an indispensable tool in the arsenal of techniques for
               program derivation. Less well-known, but equally valuable is type
               fusion, which states conditions for fusing an application of a
               functor with an initial algebra to form another initial algebra.
               We provide a novel proof of type fusion based on adjoint folds
               and discuss several applications: type firstification, type
               specialisation and tabulation.",
  year      =  2011,
  url       = "http://dx.doi.org/10.1007/978-3-642-17796-5_6"
}

@INPROCEEDINGS{Shao1994-kp,
  title     = "Unrolling lists",
  author    = "Shao, Zhong and Reppy, John H and Appel, Andrew W",
  booktitle = "Proceedings of the 1994 ACM conference on LISP and functional
               programming",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "185--195",
  abstract  = "Lists are ubiquitous in functional programs, thus supporting
               lists efficiently is a major concern to compiler writers for
               functional languages. Lists are normally represented as linked
               cons cells, with each cons cell containing a car (the data) and a
               cdr (the link); this is inefficient in the use of space, because
               50\% of the storage is used for links. Loops and recursions on
               lists are slow on modern machines because of the long chains of
               control dependencies (in checking for nil) and data dependencies
               (in fetching cdr fields).We present a data structure for
               “unrolled lists”, where each cell has several data items (car
               fields) and one link (cdr). This reduces the memory used for
               links, and it significantly shortens the length of
               control-dependence and data-dependence chains in operations on
               lists.We further present an efficient compile-time analysis that
               transforms programs written for “ordinary” lists into programs on
               unrolled lists. The use of our new representation requires no
               change to existing programs.We sketch the proof of soundness of
               our analysis—which is based on refinement types—and present some
               preliminary measurements of our technique.",
  series    = "LFP '94",
  month     =  jul,
  year      =  1994,
  url       = "https://doi.org/10.1145/182409.182453"
}

@ARTICLE{Awodey2014-hh,
  title         = "Natural models of homotopy type theory",
  author        = "Awodey, Steve",
  journal       = "arXiv [math.CT]",
  abstract      = "The notion of a natural model of type theory is defined in
                   terms of that of a representable natural transfomation of
                   presheaves. It is shown that such models agree exactly with
                   the concept of a category with families in the sense of
                   Dybjer, which can be regarded as an algebraic formulation of
                   type theory. We determine conditions for such models to
                   satisfy the inference rules for dependent sums, dependent
                   products, and intensional identity types, as used in homotopy
                   type theory. It is then shown that a category admits such a
                   model if it has a class of maps that behave like the abstract
                   fibrations in axiomatic homotopy theory: they should be
                   stable under pullback, closed under composition and relative
                   products, and there should be weakly orthogonal
                   factorizations into the class. It follows that many familiar
                   settings for homotopy theory also admit natural models of the
                   basic system of homotopy type theory.",
  month         =  jun,
  year          =  2014,
  url           = "http://arxiv.org/abs/1406.3219",
  archivePrefix = "arXiv",
  primaryClass  = "math.CT"
}

@INPROCEEDINGS{Taha1997-ag,
  title     = "Multi-stage programming with explicit annotations",
  author    = "Taha, Walid and Sheard, Tim",
  booktitle = "Proceedings of the 1997 ACM SIGPLAN symposium on Partial
               evaluation and semantics-based program manipulation",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "203--217",
  abstract  = "We introduce MetaML, a statically-typed multi-stage programming
               language extending Nielson and Nielson's two stage notation to an
               arbitrary number of stages. MetaML extends previous work by
               introducing four distinct staging annotations which generalize
               those published previously [25, 12, 7, 6]We give a static
               semantics in which type checking is done once and for all before
               the first stage, and a dynamic semantics which introduces a new
               concept of cross-stage persistence, which requires that variables
               available in any stage are also available in all future stages.We
               illustrate that staging is a manual form of binding time
               analysis. We explain why, even in the presence of automatic
               binding time analysis, explicit annotations are useful,
               especially for programs with more than two stages.A thesis of
               this paper is that multi-stage languages are useful as
               programming languages in their own right, and should support
               features that make it possible for programmers to write staged
               computations without significantly changing their normal
               programming style. To illustrate this we provide a simple three
               stage example, and an extended two-stage example elaborating a
               number of practical issues.",
  series    = "PEPM '97",
  month     =  dec,
  year      =  1997,
  url       = "https://doi.org/10.1145/258993.259019"
}

@ARTICLE{Annenkov2017-pd,
  title         = "Two-Level Type Theory and Applications",
  author        = "Annenkov, Danil and Capriotti, Paolo and Kraus, Nicolai and
                   Sattler, Christian",
  journal       = "arXiv [cs.LO]",
  abstract      = "We define and develop two-level type theory (2LTT), a version
                   of Martin-Lof type theory which combines two different type
                   theories. We refer to them as the inner and the outer type
                   theory. In our case of interest, the inner theory is homotopy
                   type theory (HoTT) which may include univalent universes and
                   higher inductive types. The outer theory is a traditional
                   form of type theory validating uniqueness of identity proofs
                   (UIP). One point of view on it is as internalised meta-theory
                   of the inner type theory. There are two motivations for 2LTT.
                   Firstly, there are certain results about HoTT which are of
                   meta-theoretic nature, such as the statement that
                   semisimplicial types up to level $n$ can be constructed in
                   HoTT for any externally fixed natural number $n$. Such
                   results cannot be expressed in HoTT itself, but they can be
                   formalised and proved in 2LTT, where $n$ will be a variable
                   in the outer theory. This point of view is inspired by
                   observations about conservativity of presheaf models.
                   Secondly, 2LTT is a framework which is suitable for
                   formulating additional axioms that one might want to add to
                   HoTT. This idea is heavily inspired by Voevodsky's Homotopy
                   Type System (HTS), which constitutes one specific instance of
                   a 2LTT. HTS has an axiom ensuring that the type of natural
                   numbers behaves like the external natural numbers, which
                   allows the construction of a universe of semisimplicial
                   types. In 2LTT, this axiom can be stated simply be asking the
                   inner and outer natural numbers to be isomorphic. After
                   defining 2LTT, we set up a collection of tools with the goal
                   of making 2LTT a convenient language for future developments.
                   As a first such application, we develop the theory of Reedy
                   fibrant diagrams in the style of Shulman. Continuing this
                   line of thought, we suggest a definition of
                   (infinity,1)-category and give some examples.",
  month         =  may,
  year          =  2017,
  url           = "http://arxiv.org/abs/1705.03307",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LO"
}

@ARTICLE{Ayoun2024-br,
  title         = "A hybrid approach to semi-automated Rust verification",
  author        = "Ayoun, Sacha-Élie and Denis, Xavier and Maksimović, Petar and
                   Gardner, Philippa",
  journal       = "arXiv [cs.PL]",
  abstract      = "While recent years have been witness to a large body of work
                   on efficient and automated verification of safe Rust code,
                   enabled by the rich guarantees of the Rust type system, much
                   less progress has been made on reasoning about unsafe code
                   due to its unique complexities. We propose a hybrid approach
                   to end-to-end Rust verification in which powerful automated
                   verification of safe Rust is combined with targeted
                   semi-automated verification of unsafe~Rust. To this end, we
                   present Gillian-Rust, a proof-of-concept semi-automated
                   verification tool that is able to reason about type safety
                   and functional correctness of unsafe~code. Built on top of
                   the Gillian parametric compositional verification platform,
                   Gillian-Rust automates a rich separation logic for real-world
                   Rust, embedding the lifetime logic of RustBelt and the
                   parametric propheciees of RustHornBelt. Using the unique
                   extensibility of Gillian, our novel encoding of these
                   features is fine-tuned to maximise automation and exposes a
                   user-friendly API, allowing for low-effort verification of
                   unsafe code. We link Gillian-Rust with Creusot, a
                   state-of-the-art verifier for safe Rust, by providing a
                   systematic encoding of unsafe code specifications that
                   Creusot may use but not verify, demonstrating the feasibility
                   of our hybrid~approach.",
  month         =  mar,
  year          =  2024,
  url           = "http://arxiv.org/abs/2403.15122",
  archivePrefix = "arXiv",
  primaryClass  = "cs.PL"
}

@ARTICLE{Altenkirch2015-yl,
  title     = "Indexed containers",
  author    = "Altenkirch, Thorsten and Ghani, Neil and Hancock, Peter and
               Mcbride, Conor and Morris, Peter",
  journal   = "J. Funct. Programming",
  publisher = "Cambridge University Press",
  volume    =  25,
  pages     = "e5",
  abstract  = "We show that the syntactically rich notion of strictly positive
               families can be reduced to a core type theory with a fixed number
               of type constructors exploiting the novel notion of indexed
               containers. As a result, we show indexed containers provide
               normal forms for strictly positive families in much the same way
               that containers provide normal forms for strictly positive types.
               Interestingly, this step from containers to indexed containers is
               achieved without having to extend the core type theory. Most of
               the construction presented here has been formalized using the
               Agda system.",
  month     =  jan,
  year      =  2015,
  url       = "https://www.cambridge.org/core/services/aop-cambridge-core/content/view/FB9C7DC88A65E7529D39554379D9765F/S095679681500009Xa.pdf/div-class-title-indexed-containers-div.pdf"
}

@ARTICLE{Goncharov2022-pc,
  title         = "Towards a higher-order mathematical operational semantics",
  author        = "Goncharov, Sergey and Milius, Stefan and Schröder, Lutz and
                   Tsampas, Stelios and Urbat, Henning",
  journal       = "arXiv [cs.LO]",
  abstract      = "Compositionality proofs in higher-order languages are
                   notoriously involved, and general semantic frameworks
                   guaranteeing compositionality are hard to come by. In
                   particular, Turi and Plotkin's bialgebraic abstract GSOS
                   framework, which has been successfully applied to obtain
                   off-the-shelf compositionality results for first-order
                   languages, so far does not apply to higher-order languages.
                   In the present work, we develop a theory of abstract GSOS
                   specifications for higher-order languages, in effect
                   transferring the core principles of Turi and Plotkin's
                   framework to a higher-order setting. In our theory, the
                   operational semantics of higher-order languages is
                   represented by certain dinatural transformations that we term
                   pointed higher-order GSOS laws. We give a general
                   compositionality result that applies to all systems specified
                   in this way and discuss how compositionality of the SKI
                   calculus and the $\lambda$-calculus w.r.t. a strong variant
                   of Abramsky's applicative bisimilarity are obtained as
                   instances.",
  month         =  oct,
  year          =  2022,
  url           = "http://arxiv.org/abs/2210.13387",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LO"
}

@TECHREPORT{Benaissa1998-et,
  title     = "A categorical analysis of multi-level languages (extended
               abstract)",
  author    = "Benaissa, Zine E and Moggi, Eugenio and Taha, Walid and Sheard,
               Tim",
  publisher = "Oregon Graduate Institute School of Science \& Engineering",
  abstract  = "We propose categorical models for lambda-circle, lambda-box,
               MetaML, and AIM. First, we focus on the underlying logical
               modalities and the interactions between them, then we investigate
               the interactions between logical modalities and computational
               monads. We give two examples of categorical model: one simpler
               but with some limitations, the other more complex but able to
               model all features of AIM.",
  month     =  nov,
  year      =  1998,
  url       = "https://dl.acm.org/doi/10.5555/890873"
}

@MISC{Szamozvancev_undated-rb,
  title       = "ppx\_deriving: Type-driven code generation for {OCaml}",
  author      = "Szamozvancev, Dmitrij and White, Leo and Xie, Ningning and
                 Yallop, Jeremy",
  institution = "Github",
  abstract    = "Type-driven code generation for OCaml. Contribute to
                 ocaml-ppx/ppx\_deriving development by creating an account on
                 GitHub.",
  url         = "https://github.com/ocaml-ppx/ppx_deriving",
  language    = "en"
}

@ARTICLE{Yallop2017-cg,
  title     = "Staged generic programming",
  author    = "Yallop, Jeremy",
  journal   = "Proc. ACM Program. Lang.",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  volume    =  1,
  number    = "ICFP",
  pages     = "1--29",
  abstract  = "Generic programming libraries such as Scrap Your Boilerplate
               eliminate the need to write repetitive code, but typically
               introduce significant performance overheads. This leaves
               programmers with the regrettable choice between writing succinct
               but slow programs and writing tedious but efficient programs.
               Applying structured multi-stage programming techniques transforms
               Scrap Your Boilerplate from an inefficient library into a typed
               optimising code generator, bringing its performance in line with
               hand-written code, and so combining high-level programming with
               uncompromised performance.",
  month     =  aug,
  year      =  2017,
  url       = "https://doi.org/10.1145/3110273",
  keywords  = "generic programming, metaprogramming, multi-stage programming,
               partial evaluation"
}

@ARTICLE{Huang2023-hb,
  title     = "Defunctionalization with Dependent Types",
  author    = "Huang, Yulong and Yallop, Jeremy",
  journal   = "Proc. ACM Program. Lang.",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  volume    =  7,
  number    = "PLDI",
  pages     = "516--538",
  abstract  = "The defunctionalization translation that eliminates higher-order
               functions from programs forms a key part of many compilers.
               However, defunctionalization for dependently-typed languages has
               not been formally studied. We present the first
               formally-specified defunctionalization translation for a
               dependently-typed language and establish key metatheoretical
               properties such as soundness and type preservation. The
               translation is suitable for incorporation into type-preserving
               compilers for dependently-typed languages",
  month     =  jun,
  year      =  2023,
  url       = "https://doi.org/10.1145/3591241",
  keywords  = "type systems, type preservation, dependent types, compilation"
}

@INPROCEEDINGS{Boulier2017-cm,
  title     = "The next 700 syntactical models of type theory",
  author    = "Boulier, Simon and Pédrot, Pierre-Marie and Tabareau, Nicolas",
  booktitle = "Proceedings of the 6th ACM SIGPLAN Conference on Certified
               Programs and Proofs",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "182--194",
  abstract  = "A family of syntactic models for the calculus of construction
               with universes (CCω) is described, all of them preserving
               conversion of the calculus definitionally, and thus giving rise
               directly to a program transformation of CCω into itself. Those
               models are based on the remark that negative type constructors
               (e.g. dependent product, coinductive types or universes) are
               underspecified in type theory-which leaves some freedom on extra
               intensional specifications. The model construction can be seen as
               a compilation phase from a complex type theory into a simpler
               type theory. Such models can be used to derive (the negative part
               of) independence results with respect to CCω, such as functional
               extensionality, propositional extensionality, univalence or the
               fact that bisimulation on a coinductive type may not coincide
               with equality. They can also be used to add new principles to the
               theory, which we illustrate by defining a version of CCω with
               ad-hoc polymorphism that shows in particular that parametricity
               is not an implicit requirement of type theory. The correctness of
               some of the models/program transformations have been checked in
               the Coq proof assistant and have been instrumented as a Coq
               plugin.",
  series    = "CPP 2017",
  month     =  jan,
  year      =  2017,
  url       = "https://doi.org/10.1145/3018610.3018620",
  keywords  = "Dependent type theory, Program translation"
}

@ARTICLE{Boulier2018-zy,
  title    = "Extending type theory with syntactic models",
  author   = "Boulier, S",
  abstract = "This thesis is about the metatheory of intuitionnistic type
              theory. The considered systems are variants of Martin-Lof type
              theory of Calculus of Constructions, and we are interested in the
              coherence of those systems and in the independence of axioms with
              respect to those systems. The common theme of this thesis is the
              construction of syntactic models, which are models reusing type
              theory to interpret type theory. In a first part, we introduce
              type theory by a minimal system and several possible extensions.
              In a second part, we introduce the syntactic models given by
              program translation and give several examples. In a third part, we
              present Template-Coq, a plugin for metaprogramming in Coq. We
              demonstrate how to use it to implement directly some syntactic
              models. Last, we consider type theories with two equalities: one
              strict and one univalent. We propose a re-reading of works of
              Coquand et.al. and of Orton and Pitts on the cubical model by
              introducing degenerate fibrancy.",
  month    =  nov,
  year     =  2018,
  url      = "https://theses.hal.science/tel-02007839/document/"
}

@INPROCEEDINGS{Altenkirch2011-ji,
  title     = "A Categorical Semantics for Inductive-Inductive Definitions",
  author    = "Altenkirch, Thorsten and Morris, Peter and Nordvall Forsberg,
               Fredrik and Setzer, Anton",
  booktitle = "Algebra and Coalgebra in Computer Science",
  publisher = "Springer Berlin Heidelberg",
  pages     = "70--84",
  abstract  = "Induction-induction is a principle for defining data types in
               Martin-Löf Type Theory. An inductive-inductive definition
               consists of a set A, together with an A-indexed family B : A →
               Set, where both A and B are inductively defined in such a way
               that the constructors for A can refer to B and vice versa. In
               addition, the constructors for B can refer to the constructors
               for A. We extend the usual initial algebra semantics for ordinary
               inductive data types to the inductive-inductive setting by
               considering dialgebras instead of ordinary algebras. This gives a
               new and compact formalisation of inductive-inductive definitions,
               which we prove is equivalent to the usual formulation with
               elimination rules.",
  year      =  2011,
  url       = "http://dx.doi.org/10.1007/978-3-642-22944-2_6"
}

@ARTICLE{Reitz2024-iq,
  title         = "{StarMalloc}: A formally verified, concurrent, performant,
                   and security-oriented memory allocator",
  author        = "Reitz, Antonin and Fromherz, Aymeric and Protzenko, Jonathan",
  journal       = "arXiv [cs.PL]",
  abstract      = "In this work, we present StarMalloc, a verified,
                   security-oriented, concurrent memory allocator that can be
                   used as a drop-in replacement in real-world projects. Using
                   the Steel separation logic framework, we show how to specify
                   and verify StarMalloc, relying on dependent types and modular
                   abstractions to enable efficient verification. As part of
                   StarMalloc, we also develop several generic datastructures
                   and proof libraries directly reusable in future systems
                   verification projects. We finally show that StarMalloc can be
                   used with real-world projects, including the Firefox browser,
                   and evaluate it against 10 state-of-the-art memory
                   allocators, demonstrating its competitiveness.",
  month         =  mar,
  year          =  2024,
  url           = "http://arxiv.org/abs/2403.09435",
  archivePrefix = "arXiv",
  primaryClass  = "cs.PL"
}

@ARTICLE{Zilberstein2023-eo,
  title         = "Outcome Separation Logic: Local reasoning for correctness and
                   incorrectness with computational effects",
  author        = "Zilberstein, Noam and Saliling, Angelina and Silva, Alexandra",
  journal       = "arXiv [cs.LO]",
  abstract      = "Separation logic's compositionality and local reasoning
                   properties have led to significant advances in scalable
                   static analysis. But program analysis has new challenges --
                   many programs display computational effects and,
                   orthogonally, static analyzers must handle incorrectness too.
                   We present Outcome Separation Logic (OSL), a program logic
                   that is sound for both correctness and incorrectness
                   reasoning in programs with varying effects. OSL has a frame
                   rule -- just like separation logic -- but uses different
                   underlying assumptions that open up local reasoning to a
                   larger class of properties than can be handled by any single
                   existing logic. Building on this foundational theory, we also
                   define symbolic execution algorithms that use bi-abduction to
                   derive specifications for programs with effects. This
                   involves a new tri-abduction procedure to analyze programs
                   whose execution branches due to effects such as
                   nondeterministic or probabilistic choice. This work furthers
                   the compositionality promised by separation logic by opening
                   up the possibility for greater reuse of analysis tools across
                   two dimensions: bug-finding vs verification in programs with
                   varying effects.",
  month         =  may,
  year          =  2023,
  url           = "http://arxiv.org/abs/2305.04842",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LO"
}

@INCOLLECTION{Bunkenburg1994-wk,
  title     = "The Boom Hierarchy",
  author    = "Bunkenburg, Alexander",
  editor    = "O'Donnell, John T and Hammond, Kevin",
  booktitle = "Functional Programming, Glasgow 1993: Proceedings of the 1993
               Glasgow Workshop on Functional Programming, Ayr, Scotland, 5–7
               July 1993",
  publisher = "Springer London",
  address   = "London",
  pages     = "1--8",
  abstract  = "The Boom Hierarchy is the family of data structures tree, list,
               bag, set. By combining their properties in other ways, more data
               structures can be made, like mobiles. The paper defines the data
               structures of this extended Boom Hierarchy and shows how the
               functions reduce, map, andfilter are applied to them.",
  year      =  1994,
  url       = "https://doi.org/10.1007/978-1-4471-3236-3_1"
}

@ARTICLE{Hewer2024-cc,
  title     = "Quotient Haskell: Lightweight Quotient Types for All",
  author    = "Hewer, Brandon and Hutton, Graham",
  journal   = "Proc. ACM Program. Lang.",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  volume    =  8,
  number    = "POPL",
  pages     = "785--815",
  abstract  = "Subtypes and quotient types are dual type abstractions. However,
               while subtypes are widely used both explicitly and implicitly,
               quotient types have not seen much practical use outside of proof
               assistants. A key difficulty to wider adoption of quotient types
               lies in the significant burden of proof-obligations that arises
               from their use. In this article, we address this issue by
               introducing a class of quotient types for which the
               proof-obligations are decidable by an SMT solver. We demonstrate
               this idea in practice by presenting Quotient Haskell, an
               extension of Liquid Haskell with support for quotient types.",
  month     =  jan,
  year      =  2024,
  url       = "https://doi.org/10.1145/3632869",
  keywords  = "quotient types, refinement types, static verification"
}

@INPROCEEDINGS{Viera2006-xq,
  title     = "A multi-stage language with intensional analysis",
  author    = "Viera, Marcos and Pardo, Alberto",
  booktitle = "GPCE '06: Proceedings of the 5th International Conference on
               Generative Programming and Component Engineering",
  publisher = "unknown",
  abstract  = "This paper presents the definition of a language with reflection
               primitives. The language is a homogeneous multi-stage language
               that provides the capacity of code analysis by the inclusion of a
               pattern matching mechanism that permits inspection of the
               structure of quoted expressions and their destruction into
               component subparts. Quoted expressions include an explicit
               annotation of their context which is used for dynamic inference
               of type, where a dynamic typing discipline based on Hinze and
               Cheney's approach is used for typing quoted expressions.This
               paper follows the approach of Sheard and Pasalic about the use of
               the meta-language Ωmega as a tool for language design. In this
               sense, it is shown how to represent the syntax, the static as
               well as the dynamic semantics of the proposed language in terms
               of Ωmega constructs.",
  month     =  oct,
  year      =  2006,
  url       = "https://www.researchgate.net/publication/213878918_A_multi-stage_language_with_intensional_analysis"
}

@ARTICLE{Castellan2019-sh,
  title         = "Categories with Families: Unityped, Simply Typed, and
                   Dependently Typed",
  author        = "Castellan, Simon and Clairambault, Pierre and Dybjer, Peter",
  journal       = "arXiv [cs.LO]",
  abstract      = "We show how the categorical logic of untyped, simply typed
                   and dependently typed lambda calculus can be structured
                   around the notion of category with family (cwf). To this end
                   we introduce subcategories of simply typed cwfs (scwfs),
                   where types do not depend on variables, and unityped cwfs
                   (ucwfs), where there is only one type. We prove several
                   equivalence and biequivalence theorems between cwf-based
                   notions and basic notions of categorical logic, such as
                   cartesian operads, Lawvere theories, categories with finite
                   products and limits, cartesian closed categories, and locally
                   cartesian closed categories. Some of these theorems depend on
                   the restrictions of contextuality (in the sense of Cartmell)
                   or democracy (used by Clairambault and Dybjer for their
                   biequivalence theorems). Some theorems are equivalences
                   between notions with strict preservation of chosen structure.
                   Others are biequivalences between notions where properties
                   are only preserved up to isomorphism. In addition to this we
                   discuss various constructions of initial ucwfs, scwfs, and
                   cwfs with extra structure.",
  month         =  apr,
  year          =  2019,
  url           = "http://arxiv.org/abs/1904.00827",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LO"
}

@ARTICLE{Mannaa2020-se,
  title    = "Ticking clocks as dependent right adjoints: Denotational semantics
              for clocked type theory",
  author   = "Mannaa, B and Møgelberg, R E and Veltri, Niccolò",
  journal  = "Log. Methods Comput. Sci.",
  volume   =  16,
  abstract = "Clocked Type Theory (CloTT) is a type theory for guarded recursion
              useful for programming with coinductive types, allowing
              productivity to be encoded in types, and for reasoning about
              advanced programming language features using an abstract form of
              step-indexing. CloTT has previously been shown to enjoy a number
              of syntactic properties including strong normalisation, canonicity
              and decidability of the equational theory. In this paper we
              present a denotational semantics for CloTT useful, e.g., for
              studying future extensions of CloTT with constructions such as
              path types. The main challenge for constructing this model is to
              model the notion of ticks on a clock used in CloTT for coinductive
              reasoning about coinductive types. We build on a category
              previously used to model guarded recursion with multiple clocks.
              In this category there is an object of clocks but no object of
              ticks, and so tick-assumptions in a context can not be modelled
              using standard tools. Instead we model ticks using dependent right
              adjoint functors, a generalisation of the category theoretic
              notion of adjunction to the setting of categories with families.
              Dependent right adjoints are known to model Fitch-style modal
              types, but in the case of CloTT, the modal operators constitute a
              family indexed internally in the type theory by clocks. We model
              this family using a dependent right adjoint on the slice category
              over the object of clocks. Finally we show how to model the tick
              constant of CloTT using a semantic substitution. This work
              improves on a previous model by the first two named authors which
              not only had a flaw but was also considerably more complicated.",
  month    =  apr,
  year     =  2020,
  url      = "https://www.danielgratzer.com/papers/2023-semantics-primer-lecture-notes.pdf"
}

@ARTICLE{noauthor_undated-pz,
  title = "seely\_2-categorical.pdf",
  url   = "https://math.ucr.edu/home/baez/qg-winter2007/seely_2-categorical.pdf"
}

@ARTICLE{Nuyts2018-uf,
  title    = "Presheaf models of relational modalities in dependent type theory",
  author   = "Nuyts, Andreas",
  journal  = "ArXiv",
  volume   = "abs/1805.08684",
  abstract = "This report is an extension of 'A Model of Parametric Dependent
              Type Theory in Bridge/Path Cubical Sets' (Nuyts,
              arXiv:1706.04383). The purpose of this text is to prove all
              technical aspects of our model for dependent type theory with
              parametric quantifiers (Nuyts, Vezzosi and Devriese, 2017) and
              with degrees of relatedness (Nuyts and Devriese, 2018).",
  month    =  may,
  year     =  2018,
  url      = "https://perso.crans.org/alaouar/rapportm1.pdf"
}

@ARTICLE{Baez2019-lb,
  title         = "Enriched Lawvere Theories for Operational Semantics",
  author        = "Baez, John C and Williams, Christian",
  journal       = "arXiv [math.CT]",
  abstract      = "Enriched Lawvere theories are a generalization of Lawvere
                   theories that allow us to describe the operational semantics
                   of formal systems. For example, a graph enriched Lawvere
                   theory describes structures that have a graph of operations
                   of each arity, where the vertices are operations and the
                   edges are rewrites between operations. Enriched theories can
                   be used to equip systems with operational semantics, and maps
                   between enriching categories can serve to translate between
                   different forms of operational and denotational semantics.
                   The Grothendieck construction lets us study all models of all
                   enriched theories in all contexts in a single category. We
                   illustrate these ideas with the SKI-combinator calculus, a
                   variable-free version of the lambda calculus.",
  month         =  may,
  year          =  2019,
  url           = "http://arxiv.org/abs/1905.05636",
  archivePrefix = "arXiv",
  primaryClass  = "math.CT"
}

@ARTICLE{Hu2023-li,
  title         = "Layered Modal Type Theories",
  author        = "Hu, Jason Z S and Pientka, Brigitte",
  journal       = "arXiv [cs.LO]",
  abstract      = "We introduce layers to modal type theories, which
                   subsequently enables type theories for pattern matching on
                   code in meta-programming and clean and straightforward
                   semantics.",
  month         =  may,
  year          =  2023,
  url           = "http://arxiv.org/abs/2305.06548",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LO"
}

@ARTICLE{Riley2024-zk,
  title         = "A Type Theory with a Tiny Object",
  author        = "Riley, Mitchell",
  journal       = "arXiv [math.CT]",
  abstract      = "We present an extension of Martin-Lof Type Theory that
                   contains a tiny object; a type for which there is a right
                   adjoint to the formation of function types as well as the
                   expected left adjoint. We demonstrate the practicality of
                   this type theory by proving various properties related to
                   tininess internally and suggest a few potential applications.",
  month         =  mar,
  year          =  2024,
  url           = "http://arxiv.org/abs/2403.01939",
  archivePrefix = "arXiv",
  primaryClass  = "math.CT"
}

@MISC{Yallop_undated-sh,
  title        = "Unembedding Domain-Specific Languages",
  author       = "Yallop, Robert Atkey Sam Lindley",
  howpublished = "\url{https://bentnib.org/unembedding.pdf}",
  note         = "Accessed: 2024-2-22"
}

@ARTICLE{Sato2001-ap,
  title   = "A simply typed context calculus with first-class environments",
  author  = "Sato, M and Sakurai, Takafumi and Kameyama, Yukiyoshi",
  journal = "J. Funct. Log. Prog.",
  pages   = "359--374",
  month   =  mar,
  year    =  2001,
  url     = "https://scholar.lib.vt.edu/ejournals/JFLP/jflp-mirror/articles/2002/S02-01/JFLP-A02-04.pdf"
}

@ARTICLE{Chataing2023-sa,
  title         = "Unboxed data constructors -- or, how cpp decides a halting
                   problem",
  author        = "Chataing, Nicolas and Dolan, Stephen and Scherer, Gabriel and
                   Yallop, Jeremy",
  journal       = "arXiv [cs.PL]",
  abstract      = "We propose a new language feature for ML-family languages,
                   the ability to selectively *unbox* certain data constructors,
                   so that their runtime representation gets compiled away to
                   just the identity on their argument. Unboxing must be
                   statically rejected when it could introduce *confusions*,
                   that is, distinct values with the same representation. We
                   discuss the use-case of big numbers, where unboxing allows to
                   write code that is both efficient and safe, replacing either
                   a safe but slow version or a fast but unsafe version. We
                   explain the static analysis necessary to reject incorrect
                   unboxing requests. We present our prototype implementation of
                   this feature for the OCaml programming language, discuss
                   several design choices and the interaction with advanced
                   features such as Guarded Algebraic Datatypes. Our static
                   analysis requires expanding type definitions in type
                   expressions, which is not necessarily normalizing in presence
                   of recursive type definitions. In other words, we must decide
                   normalization of terms in the first-order lambda-calculus
                   with recursion. We provide an algorithm to detect
                   non-termination on-the-fly during reduction, with proofs of
                   correctness and completeness. Our termination-monitoring
                   algorithm turns out to be closely related to the
                   normalization strategy for macro expansion in the `cpp`
                   preprocessor.",
  month         =  nov,
  year          =  2023,
  url           = "http://arxiv.org/abs/2311.07369",
  archivePrefix = "arXiv",
  primaryClass  = "cs.PL"
}

@ARTICLE{Karachalias2021-jp,
  title     = "Efficient compilation of algebraic effect handlers",
  author    = "Karachalias, Georgios and Koprivec, Filip and Pretnar, Matija and
               Schrijvers, Tom",
  journal   = "Proc. ACM Program. Lang.",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  volume    =  5,
  number    = "OOPSLA",
  pages     = "1--28",
  abstract  = "The popularity of algebraic effect handlers as a programming
               language feature for user-defined computational effects is
               steadily growing. Yet, even though efficient runtime
               representations have already been studied, most handler-based
               programs are still much slower than hand-written code. This paper
               shows that the performance gap can be drastically narrowed (in
               some cases even closed) by means of type-and-effect directed
               optimising compilation. Our approach consists of source-to-source
               transformations in two phases of the compilation pipeline.
               Firstly, elementary rewrites, aided by judicious function
               specialisation, exploit the explicit type and effect information
               of the compiler’s core language to aggressively reduce handler
               applications. Secondly, after erasing the effect information
               further rewrites in the backend of the compiler emit tight code.
               This work comes with a practical implementation: an optimising
               compiler from Eff, an ML style language with algebraic effect
               handlers, to OCaml. Experimental evaluation with this
               implementation demonstrates that in a number of benchmarks, our
               approach eliminates much of the overhead of handlers, outperforms
               capability-passing style compilation and yields competitive
               performance compared to hand-written OCaml code as well Multicore
               OCaml’s dedicated runtime support.",
  month     =  oct,
  year      =  2021,
  url       = "https://doi.org/10.1145/3485479",
  keywords  = "OCaml, algebraic effect handlers, optimising compilation"
}

@ARTICLE{Cockx2021-pw,
  title     = "The taming of the rew: a type theory with computational
               assumptions",
  author    = "Cockx, Jesper and Tabareau, Nicolas and Winterhalter, Théo",
  journal   = "Proc. ACM Program. Lang.",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  volume    =  5,
  number    = "POPL",
  pages     = "1--29",
  abstract  = "Dependently typed programming languages and proof assistants such
               as Agda and Coq rely on computation to automatically simplify
               expressions during type checking. To overcome the lack of certain
               programming primitives or logical principles in those systems, it
               is common to appeal to axioms to postulate their existence.
               However, one can only postulate the bare existence of an axiom,
               not its computational behaviour. Instead, users are forced to
               postulate equality proofs and appeal to them explicitly to
               simplify expressions, making axioms dramatically more complicated
               to work with than built-in primitives. On the other hand, the
               equality reflection rule from extensional type theory solves
               these problems by collapsing computation and equality, at the
               cost of having no practical type checking algorithm. This paper
               introduces Rewriting Type Theory (RTT), a type theory where it is
               possible to add computational assumptions in the form of rewrite
               rules. Rewrite rules go beyond the computational capabilities of
               intensional type theory, but in contrast to extensional type
               theory, they are applied automatically so type checking does not
               require input from the user. To ensure type soundness of RTT—as
               well as effective type checking—we provide a framework where
               confluence of user-defined rewrite rules can be checked modularly
               and automatically, and where adding new rewrite rules is
               guaranteed to preserve subject reduction. The properties of RTT
               have been formally verified using the MetaCoq framework and an
               implementation of rewrite rules is already available in the Agda
               proof assistant.",
  month     =  jan,
  year      =  2021,
  url       = "https://doi.org/10.1145/3434341",
  keywords  = "confluence, dependent types, rewriting theory, termination, type
               theory"
}

@ARTICLE{Hinze1993-fl,
  title     = "The Categorical Abstract Machine: Basics and Enhancements",
  author    = "Hinze, Ralf",
  publisher = "unknown",
  abstract  = "The categorical abstract machine (CAM) is a well-known
               environment-based architecture for implementing strict functional
               languages. Existing literature about the CAM presumes more or
               less a category-theoretical background. Although we appreciate
               the rm grounds on which the CAM is built, we try to motivate the
               components of the CAM from an implementor's point of view. This
               undertaking is facilitated by the conceptional simplicity of the
               CAM and its proximity to conventional stack architectures. We
               describe the translation of an augmented -calculus into CAM
               instructions. The basic compilation scheme has its didactic
               virtues but for a real implementation many improvements are
               necessary (and applicable). A rst improvement concerns the
               treatment of subexpressions which do not access the environment.
               Further improvements are achieved via simple local
               transforma-tions of the generated CAM code|this is one of the
               novel features presented in this report. They include -reduction
               at compile-time, improving calls to local functions, and last
               call optimization.",
  month     =  may,
  year      =  1993,
  url       = "http://dx.doi.org/"
}

@ARTICLE{Balland2008-bb,
  title    = "Rewriting Strategies in Java",
  author   = "Balland, Emilie and Moreau, Pierre-Etienne and Reilles, Antoine",
  journal  = "Electron. Notes Theor. Comput. Sci.",
  volume   =  219,
  pages    = "97--111",
  abstract = "In any language designed to express transformations, the notion of
              rewrite rule is a key feature. Its conciseness as well as its
              strong theoretical foundations are essential. The notion of
              strategy is complementary: this describes how rules are applied.
              In this paper, we show how a high-level strategy language can be
              implemented in a Java setting. We present the integration of the
              visitor combinator design pattern into Tom. This corresponds to an
              interpreter for strategy expressions. To be more efficient, we
              present a compilation method based on bytecode specialization.
              This low-level transformation is expressed in Tom itself, using
              rules and strategies.",
  month    =  nov,
  year     =  2008,
  url      = "https://www.sciencedirect.com/science/article/pii/S1571066108004313",
  keywords = "Rewriting strategy; design pattern"
}

@ARTICLE{Bourke2021-ox,
  title     = "Accessible aspects of 2-category theory",
  author    = "Bourke, John",
  journal   = "J. Pure Appl. Algebra",
  publisher = "Elsevier BV",
  volume    =  225,
  number    =  3,
  pages     =  106519,
  month     =  mar,
  year      =  2021,
  url       = "http://dx.doi.org/10.1016/j.jpaa.2020.106519",
  language  = "en"
}

@ARTICLE{Castellan2019-qo,
  title         = "Categories with Families: Unityped, Simply Typed, and
                   Dependently Typed",
  author        = "Castellan, Simon and Clairambault, Pierre and Dybjer, Peter",
  journal       = "arXiv [cs.LO]",
  abstract      = "We show how the categorical logic of untyped, simply typed
                   and dependently typed lambda calculus can be structured
                   around the notion of category with family (cwf). To this end
                   we introduce subcategories of simply typed cwfs (scwfs),
                   where types do not depend on variables, and unityped cwfs
                   (ucwfs), where there is only one type. We prove several
                   equivalence and biequivalence theorems between cwf-based
                   notions and basic notions of categorical logic, such as
                   cartesian operads, Lawvere theories, categories with finite
                   products and limits, cartesian closed categories, and locally
                   cartesian closed categories. Some of these theorems depend on
                   the restrictions of contextuality (in the sense of Cartmell)
                   or democracy (used by Clairambault and Dybjer for their
                   biequivalence theorems). Some theorems are equivalences
                   between notions with strict preservation of chosen structure.
                   Others are biequivalences between notions where properties
                   are only preserved up to isomorphism. In addition to this we
                   discuss various constructions of initial ucwfs, scwfs, and
                   cwfs with extra structure.",
  month         =  apr,
  year          =  2019,
  url           = "http://arxiv.org/abs/1904.00827",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LO"
}

@ARTICLE{Misu2024-mo,
  title         = "Towards {AI}-assisted synthesis of verified Dafny methods",
  author        = "Misu, Md Rakib Hossain and Lopes, Cristina V and Ma, Iris and
                   Noble, James",
  journal       = "arXiv [cs.SE]",
  abstract      = "Large stochastic language models show great promise in many
                   domains, including programming. A promise is easy to make but
                   hard to keep, and language models often fail to keep their
                   promises when applied to programming, generating erroneous
                   code. One promising avenue to keep models honest is to have
                   them generate code in a language that supports formal
                   verification: if and when that is adopted, the model would
                   provide proof along with the code, and that proof would be
                   automatically verified. Unfortunately, existing large
                   language models show a severe lack of proficiency in verified
                   programming languages. In this paper we demonstrate how to
                   improve two pretrained models' proficiency in the Dafny
                   verified programming language. Using 178 programming problems
                   from the MBPP dataset, we prompt two contemporary models
                   (GPT-4 and PaLM-2) to generate methods in Dafny. We use three
                   different types of prompts: a direct contextless prompt, a
                   second one that includes a signature of the method and test
                   cases, and a third one that decomposes the problem into steps
                   and includes dynamically chosen similar examples. Our results
                   show that GPT-4 is better than PaLM-2, but that, in both
                   models, the third prompt greatly improves the success of the
                   generation task for the direct prompt. With the third prompt,
                   GPT-4 was able to generate verified (and human-evaluated)
                   Dafny methods in 58\% of the cases, while the first prompt
                   generated verified (and human-evaluated) methods in only 19\%
                   of the cases. Surprisingly, the second prompt had the worst
                   performance, with only 10\%. One tangible contribution of our
                   work is a collection of 153 MBPP problems that are
                   implemented and formally verified in Dafny, 50 of which were
                   written by us and 103 were automatically synthesized by
                   GPT-4. Additionally, our results demonstrate that the
                   benefits of formal program verification (proof of
                   correctness) are now within reach...",
  month         =  feb,
  year          =  2024,
  url           = "http://arxiv.org/abs/2402.00247",
  archivePrefix = "arXiv",
  primaryClass  = "cs.SE"
}

@ARTICLE{Hirschowitz_undated-on,
  title    = "Cartesian closed 2-categories and permutation equivalence in
              higher-order rewriting",
  author   = "Hirschowitz, Tom",
  journal  = "https://hal.science/hal-",
  abstract = "We propose a semantics for permutation equivalence in higher-order
              rewriting. This semantics takes place in cartesian closed
              2-categories, and is proved sound and complete.",
  url      = "http://dx.doi.org/10.2168/LMCS-9(3:10)2013"
}

@ARTICLE{Paykin2018-zw,
  title    = "Linear/non-linear types for embedded domain-specific languages",
  author   = "Paykin, Jennifer",
  abstract = "LINEAR/NON-LINEAR TYPES FOR EMBEDDED DOMAIN-SPECIFIC LANGUAGES
              Jennifer Paykin Steve Zdancewic Domain-specific languages are
              often embedded inside of general-purpose host languages so that
              the embedded language can take advantage of host-language data
              structures, libraries, and tools. However, when the
              domain-specific language uses linear types, existing techniques
              for embedded languages fall short. Linear type systems, which have
              applications in a wide variety of programming domains including
              mutable state, I/O, concurrency, and quantum computing, can
              manipulate embedded non-linear data via the linear type !σ.
              However, prior work has not been able to produce linear embedded
              languages that have full and easy access to host-language data,
              libraries, and tools. This dissertation proposes a new perspective
              on linear, embedded, domain-specific languages derived from the
              linear/non-linear (LNL) interpretation of linear logic. The LNL
              model consists of two distinct fragments—one with linear types and
              another with non-linear types—and provides a simple categorical
              interface between the two. This dissertation identifies the linear
              fragment with the linear embedded language and the non-linear
              fragment with the general-purpose host language. The effectiveness
              of this framework is illustrated via a number of examples,
              implemented in a variety of host languages. In Haskell, linear
              domain-specific languages using mutable state and concurrency can
              take advantage of the monad that arises from the LNL model. In
              Coq, the Qwire quantum circuit language uses linearity to enforce
              the no-cloning axiom of quantum mechanics. In homotopy type
              theory, quantum transformations can be encoded as higher inductive
              types to simplify the presentation of a quantum equational theory.
              These examples serve as case studies that prove linear/non-linear
              type theory is a natural and expressive interface in which to
              embed linear domain-specific languages.",
  year     =  2018,
  url      = "https://core.ac.uk/download/pdf/214213829.pdf"
}

@INPROCEEDINGS{Alon2020-vn,
  title     = "Structural language models of code",
  author    = "Alon, Uri and Sadaka, Roy and Levy, Omer and Yahav, Eran",
  editor    = "Iii, Hal Daumé and Singh, Aarti",
  booktitle = "Proceedings of the 37th International Conference on Machine
               Learning",
  publisher = "PMLR",
  volume    =  119,
  pages     = "245--256",
  abstract  = "We address the problem of any-code completion - generating a
               missing piece of source code in a given program without any
               restriction on the vocabulary or structure. We introduce a new
               approach to any-code completion that leverages the strict syntax
               of programming languages to model a code snippet as a tree -
               structural language modeling (SLM). SLM estimates the probability
               of the program’s abstract syntax tree (AST) by decomposing it
               into a product of conditional probabilities over its nodes. We
               present a neural model that computes these conditional
               probabilities by considering all AST paths leading to a target
               node. Unlike previous techniques that have severely restricted
               the kinds of expressions that can be generated in this task, our
               approach can generate arbitrary code in any programming language.
               Our model significantly outperforms both seq2seq and a variety of
               structured approaches in generating Java and C\# code. Our code,
               data, and trained models are available at
               http://github.com/tech-srl/slm-code-generation/. An online demo
               is available at http://AnyCodeGen.org.",
  series    = "Proceedings of Machine Learning Research",
  year      =  2020,
  url       = "https://proceedings.mlr.press/v119/alon20a.html"
}
@INPROCEEDINGS{Marshall2022-fk,
  title     = "Linearity and Uniqueness: An Entente Cordiale",
  author    = "Marshall, Daniel and Vollmer, Michael and Orchard, Dominic",
  booktitle = "Programming Languages and Systems",
  publisher = "Springer International Publishing",
  pages     = "346--375",
  abstract  = "Substructural type systems are growing in popularity because they
               allow for a resourceful interpretation of data which can be used
               to rule out various software bugs. Indeed, substructurality is
               finally taking hold in modern programming; Haskell now has linear
               types roughly based on Girard’s linear logic but integrated via
               graded function arrows, Clean has uniqueness types designed to
               ensure that values have at most a single reference to them, and
               Rust has an intricate ownership system for guaranteeing memory
               safety. But despite this broad range of resourceful type systems,
               there is comparatively little understanding of their relative
               strengths and weaknesses or whether their underlying frameworks
               can be unified. There is often confusion about whether linearity
               and uniqueness are essentially the same, or are instead ‘dual’ to
               one another, or somewhere in between. This paper formalises the
               relationship between these two well-studied but rarely contrasted
               ideas, building on two distinct bodies of literature, showing
               that it is possible and advantageous to have both linear and
               unique types in the same type system. We study the guarantees of
               the resulting system and provide a practical implementation in
               the graded modal setting of the Granule language, adding a third
               kind of modality alongside coeffect and effect modalities. We
               then demonstrate via a benchmark that our implementation benefits
               from expected efficiency gains enabled by adding uniqueness to a
               language that already has a linear basis.",
  year      =  2022,
  url       = "http://dx.doi.org/10.1007/978-3-030-99336-8_13"
}

@ARTICLE{Marshall2023-oz,
  title         = "Functional Ownership through Fractional Uniqueness",
  author        = "Marshall, Daniel and Orchard, Dominic",
  journal       = "arXiv [cs.PL]",
  abstract      = "Ownership and borrowing systems, designed to enforce safe
                   memory management without the need for garbage collection,
                   have been brought to the fore by the Rust programming
                   language. Rust also aims to bring some guarantees offered by
                   functional programming into the realm of performant systems
                   code, but the type system is largely separate from the
                   ownership model, with type and borrow checking happening in
                   separate compilation phases. Recent models such as RustBelt
                   and Oxide aim to formalise Rust in depth, but there is less
                   focus on integrating the basic ideas into more traditional
                   type systems. An approach designed to expose an essential
                   core for ownership and borrowing would open the door for
                   functional languages to borrow concepts found in Rust and
                   other ownership frameworks, so that more programmers can
                   enjoy their benefits. One strategy for managing memory in a
                   functional setting is through uniqueness types, but these
                   offer a coarse-grained view: either a value has exactly one
                   reference, and can be mutated safely, or it cannot, since
                   other references may exist. Recent work demonstrates that
                   linear and uniqueness types can be combined in a single
                   system to offer restrictions on program behaviour and
                   guarantees about memory usage. We develop this connection
                   further, showing that just as graded type systems like those
                   of Granule and Idris generalise linearity, Rust's ownership
                   model arises as a graded generalisation of uniqueness. We
                   combine fractional permissions with grading to give the first
                   account of ownership and borrowing that smoothly integrates
                   into a standard type system alongside linearity and graded
                   types, and extend Granule accordingly with these ideas.",
  month         =  oct,
  year          =  2023,
  url           = "http://arxiv.org/abs/2310.18166",
  archivePrefix = "arXiv",
  primaryClass  = "cs.PL"
}

@ARTICLE{Brady2010-sf,
  title     = "Scrapping your inefficient engine: using partial evaluation to
               improve domain-specific language implementation",
  author    = "Brady, Edwin C and Hammond, Kevin",
  journal   = "SIGPLAN Not.",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  volume    =  45,
  number    =  9,
  pages     = "297--308",
  abstract  = "Partial evaluation aims to improve the efficiency of a program by
               specialising it with respect to some known inputs. In this paper,
               we show that partial evaluation can be an effective and,
               unusually, easy to use technique for the efficient implementation
               of embedded domain-specific languages. We achieve this by
               exploiting dependent types and by following some simple rules in
               the definition of the interpreter for the domain-specific
               language. We present experimental evidence that partial
               evaluation of programs in domain-specific languages can yield
               efficient residual programs whose performance is competitive with
               their Java and C equivalents and which are also, through the use
               of dependent types, verifiably resource-safe. Using our
               technique, it follows that a verifiably correct and resource-safe
               program can also be an efficient program",
  month     =  sep,
  year      =  2010,
  url       = "https://doi.org/10.1145/1932681.1863587",
  keywords  = "dependent types, partial evaluation"
}

@ARTICLE{Pedrot2019-bt,
  title     = "The fire triangle: how to mix substitution, dependent
               elimination, and effects",
  author    = "Pédrot, Pierre-Marie and Tabareau, Nicolas",
  journal   = "Proc. ACM Program. Lang.",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  volume    =  4,
  number    = "POPL",
  pages     = "1--28",
  abstract  = "There is a critical tension between substitution, dependent
               elimination and effects in type theory. In this paper, we
               crystallize this tension in the form of a no-go theorem that
               constitutes the fire triangle of type theory. To release this
               tension, we propose ∂CBPV, an extension of call-by-push-value
               (CBPV) —a general calculus of effects—to dependent types. Then,
               by extending to ∂CBPV the well-known decompositions of
               call-by-name and call-by-value into CBPV, we show why, in
               presence of effects, dependent elimination must be restricted in
               call-by-name, and substitution must be restricted in
               call-by-value. To justify ∂CBPV and show that it is general
               enough to interpret many kinds of effects, we define various
               effectful syntactic translations from ∂CBPV to Martin-Löf type
               theory: the reader, weaning and forcing translations.",
  month     =  dec,
  year      =  2019,
  url       = "https://doi.org/10.1145/3371126",
  keywords  = "type theory, effects"
}

@INPROCEEDINGS{Altenkirch2016-zc,
  title     = "Type theory in type theory using quotient inductive types",
  author    = "Altenkirch, Thorsten and Kaposi, Ambrus",
  booktitle = "Proceedings of the 43rd Annual ACM SIGPLAN-SIGACT Symposium on
               Principles of Programming Languages",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "18--29",
  abstract  = "We present an internal formalisation of a type heory with
               dependent types in Type Theory using a special case of higher
               inductive types from Homotopy Type Theory which we call quotient
               inductive types (QITs). Our formalisation of type theory avoids
               referring to preterms or a typability relation but defines
               directly well typed objects by an inductive definition. We use
               the elimination principle to define the set-theoretic and logical
               predicate interpretation. The work has been formalized using the
               Agda system extended with QITs using postulates.",
  series    = "POPL '16",
  month     =  jan,
  year      =  2016,
  url       = "https://doi.org/10.1145/2837614.2837638",
  keywords  = "Metaprogramming, Homotopy Type Theory, Agda, Logical Relations,
               Higher Inductive Types"
}

@ARTICLE{Kovacs2022-vb,
  title         = "Staged compilation with two-level type theory",
  author        = "Kovács, András",
  journal       = "arXiv [cs.PL]",
  abstract      = "The aim of staged compilation is to enable metaprogramming in
                   a way such that we have guarantees about the well-formedness
                   of code output, and we can also mix together object-level and
                   meta-level code in a concise and convenient manner. In this
                   work, we observe that two-level type theory (2LTT), a system
                   originally devised for the purpose of developing synthetic
                   homotopy theory, also serves as a system for staged
                   compilation with dependent types. 2LTT has numerous good
                   properties for this use case: it has a concise specification,
                   well-behaved model theory, and it supports a wide range of
                   language features both at the object and the meta level.
                   First, we give an overview of 2LTT's features and
                   applications in staging. Then, we present a staging algorithm
                   and prove its correctness. Our algorithm is
                   ``staging-by-evaluation'', analogously to the technique of
                   normalization-by-evaluation, in that staging is given by the
                   evaluation of 2LTT syntax in a semantic domain. The staging
                   algorithm together with its correctness constitutes a proof
                   of strong conservativity of 2LLT over the object theory. To
                   our knowledge, this is the first description of staged
                   compilation which supports full dependent types and
                   unrestricted staging for types.",
  month         =  sep,
  year          =  2022,
  url           = "http://arxiv.org/abs/2209.09729",
  archivePrefix = "arXiv",
  primaryClass  = "cs.PL"
}

@INPROCEEDINGS{Hurkens1995-uu,
  title     = "A Simplification of Girard's Paradox",
  author    = "Hurkens, Antonius J C",
  booktitle = "Proceedings of the Second International Conference on Typed
               Lambda Calculi and Applications",
  publisher = "Springer-Verlag",
  address   = "Berlin, Heidelberg",
  pages     = "266--278",
  series    = "TLCA '95",
  month     =  apr,
  year      =  1995,
  url       = "https://dl.acm.org/doi/10.5555/645892.671442"
}

@ARTICLE{Paulin-Mohring2015-qi,
  title     = "Introduction to the Calculus of Inductive Constructions",
  author    = "Paulin-Mohring, Christine",
  journal   = "All about Proofs, Proofs for All",
  publisher = "College Publications",
  volume    =  55,
  abstract  = "This paper gives an introduction to the Calculus of Inductive
               Constructions, the formalism behind the Coq proof assistant. We
               present the language and the typing rules, starting with the pure
               functional part and then introducing the inductive declarations.
               We briefly discuss the properties of this language, both from the
               theoretical and pragmatic points of view and give examples of
               applications.",
  month     =  jan,
  year      =  2015,
  url       = "https://inria.hal.science/hal-01094195",
  keywords  = "Calculus of Inductive Constructions;Coq proof assistant",
  language  = "en"
}

@MISC{noauthor_undated-rc,
  title        = "Official page for Language Server Protocol",
  abstract     = "Language Server Protocol documentation and specification page.",
  howpublished = "\url{https://microsoft.github.io/language-server-protocol/}",
  note         = "Accessed: 2023-12-6",
  language     = "en"
}

@INPROCEEDINGS{Franke2022-qy,
  title     = "Collection Skeletons: Declarative Abstractions for Data
               Collections",
  author    = "Franke, Björn and Li, Zhibo and Morton, Magnus and Steuwer,
               Michel",
  booktitle = "Proceedings of the 15th ACM SIGPLAN International Conference on
               Software Language Engineering",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "189--201",
  abstract  = "Modern programming languages provide programmers with rich
               abstractions for data collections as part of their standard
               libraries, e.g. Containers in the C++ STL, the Java Collections
               Framework, or the Scala Collections API. Typically, these
               collections frameworks are organised as hierarchies that provide
               programmers with common abstract data types (ADTs) like lists,
               queues, and stacks. While convenient, this approach introduces
               problems which ultimately affect application performance due to
               users over-specifying collection data types limiting
               implementation flexibility. In this paper, we develop Collection
               Skeletons which provide a novel, declarative approach to data
               collections. Using our framework, programmers explicitly select
               properties for their collections, thereby truly decoupling
               specification from implementation. By making collection
               properties explicit immediate benefits materialise in form of
               reduced risk of over-specification and increased implementation
               flexibility. We have prototyped our declarative abstractions for
               collections as a C++ library, and demonstrate that benchmark
               applications rewritten to use Collection Skeletons incur little
               or no overhead. In fact, for several benchmarks, we observe
               performance speedups (on average between 2.57 to 2.93, and up to
               16.37) and also enhanced performance portability across three
               different hardware platforms.",
  series    = "SLE 2022",
  month     =  dec,
  year      =  2022,
  url       = "https://doi.org/10.1145/3567512.3567528",
  keywords  = "data structures, properties, Containers, collections"
}

@INPROCEEDINGS{Abbott2003-no,
  title     = "Categories of Containers",
  author    = "Abbott, Michael and Altenkirch, Thorsten and Ghani, Neil",
  booktitle = "Foundations of Software Science and Computation Structures",
  publisher = "Springer Berlin Heidelberg",
  pages     = "23--38",
  abstract  = "We introduce the notion of containers as a mathematical
               formalisation of the idea that many important datatypes consist
               of templates where data is stored. We show that containers have
               good closure properties under a variety of constructions
               including the formation of initial algebras and final coalgebras.
               We also show that containers include strictly positive types and
               shapely types but that there are containers which do not
               correspond to either of these. Further, we derive a
               representation result classifying the nature of polymorphic
               functions between containers. We finish this paper with an
               application to the theory of shapely types and refer to a
               forthcoming paper which applies this theory to differentiable
               types.",
  year      =  2003,
  url       = "http://dx.doi.org/10.1007/3-540-36576-1_2"
}

@MISC{Kock_undated-ba,
  title  = "Notes on Polynomial Functors",
  author = "Kock, Joachim",
  url    = "https://mat.uab.cat/~kock/cat/polynomial.pdf"
}

@BOOK{Niu_undated-ci,
  title    = "Polynomial Functors: A Mathematical Theory of Interaction",
  author   = "Niu, Nelson and Spivak, David I",
  abstract = "Contribute to ToposInstitute/poly development by creating an
              account on GitHub.",
  url      = "https://github.com/ToposInstitute/poly",
  language = "en"
}

@ARTICLE{Kock2008-ca,
  title         = "Polynomial functors and trees",
  author        = "Kock, Joachim",
  journal       = "arXiv [math.CT]",
  abstract      = "We explore the relationship between polynomial functors and
                   (rooted) trees. In the first part we use polynomial functors
                   to derive a new convenient formalism for trees, and obtain a
                   natural and conceptual construction of the category $\Omega$
                   of Moerdijk and Weiss; its main properties are described in
                   terms of some factorisation systems. Although the
                   constructions are motivated and explained in terms of
                   polynomial functors, they all amount to elementary
                   manipulations with finite sets. In the second part we
                   describe polynomial endofunctors and monads as structures
                   built from trees, characterising the images of several nerve
                   functors from polynomial endofunctors and monads into
                   presheaves on categories of trees. Polynomial endofunctors
                   and monads over a base are characterised by a sheaf condition
                   on categories of decorated trees. In the absolute case, one
                   further condition is needed, a certain projectivity
                   condition, which serves also to characterise polynomial
                   endofunctors and monads among (coloured) collections and
                   operads.",
  month         =  jul,
  year          =  2008,
  url           = "http://arxiv.org/abs/0807.2874",
  archivePrefix = "arXiv",
  primaryClass  = "math.CT"
}

@ARTICLE{Larsen2021-np,
  title         = "How many degrees of freedom do we need to train deep
                   networks: a loss landscape perspective",
  author        = "Larsen, Brett W and Fort, Stanislav and Becker, Nic and
                   Ganguli, Surya",
  journal       = "arXiv [cs.LG]",
  abstract      = "A variety of recent works, spanning pruning, lottery tickets,
                   and training within random subspaces, have shown that deep
                   neural networks can be trained using far fewer degrees of
                   freedom than the total number of parameters. We analyze this
                   phenomenon for random subspaces by first examining the
                   success probability of hitting a training loss sub-level set
                   when training within a random subspace of a given training
                   dimensionality. We find a sharp phase transition in the
                   success probability from $0$ to $1$ as the training dimension
                   surpasses a threshold. This threshold training dimension
                   increases as the desired final loss decreases, but decreases
                   as the initial loss decreases. We then theoretically explain
                   the origin of this phase transition, and its dependence on
                   initialization and final desired loss, in terms of properties
                   of the high-dimensional geometry of the loss landscape. In
                   particular, we show via Gordon's escape theorem, that the
                   training dimension plus the Gaussian width of the desired
                   loss sub-level set, projected onto a unit sphere surrounding
                   the initialization, must exceed the total number of
                   parameters for the success probability to be large. In
                   several architectures and datasets, we measure the threshold
                   training dimension as a function of initialization and
                   demonstrate that it is a small fraction of the total
                   parameters, implying by our theory that successful training
                   with so few dimensions is possible precisely because the
                   Gaussian width of low loss sub-level sets is very large.
                   Moreover, we compare this threshold training dimension to
                   more sophisticated ways of reducing training degrees of
                   freedom, including lottery tickets as well as a new,
                   analogous method: lottery subspaces. Code is available at
                   https://github.com/ganguli-lab/degrees-of-freedom.",
  month         =  jul,
  year          =  2021,
  url           = "http://arxiv.org/abs/2107.05802",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG"
}

@ARTICLE{Allais2018-gi,
  title     = "A type and scope safe universe of syntaxes with binding: their
               semantics and proofs",
  author    = "Allais, Guillaume and Atkey, Robert and Chapman, James and
               McBride, Conor and McKinna, James",
  journal   = "Proc. ACM Program. Lang.",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  volume    =  2,
  number    = "ICFP",
  pages     = "1--30",
  abstract  = "Almost every programming language’s syntax includes a notion of
               binder and corresponding bound occurrences, along with the
               accompanying notions of α-equivalence, capture avoiding
               substitution, typing contexts, runtime environments, and so on.
               In the past, implementing and reasoning about programming
               languages required careful handling to maintain the correct
               behaviour of bound variables. Modern programming languages
               include features that enable constraints like scope safety to be
               expressed in types. Nevertheless, the programmer is still forced
               to write the same boilerplate over again for each new
               implementation of a scope safe operation (e.g., renaming,
               substitution, desugaring, printing, etc.), and then again for
               correctness proofs. We present an expressive universe of syntaxes
               with binding and demonstrate how to (1) implement scope safe
               traversals once and for all by generic programming; and (2) how
               to derive properties of these traversals by generic proving. Our
               universe description, generic traversals and proofs, and our
               examples have all been formalised in Agda and are available in
               the accompanying material. NB. we recommend printing the paper in
               colour to benefit from syntax highlighting in code fragments.",
  month     =  jul,
  year      =  2018,
  url       = "https://doi.org/10.1145/3236785",
  keywords  = "Syntax with Binding, Semantics, Logical Relations, Generic
               Programming, Fusion, Agda, Simulation"
}

@ARTICLE{Diehl2018-ba,
  title     = "Generic zero-cost reuse for dependent types",
  author    = "Diehl, Larry and Firsov, Denis and Stump, Aaron",
  journal   = "Proc. ACM Program. Lang.",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  volume    =  2,
  number    = "ICFP",
  pages     = "1--30",
  abstract  = "Dependently typed languages are well known for having a problem
               with code reuse. Traditional non-indexed algebraic datatypes
               (e.g. lists) appear alongside a plethora of indexed variations
               (e.g. vectors). Functions are often rewritten for both
               non-indexed and indexed versions of essentially the same
               datatype, which is a source of code duplication. We work in a
               Curry-style dependent type theory, where the same untyped term
               may be classified as both the non-indexed and indexed versions of
               a datatype. Many solutions have been proposed for the problem of
               dependently typed reuse, but we exploit Curry-style type theory
               in our solution to not only reuse data and programs, but do so at
               zero-cost (without a runtime penalty). Our work is an exercise in
               dependently typed generic programming, and internalizes the
               process of zero-cost reuse as the identity function in a
               Curry-style theory.",
  month     =  jul,
  year      =  2018,
  url       = "https://doi.org/10.1145/3236799",
  keywords  = "dependent types, reuse, generic programming"
}

@ARTICLE{Dagand2012-gp,
  title         = "Elaborating Inductive Definitions",
  author        = "Dagand, Pierre-Evariste and McBride, Conor",
  journal       = "arXiv [cs.PL]",
  abstract      = "We present an elaboration of inductive definitions down to a
                   universe of datatypes. The universe of datatypes is an
                   internal presentation of strictly positive families within
                   type theory. By elaborating an inductive definition -- a
                   syntactic artifact -- to its code -- its semantics -- we
                   obtain an internalized account of inductives inside the type
                   theory itself: we claim that reasoning about inductive
                   definitions could be carried in the type theory, not in the
                   meta-theory as it is usually the case. Besides, we give a
                   formal specification of that elaboration process. It is
                   therefore amenable to formal reasoning too. We prove the
                   soundness of our translation and hint at its correctness with
                   respect to Coq's Inductive definitions. The practical
                   benefits of this approach are numerous. For the type
                   theorist, this is a small step toward bootstrapping, ie.
                   implementing the inductive fragment in the type theory
                   itself. For the programmer, this means better support for
                   generic programming: we shall present a lightweight deriving
                   mechanism, entirely definable by the programmer and therefore
                   not requiring any extension to the type theory.",
  month         =  oct,
  year          =  2012,
  url           = "http://arxiv.org/abs/1210.6390",
  archivePrefix = "arXiv",
  primaryClass  = "cs.PL"
}

@PHDTHESIS{Diehl2017-li,
  title    = "Fully Generic Programming Over Closed Universes of
              Inductive-Recursive Types",
  author   = "Diehl, Larry",
  abstract = "Dependently typed programming languages allow the type system to
              express arbitrary propositions of intuitionistic logic, thanks to
              the Curry-Howard isomorphism. Taking full advantage of this type
              system requires defining more types than usual, in order to encode
              logical correctness criteria into the definitions of datatypes.
              While an abundance of specialized types helps ensure correctness,
              it comes at the cost of needing to redefine common functions for
              each specialized type. This dissertation makes an effort to attack
              the problem of code reuse in dependently typed languages. Our
              solution is to write generic functions, which can be applied to
              any datatype. Such a generic function can be applied to datatypes
              that are defined at the time the generic function was written, but
              they can also be applied to any datatype that is defined in the
              future. Our solution builds upon previous work on generic
              programming within dependently typed programming. Type theory
              supports generic programming using a construction known as a
              universe. A universe can be considered the model of a programming
              language, such that writing functions over it models writing
              generic programs in the programming language. Historically, there
              has been a trade-off between the expressive power of the modeled
              programming language, and the kinds of generic functions that can
              be written in it. Our dissertation shows that no such trade-off is
              necessary, and that we can write future-proof generic functions in
              a model of a dependently typed programming language with a rich
              collection of types.",
  year     =  2017,
  url      = "https://pdxscholar.library.pdx.edu/open_access_etds/3647",
  school   = "Portland State University"
}

@ARTICLE{Lago2021-kt,
  title         = "Modal Reasoning = Metric Reasoning, via Lawvere",
  author        = "Lago, Ugo Dal and Gavazzo, Francesco",
  journal       = "arXiv [cs.LO]",
  abstract      = "Graded modal types systems and coeffects are becoming a
                   standard formalism to deal with context-dependent
                   computations where code usage plays a central role. The
                   theory of program equivalence for modal and coeffectful
                   languages, however, is considerably underdeveloped if
                   compared to the denotational and operational semantics of
                   such languages. This raises the question of how much of the
                   theory of ordinary program equivalence can be given in a
                   modal scenario. In this work, we show that coinductive
                   equivalences can be extended to a modal setting, and we do so
                   by generalising Abramsky's applicative bisimilarity to
                   coeffectful behaviours. To achieve this goal, we develop a
                   general theory of ternary program relations based on the
                   novel notion of a comonadic lax extension, on top of which we
                   define a modal extension of Abramsky's applicative
                   bisimilarity (which we dub modal applicative bisimilarity).
                   We prove such a relation to be a congruence, this way
                   obtaining a compositional technique for reasoning about modal
                   and coeffectful behaviours. But this is not the end of the
                   story: we also establish a correspondence between modal
                   program relations and program distances. This correspondence
                   shows that modal applicative bisimilarity and (a properly
                   extended) applicative bisimilarity distance coincide, this
                   way revealing that modal program equivalences and program
                   distances are just two sides of the same coin.",
  month         =  mar,
  year          =  2021,
  url           = "http://arxiv.org/abs/2103.03871",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LO"
}

@ARTICLE{Bianchini2022-pk,
  title     = "Coeffects for sharing and mutation",
  author    = "Bianchini, Riccardo and Dagnino, Francesco and Giannini, Paola
               and Zucca, Elena and Servetto, Marco",
  journal   = "Proc. ACM Program. Lang.",
  publisher = "Association for Computing Machinery (ACM)",
  volume    =  6,
  number    = "OOPSLA2",
  pages     = "870--898",
  abstract  = "In type-and-coeffect systems , contexts are enriched by coeffects
               modeling how they are actually used, typically through
               annotations on single variables. Coeffects are computed
               bottom-up, combining, for each term, the coeffects of its
               subterms, through a fixed set of algebraic operators. We show
               that this principled approach can be adopted to track sharing in
               the imperative paradigm, that is, links among variables possibly
               introduced by the execution. This provides a significant example
               of non-structural coeffects, which cannot be computed
               by-variable, since the way a given variable is used can affect
               the coeffects of other variables. To illustrate the effectiveness
               of the approach, we enhance the type system tracking sharing to
               model a sophisticated set of features related to uniqueness and
               immutability. Thanks to the coeffect-based approach, we can
               express such features in a simple way and prove related
               properties with standard techniques.",
  month     =  oct,
  year      =  2022,
  url       = "https://dl.acm.org/doi/10.1145/3563319",
  language  = "en"
}

@ARTICLE{Bianchini2023-op,
  title     = "Resource-aware soundness for big-step semantics",
  author    = "Bianchini, Riccardo and Dagnino, Francesco and Giannini, Paola
               and Zucca, Elena",
  journal   = "Proc. ACM Program. Lang.",
  publisher = "Association for Computing Machinery (ACM)",
  volume    =  7,
  number    = "OOPSLA2",
  pages     = "1281--1309",
  abstract  = "We extend the semantics and type system of a lambda calculus
               equipped with common constructs to be resource-aware . That is,
               reduction is instrumented to keep track of the usage of
               resources, and the type system guarantees, besides standard
               soundness, that for well-typed programs there is a computation
               where no needed resource gets exhausted. The resource-aware
               extension is parametric on an arbitrary grade algebra , and does
               not require ad-hoc changes to the underlying language. To this
               end, the semantics needs to be formalized in big-step style; as a
               consequence, expressing and proving (resource-aware) soundness is
               challenging, and is achieved by applying recent techniques based
               on coinductive reasoning.",
  month     =  oct,
  year      =  2023,
  url       = "https://dl.acm.org/doi/10.1145/3622843",
  language  = "en"
}

@INPROCEEDINGS{Dagand2012-du,
  title     = "Transporting functions across ornaments",
  author    = "Dagand, Pierre-Evariste and McBride, Conor",
  booktitle = "Proceedings of the 17th ACM SIGPLAN international conference on
               Functional programming",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "103--114",
  abstract  = "Programming with dependent types is a blessing and a curse. It is
               a blessing to be able to bake invariants into the definition of
               datatypes: we can finally write correct-by-construction software.
               However, this extreme accuracy is also a curse: a datatype is the
               combination of a structuring medium together with a special
               purpose logic. These domain-specific logics hamper any effort of
               code reuse among similarly structured data. In this paper, we
               exorcise our datatypes by adapting the notion of ornament to our
               universe of inductive families. We then show how code reuse can
               be achieved by ornamenting functions. Using these functional
               ornaments, we capture the relationship between functions such as
               the addition of natural numbers and the concatenation of lists.
               With this knowledge, we demonstrate how the implementation of the
               former informs the implementation of the latter: the user can ask
               the definition of addition to be lifted to lists and she will
               only be asked the details necessary to carry on adding lists
               rather than numbers. Our presentation is formalised in a type
               theory with a universe of datatypes and all our constructions
               have been implemented as generic programs, requiring no extension
               to the type theory.",
  series    = "ICFP '12",
  month     =  sep,
  year      =  2012,
  url       = "https://doi.org/10.1145/2364527.2364544",
  keywords  = "datatype, ornament, dependent types"
}

@ARTICLE{Moerdijk2000-tj,
  title    = "Wellfounded trees in categories",
  author   = "Moerdijk, Ieke and Palmgren, Erik",
  journal  = "Ann. Pure Appl. Logic",
  volume   =  104,
  number   =  1,
  pages    = "189--218",
  abstract = "In this paper we present and study a categorical formulation of
              the W-types of Martin-Löf. These are essentially free term
              algebras where the operations may have finite or infinite arity.
              It is shown that W-types are preserved under the construction of
              sheaves and Artin gluing. In the proofs we avoid using
              impredicative or nonconstructive principles.",
  month    =  jul,
  year     =  2000,
  url      = "https://www.sciencedirect.com/science/article/pii/S0168007200000129",
  keywords = "Initial algebras; Locally cartesian closed categories; Pretoposes;
              Artin gluing; Sheaves; Wellfounded trees; Martin-Löf type theory"
}

@ARTICLE{Gambino2009-tj,
  title         = "Polynomial functors and polynomial monads",
  author        = "Gambino, Nicola and Kock, Joachim",
  journal       = "arXiv [math.CT]",
  abstract      = "We study polynomial functors over locally cartesian closed
                   categories. After setting up the basic theory, we show how
                   polynomial functors assemble into a double category, in fact
                   a framed bicategory. We show that the free monad on a
                   polynomial endofunctor is polynomial. The relationship with
                   operads and other related notions is explored.",
  month         =  jun,
  year          =  2009,
  url           = "http://arxiv.org/abs/0906.4931",
  archivePrefix = "arXiv",
  primaryClass  = "math.CT"
}

@INPROCEEDINGS{Williams2014-jr,
  title     = "Ornaments in practice",
  author    = "Williams, Thomas and Dagand, Pierre-Évariste and Rémy, Didier",
  booktitle = "Proceedings of the 10th ACM SIGPLAN workshop on Generic
               programming",
  publisher = "ACM",
  address   = "New York, NY, USA",
  month     =  aug,
  year      =  2014,
  url       = "http://dx.doi.org/10.1145/2633628.2633631"
}

@INPROCEEDINGS{Atanassow2004-yg,
  title     = "Inferring Type Isomorphisms Generically",
  author    = "Atanassow, Frank and Jeuring, Johan",
  booktitle = "Mathematics of Program Construction",
  publisher = "Springer Berlin Heidelberg",
  pages     = "32--53",
  abstract  = "Datatypes which differ inessentially in their names and structure
               are said to be isomorphic; for example, a ternary product is
               isomorphic to a nested pair of binary products. In some canonical
               cases, the conversion function is uniquely determined solely by
               the two types involved. In this article we describe and implement
               a program in Generic Haskell which automatically infers this
               function by normalizing types w.r.t. an algebraic theory of
               canonical isomorphisms. A simple generalization of this technique
               also allows to infer some non-invertible coercions such as
               projections, injections and ad hoc coercions between base types.
               We explain how this technique has been used to drastically
               improve the usability of a Haskell–XML Schema data binding, and
               suggest how it might be applied to improve other type-safe
               language embeddings.",
  year      =  2004,
  url       = "http://dx.doi.org/10.1007/978-3-540-27764-4_4"
}

@ARTICLE{Nanevski2006-au,
  title     = "Polymorphism and separation in hoare type theory",
  author    = "Nanevski, Aleksandar and Morrisett, Greg and Birkedal, Lars",
  journal   = "SIGPLAN Not.",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  volume    =  41,
  number    =  9,
  pages     = "62--73",
  abstract  = "In previous work, we proposed a Hoare Type Theory (HTT) which
               combines effectful higher-order functions, dependent types and
               Hoare Logic specifications into a unified framework. However, the
               framework did not support polymorphism, and ailed to provide a
               modular treatment of state in specifications. In this paper, we
               address these shortcomings by showing that the addition of
               polymorphism alone is sufficient for capturing modular state
               specifications in the style of Separation Logic. Furthermore, we
               argue that polymorphism is an essential ingredient of the
               extension, as the treatment of higher-order functions requires
               operations not encodable via the spatial connectives of
               Separation Logic.",
  month     =  sep,
  year      =  2006,
  url       = "https://doi.org/10.1145/1160074.1159812",
  keywords  = "hoare logic, separation logic, type theory"
}

@INPROCEEDINGS{Von_Oheimb2003-rr,
  title     = "Hoare Logic for Mutual Recursion and Local Variables",
  author    = "von Oheimb, David",
  booktitle = "International Conference on Foundations of Software Technology
               and Theoretical Computer Science",
  publisher = "unknown",
  abstract  = "We present a (the first?)sou nd and relatively complete Hoare
               logic for a simple imperative programming language including
               mutually recursive procedures with call-by-value parameters as
               well as global and local variables. For such a language we
               formalize an operational and an axiomatic semantics of partial
               correctness and prove their equivalence. Global and local
               variables, including parameters, are handled in a rather
               straightforward way allowing for both dynamic and simple static
               scoping. For the completeness proof we employ the powerful MGF
               (Most General Formula)a pproach, introducing and comparing three
               variants for dealing with complications arising from mutual
               recursion. All this work is done using the theorem prover
               Isabelle/HOL, which ensures a rigorous treatment of the subject
               and thus reliable results. The paper gives some new insights in
               the nature of Hoare logic, in particular motivates a stronger
               rule of consequence and a new flexible Call rule.",
  month     =  sep,
  year      =  2003,
  url       = "https://www.researchgate.net/publication/2913331_Hoare_Logic_for_Mutual_Recursion_and_Local_Variables"
}

@INPROCEEDINGS{Nipkow2002-fh,
  title     = "Hoare Logics for Recursive Procedures and Unbounded
               Nondeterminism",
  author    = "Nipkow, Tobias",
  booktitle = "Computer Science Logic",
  publisher = "Springer Berlin Heidelberg",
  pages     = "103--119",
  abstract  = "This paper presents sound and complete Hoare logics for partial
               and total correctness of recursive parameterless procedures in
               the context of unbounded nondeterminism. For total correctness,
               the literature so far has either restricted recursive procedures
               to be deterministic or has studied unbounded nondeterminism only
               in conjunction with loops rather than procedures. We consider
               both single procedures and systems of mutually recursive
               procedures. All proofs have been checked with the theorem prover
               Isabelle/HOL.",
  year      =  2002,
  url       = "http://dx.doi.org/10.1007/3-540-45793-3_8"
}

@INPROCEEDINGS{Jianhua2013-uq,
  title     = "Scope Logic: An Extension to Hoare Logic for Pointers and
               Recursive Data Structures",
  author    = "Jianhua, Zhao and Xuandong, Li",
  booktitle = "Theoretical Aspects of Computing – ICTAC 2013",
  publisher = "Springer Berlin Heidelberg",
  pages     = "409--426",
  abstract  = "This paper presents an extension to Hoare Logic for pointer
               program verification. The main observation leading to this logic
               is that the value of an expression e depends only on the contents
               stored in a finite set of memory units. This set can be specified
               using another expression (called the memory scope of e)
               constructed syntactically from e. A set of construction rules are
               given in this paper for expressions which may contain recursive
               functions (predicates). It is also observed that the memory scope
               of e is a super set of the memory scope of the memory scope of e.
               Based on this, local reasoning can be supported using assertion
               variables which represent arbitrary assertions.
               Program-point-specific expressions are used to specify the
               relations between different program points. Another feature of
               this logic is that for formulas with no user-defined functions,
               the weakest-preconditions can be calculated w.r.t. assignments.",
  year      =  2013,
  url       = "http://dx.doi.org/10.1007/978-3-642-39718-9_24"
}

@ARTICLE{Isaev2016-bm,
  title         = "Model structures on categories of models of type theories",
  author        = "Isaev, Valery",
  journal       = "arXiv [math.CT]",
  abstract      = "Models of dependent type theories are contextual categories
                   with some additional structure. We prove that if a theory $T$
                   has enough structure, then the category
                   $T\text{-}\mathbf{Mod}$ of its models carries the structure
                   of a model category. We also show that if $T$ has $\Sigma$
                   types, then weak equivalences can be characterized in terms
                   of homotopy categories of models.",
  month         =  jul,
  year          =  2016,
  url           = "http://arxiv.org/abs/1607.07407",
  archivePrefix = "arXiv",
  primaryClass  = "math.CT"
}

@ARTICLE{Kapulkin2018-vz,
  title     = "The homotopy theory of type theories",
  author    = "Kapulkin, Krzysztof and Lumsdaine, Peter Lefanu",
  journal   = "Adv. Math.",
  publisher = "Elsevier BV",
  volume    =  337,
  pages     = "1--38",
  abstract  = "We construct a left semi-model structure on the category of
               intensional type theories (precisely, on CxlCatId,1,Σ(,Πext)).
               This presents an ∞-category of such type theories; we show
               moreover that there is an ∞-functor Cl∞ from there to the
               ∞-category of suitably structured quasi-categories. This allows a
               precise formulation of the conjectures that intensional type
               theory gives internal languages for higher categories, and
               provides a framework and toolbox for further progress on these
               conjectures.",
  month     =  oct,
  year      =  2018,
  url       = "https://www.sciencedirect.com/science/article/pii/S0001870818303062",
  keywords  = "Homotopy type theory; Higher category theory; Internal language;
               Model category",
  language  = "en"
}

@BOOK{Awodey2010-xu,
  title     = "Category Theory",
  author    = "Awodey, Steve",
  publisher = "OUP Oxford",
  abstract  = "Category theory is a branch of abstract algebra with incredibly
               diverse applications. This text and reference book is aimed not
               only at mathematicians, but also researchers and students of
               computer science, logic, linguistics, cognitive science,
               philosophy, and any of the other fields in which the ideas are
               being applied. Containing clear definitions of the essential
               concepts, illuminated with numerous accessible examples, and
               providing full proofs of all important propositions and theorems,
               this book aims to make the basic ideas, theorems, and methods of
               category theory understandable to this broad readership. Although
               assuming few mathematical pre-requisites, the standard of
               mathematical rigour is not compromised. The material covered
               includes the standard core of categories; functors; natural
               transformations; equivalence; limits and colimits; functor
               categories; representables; Yoneda's lemma; adjoints; monads. An
               extra topic of cartesian closed categories and the
               lambda-calculus is also provided - a must for computer
               scientists, logicians and linguists! This Second Edition contains
               numerous revisions to the original text, including expanding the
               exposition, revising and elaborating the proofs, providing
               additional diagrams, correcting typographical errors and,
               finally, adding an entirely new section on monoidal categories.
               Nearly a hundred new exercises have also been added, many with
               solutions, to make the book more useful as a course text and for
               self-study.",
  month     =  jun,
  year      =  2010,
  url       = "https://play.google.com/store/books/details?id=xC1bpwAACAAJ",
  language  = "en"
}

@BOOK{Fitting2013-it,
  title     = "First-Order Logic and Automated Theorem Proving",
  author    = "Fitting, Melvin",
  publisher = "Springer New York",
  address   = "New York, NY",
  edition   =  2,
  abstract  = "There are many kinds of books on formal logic. Some have
               philosophers as their intended audience, some mathematicians,
               some computer scien tists. Although there is a common core to all
               such books, they will be very different in emphasis, methods, and
               even appearance. This book is intended for computer scientists.
               But even this is not precise. Within computer science formal
               logic turns up in a number of areas, from pro gram verification
               to logic programming to artificial intelligence. This book is
               intended for computer scientists interested in automated theo rem
               proving in classical logic. To be more precise yet, it is
               essentially a theoretical treatment, not a how-to book, although
               how-to issues are not neglected. This does not mean, of course,
               that the book will be of no interest to philosophers or
               mathematicians. It does contain a thorough presentation of formal
               logic and many proof techniques, and as such it contains all the
               material one would expect to find in a course in formal logic
               covering completeness but, not incompleteness issues. The first
               item to be addressed is, What are we talking about and why are we
               interested in it? We are primarily talking about truth as used in
               mathematical discourse, and our interest in it is, or should be,
               self evident. Truth is a semantic concept, so we begin with
               models and their properties. These are used to define our
               subject.",
  series    = "Texts in Computer Science",
  month     =  jun,
  year      =  2013,
  url       = "https://play.google.com/store/books/details?id=XGoenwEACAAJ",
  language  = "en"
}

@BOOK{MacLane1994-ry,
  title     = "Sheaves in Geometry and Logic: A First Introduction to Topos
               Theory",
  author    = "MacLane, Saunders and Moerdijk, Ieke",
  publisher = "Springer Science \& Business Media",
  abstract  = "We dedicate this book to the memory of J. Frank Adams. His clear
               insights have inspired many mathematicians, including both of us.
               In January 1989, when the first draft of our book had been
               completed, we heard the sad news of his untimely death. This has
               cast a shadow on our subsequent work. Our views of topos theory,
               as presented here, have been shaped by continued study, by
               conferences, and by many personal contacts with friends and
               colleagues-including especially O. Bruno, P. Freyd, J.M.E.
               Hyland, P.T. Johnstone, A. Joyal, A. Kock, F.W. Lawvere, G.E.
               Reyes, R Solovay, R Swan, RW. Thomason, M. Tierney, and G.C.
               Wraith. Our presentation combines ideas and results from these
               people and from many others, but we have not endeavored to
               specify the various original sources. Moreover, a number of
               people have assisted in our work by pro viding helpful comments
               on portions of the manuscript. In this respect, we extend our
               hearty thanks in particular to P. Corazza, K. Edwards, J.
               Greenlees, G. Janelidze, G. Lewis, and S. Schanuel.",
  month     =  oct,
  year      =  1994,
  url       = "https://play.google.com/store/books/details?id=SGwwDerbEowC",
  language  = "en"
}

@BOOK{Marker2002-rh,
  title     = "Model Theory : An Introduction",
  author    = "Marker, David",
  publisher = "Springer Science \& Business Media",
  abstract  = "Assumes only a familiarity with algebra at the beginning graduate
               level; Stresses applications to algebra; Illustrates several of
               the ways Model Theory can be a useful tool in analyzing classical
               mathematical structures",
  month     =  aug,
  year      =  2002,
  url       = "https://play.google.com/store/books/details?id=gkvogoiEnuYC",
  language  = "en"
}

@BOOK{Pierce2004-om,
  title     = "Advanced Topics in Types and Programming Languages",
  author    = "Pierce, Benjamin C",
  publisher = "MIT Press",
  abstract  = "A thorough and accessible introduction to a range of key ideas in
               type systems for programming language.The study of type systems
               for programming languages now touches many areas of computer
               science, from language design and implementation to software
               engineering, network security, databases, and analysis of
               concurrent and distributed systems. This book offers accessible
               introductions to key ideas in the field, with contributions by
               experts on each topic.The topics covered include precise type
               analyses, which extend simple type systems to give them a better
               grip on the run time behavior of systems; type systems for
               low-level languages; applications of types to reasoning about
               computer programs; type theory as a framework for the design of
               sophisticated module systems; and advanced techniques in ML-style
               type inference.Advanced Topics in Types and Programming Languages
               builds on Benjamin Pierce's Types and Programming Languages (MIT
               Press, 2002); most of the chapters should be accessible to
               readers familiar with basic notations and techniques of
               operational semantics and type systems—the material covered in
               the first half of the earlier book.Advanced Topics in Types and
               Programming Languages can be used in the classroom and as a
               resource for professionals. Most chapters include exercises,
               ranging in difficulty from quick comprehension checks to
               challenging extensions, many with solutions.",
  month     =  dec,
  year      =  2004,
  url       = "https://play.google.com/store/books/details?id=A5ic1MPTvVsC",
  language  = "en"
}

@BOOK{Lane2010-oj,
  title     = "Categories for the Working Mathematician",
  author    = "Lane, Saunders Mac",
  publisher = "Springer New York",
  abstract  = "Categories for the Working Mathematician provides an array of
               general ideas useful in a wide variety of fields. Starting from
               the foundations, this book illuminates the concepts of category,
               functor, natural transformation, and duality. The book then turns
               to adjoint functors, which provide a description of universal
               constructions, an analysis of the representations of functors by
               sets of morphisms, and a means of manipulating direct and inverse
               limits. These categorical concepts are extensively illustrated in
               the remaining chapters, which include many applications of the
               basic existence theorem for adjoint functors. The categories of
               algebraic systems are constructed from certain adjoint-like data
               and characterized by Beck's theorem. After considering a variety
               of applications, the book continues with the construction and
               exploitation of Kan extensions. This second edition includes a
               number of revisions and additions, including two new chapters on
               topics of active interest. One is on symmetric monoidal
               categories and braided monoidal categories and the coherence
               theorems for them. The second describes 2-categories and the
               higher dimensional categories which have recently come into
               prominence. The bibliography has also been expanded to cover some
               of the many other recent advances concerning categories.",
  month     =  nov,
  year      =  2010,
  url       = "https://play.google.com/store/books/details?id=cUNdcgAACAAJ",
  language  = "en"
}

@BOOK{Lurie2009-qi,
  title     = "Higher Topos Theory",
  author    = "Lurie, Jacob",
  publisher = "Princeton University Press",
  address   = "Princeton, NJ",
  abstract  = "Higher category theory is generally regarded as technical and
               forbidding, but part of it is considerably more tractable: the
               theory of infinity-categories, higher categories in which all
               higher morphisms are assumed to be invertible. In Higher Topos
               Theory, Jacob Lurie presents the foundations of this theory,
               using the language of weak Kan complexes introduced by Boardman
               and Vogt, and shows how existing theorems in algebraic topology
               can be reformulated and generalized in the theory's new language.
               The result is a powerful theory with applications in many areas
               of mathematics. The book's first five chapters give an exposition
               of the theory of infinity-categories that emphasizes their role
               as a generalization of ordinary categories. Many of the
               fundamental ideas from classical category theory are generalized
               to the infinity-categorical setting, such as limits and colimits,
               adjoint functors, ind-objects and pro-objects, locally accessible
               and presentable categories, Grothendieck fibrations, presheaves,
               and Yoneda's lemma. A sixth chapter presents an
               infinity-categorical version of the theory of Grothendieck topoi,
               introducing the notion of an infinity-topos, an infinity-category
               that resembles the infinity-category of topological spaces in the
               sense that it satisfies certain axioms that codify some of the
               basic principles of algebraic topology. A seventh and final
               chapter presents applications that illustrate connections between
               the theory of higher topoi and ideas from classical topology.",
  series    = "Annals of Mathematics Studies",
  month     =  jul,
  year      =  2009,
  url       = "https://play.google.com/store/books/details?id=aSZA6ojL3kwC",
  language  = "en"
}

@BOOK{Aluffi2009-sp,
  title     = "Algebra: Chapter 0",
  author    = "Aluffi, Paolo",
  publisher = "American Mathematical Soc.",
  abstract  = "Algebra: Chapter 0 is a self-contained introduction to the main
               topics of algebra, suitable for a first sequence on the subject
               at the beginning graduate or upper undergraduate level. The
               primary distinguishing feature of the book, compared to standard
               textbooks in algebra, is the early introduction of categories,
               used as a unifying theme in the presentation of the main topics.
               A second feature consists of an emphasis on homological algebra:
               basic notions on complexes are presented as soon as modules have
               been introduced, and an extensive last chapter on homological
               algebra can form the basis for a follow-up introductory course on
               the subject. Approximately 1,000 exercises both provide adequate
               practice to consolidate the understanding of the main body of the
               text and offer the opportunity to explore many other topics,
               including applications to number theory and algebraic geometry.
               This will allow instructors to adapt the textbook to their
               specific choice of topics and provide the independent reader with
               a richer exposure to algebra. Many exercises include substantial
               hints, and navigation of the topics is facilitated by an
               extensive index and by hundreds of cross-references.",
  year      =  2009,
  url       = "https://play.google.com/store/books/details?id=deWkZWYbyHQC",
  language  = "en"
}

@ARTICLE{The_Univalent_Foundations_Program2013-bi,
  title         = "Homotopy Type Theory: Univalent Foundations of Mathematics",
  author        = "{The Univalent Foundations Program}",
  journal       = "arXiv [math.LO]",
  abstract      = "Homotopy type theory is a new branch of mathematics, based on
                   a recently discovered connection between homotopy theory and
                   type theory, which brings new ideas into the very foundation
                   of mathematics. On the one hand, Voevodsky's subtle and
                   beautiful ``univalence axiom'' implies that isomorphic
                   structures can be identified. On the other hand, ``higher
                   inductive types'' provide direct, logical descriptions of
                   some of the basic spaces and constructions of homotopy
                   theory. Both are impossible to capture directly in classical
                   set-theoretic foundations, but when combined in homotopy type
                   theory, they permit an entirely new kind of ``logic of
                   homotopy types''. This suggests a new conception of
                   foundations of mathematics, with intrinsic homotopical
                   content, an ``invariant'' conception of the objects of
                   mathematics -- and convenient machine implementations, which
                   can serve as a practical aid to the working mathematician.
                   This book is intended as a first systematic exposition of the
                   basics of the resulting ``Univalent Foundations'' program,
                   and a collection of examples of this new style of reasoning
                   -- but without requiring the reader to know or learn any
                   formal logic, or to use any computer proof assistant.",
  month         =  aug,
  year          =  2013,
  url           = "http://arxiv.org/abs/1308.0729",
  archivePrefix = "arXiv",
  primaryClass  = "math.LO"
}

@BOOK{Brown2006-pg,
  title     = "Topology and Groupoids",
  author    = "Brown, Ronald",
  publisher = "www.groupoids.org",
  abstract  = "This is the third edition of a classic text, previously published
               in 1968, 1988, and now extended, revised, retitled, updated, and
               reasonably priced. Throughout it gives motivation and context for
               theorems and definitions. Thus the definition of a topology is
               first related to the example of the real line; it is then given
               in terms of the intuitive notion of neighbourhoods, and then
               shown to be equivalent to the elegant but spare definition in
               terms of open sets. Many constructions of topologies are shown to
               be necessitated by the desire to construct continuous functions,
               either from or into a space. This is in the modern categorical
               spirit, and often leads to clearer and simpler proofs. There is a
               full treatment of finite cell complexes, with the cell
               decompositions given of projective spaces, in the real, complex
               and quaternionic cases. This is based on an exposition of
               identification spaces and adjunction spaces. The exposition of
               general topology ends with a description of the topology for
               function spaces, using the modern treatment of the test-open
               topology, from compact Hausdorff spaces, and so a description of
               a convenient category of spaces (a term due to the author) in the
               non Hausdorff case. The second half of the book demonstrates how
               the use of groupoids rather than just groups gives in
               1-dimensional homotopy theory more powerful theorems with simpler
               proofs. Some of the proofs of results on the fundamental groupoid
               would be difficult to envisage except in the form given: `We
               verify the required universal property'. This is in the modern
               categorical spirit. Chapter 6 contains the development of the
               fundamental groupoid on a set of base points, including the
               background in category theory. The proof of the van Kampen
               Theorem in this general form resolves a failure of traditional
               treatments, in giving a direct computation of the fundamental
               group of the circle, as well as more complicated examples.
               Chapter 7 uses the notion of cofibration to develop the notion of
               operations of the fundamental groupoid on certain sets of
               homotopy classes. This allows for an important theorem on gluing
               homotopy equivalences by a method which gives control of the
               homotopies involved. This theorem first appeared in the 1968
               edition. Also given is the family of exact sequences arising from
               a fibration of groupoids. The development of Combinatorial
               Groupoid Theory in Chapter 8 allows for unified treatments of
               free groups, free products of groups, and HNN-extensions, in
               terms of pushouts of groupoids, and well models the topology of
               gluing spaces together. These methods lead in Chapter 9 to
               results on the Phragmen-Brouwer Property, with a Corollary that
               the complement of any arc in an n-sphere is connected, and then
               to a proof of the Jordan Curve Theorem. Chapter 10 on covering
               spaces is again fully in the base point free spirit; it proves
               the natural theorem that for suitable spaces X, the category of
               covering spaces of X is equivalent to the category of covering
               morphisms of the fundamental groupoid of X. This approach gives a
               convenient way of obtaining covering maps from covering
               morphisms, and leads easily to traditional results using
               operations of the fundamental group. Results on pullbacks of
               coverings are proved using a Mayer-Vietoris type sequence. No
               other text treats the whole theory directly in this way. Chapter
               11 is on Orbit Spaces and Orbit Groupoids, and gives conditions
               for the fundamental groupoid of the orbit space to be the orbit
               groupoid of the fundamental groupoid. No other topology text
               treats this important area. Comments on the relations to the
               literature are given in Notes at the end of each Chapter. There
               are over 500 exercises, 114 figures, numerous diagrams. See http:
               //www.bangor.ac.uk/r.brown/topgpds.html for more information. See
               http: //mathdl.maa.org/mathDL/19/?rpa=reviews\&sa=viewBook\&
               bookId=69421 for a Mathematical Association of America review.",
  year      =  2006,
  url       = "https://play.google.com/store/books/details?id=U9mMQAAACAAJ",
  language  = "en"
}

@BOOK{Bordeaux2014-wq,
  title     = "Tractability: Practical Approaches to Hard Problems",
  author    = "Bordeaux, Lucas and Hamadi, Youssef and Kohli, Pushmeet",
  publisher = "Cambridge University Press",
  abstract  = "Classical computer science textbooks tell us that some problems
               are 'hard'. Yet many areas, from machine learning and computer
               vision to theorem proving and software verification, have defined
               their own set of tools for effectively solving complex problems.
               Tractability provides an overview of these different techniques,
               and of the fundamental concepts and properties used to tame
               intractability. This book will help you understand what to do
               when facing a hard computational problem. Can the problem be
               modelled by convex, or submodular functions? Will the instances
               arising in practice be of low treewidth, or exhibit another
               specific graph structure that makes them easy? Is it acceptable
               to use scalable, but approximate algorithms? A wide range of
               approaches is presented through self-contained chapters written
               by authoritative researchers on each topic. As a reference on a
               core problem in computer science, this book will appeal to
               theoreticians and practitioners alike.",
  month     =  feb,
  year      =  2014,
  url       = "https://play.google.com/store/books/details?id=YxliAgAAQBAJ",
  language  = "en"
}

@BOOK{Pierce2002-yz,
  title     = "Types and Programming Languages",
  author    = "Pierce, Benjamin C",
  publisher = "MIT Press",
  abstract  = "A comprehensive introduction to type systems and programming
               languages.A type system is a syntactic method for automatically
               checking the absence of certain erroneous behaviors by
               classifying program phrases according to the kinds of values they
               compute. The study of type systems—and of programming languages
               from a type-theoretic perspective—has important applications in
               software engineering, language design, high-performance
               compilers, and security.This text provides a comprehensive
               introduction both to type systems in computer science and to the
               basic theory of programming languages. The approach is pragmatic
               and operational; each new concept is motivated by programming
               examples and the more theoretical sections are driven by the
               needs of implementations. Each chapter is accompanied by numerous
               exercises and solutions, as well as a running implementation,
               available via the Web. Dependencies between chapters are
               explicitly identified, allowing readers to choose a variety of
               paths through the material.The core topics include the untyped
               lambda-calculus, simple type systems, type reconstruction,
               universal and existential polymorphism, subtyping, bounded
               quantification, recursive types, kinds, and type operators.
               Extended case studies develop a variety of approaches to modeling
               the features of object-oriented languages.",
  month     =  jan,
  year      =  2002,
  url       = "https://play.google.com/store/books/details?id=ti6zoAC9Ph8C",
  language  = "en"
}

@PHDTHESIS{Ivaskovic_undated-wf,
  title  = "Programming and static analysis with graded monads",
  author = "Ivašković, Andrej",
  url    = "https://api.repository.cam.ac.uk/server/api/core/bitstreams/901716b2-87aa-43e4-94d3-d8e0902edce1/content"
}

@BOOK{Jacobs1999-lw,
  title     = "Categorical Logic and Type Theory",
  author    = "Jacobs, B",
  publisher = "Elsevier",
  address   = "Amsterdam",
  abstract  = "This book is an attempt to give a systematic presentation of both
               logic and type theory from a categorical perspective, using the
               unifying concept of fibred category. Its intended audience
               consists of logicians, type theorists, category theorists and
               (theoretical) computer scientists.",
  series    = "Studies in Logic and the Foundations of Mathematics",
  month     =  jan,
  year      =  1999,
  url       = "https://play.google.com/store/books/details?id=f4_Bd3Y8ZEcC",
  language  = "en"
}

@BOOK{Levy_undated-lw,
  title     = "Call-By-Push-Value",
  author    = "Levy, Paul Blain",
  publisher = "Springer Netherlands",
  url       = "https://link.springer.com/book/10.1007/978-94-007-0954-6"
}

@BOOK{Borceux1994-dr,
  title     = "Handbook of Categorical Algebra: Volume 1, Basic Category Theory",
  author    = "Borceux, Francis",
  publisher = "Cambridge University Press",
  abstract  = "The Handbook of Categorical Algebra is designed to give, in three
               volumes, a detailed account of what should be known by everybody
               working in, or using, category theory. As such it will be a
               unique reference. The volumes are written in sequence, with the
               first being essentially self-contained, and are accessible to
               graduate students with a good background in mathematics. In
               particular, Volume 1, which is devoted to general concepts, can
               be used for advanced undergraduate courses on category theory.",
  month     =  aug,
  year      =  1994,
  url       = "https://play.google.com/store/books/details?id=YfzImoopB-IC",
  language  = "en"
}

@ARTICLE{Loregian2015-ww,
  title         = "Coend calculus",
  author        = "Loregian, Fosco",
  journal       = "arXiv [math.CT]",
  abstract      = "The book formerly known as ``This is the (co)end, my only
                   (co)friend''.",
  month         =  jan,
  year          =  2015,
  url           = "http://arxiv.org/abs/1501.02503",
  archivePrefix = "arXiv",
  primaryClass  = "math.CT"
}

@ARTICLE{Maillard2019-cj,
  title         = "Dijkstra Monads for All",
  author        = "Maillard, Kenji and Ahman, Danel and Atkey, Robert and
                   Martinez, Guido and Hritcu, Catalin and Rivas, Exequiel and
                   Tanter, Éric",
  journal       = "arXiv [cs.PL]",
  abstract      = "This paper proposes a general semantic framework for
                   verifying programs with arbitrary monadic side-effects using
                   Dijkstra monads, which we define as monad-like structures
                   indexed by a specification monad. We prove that any monad
                   morphism between a computational monad and a specification
                   monad gives rise to a Dijkstra monad, which provides great
                   flexibility for obtaining Dijkstra monads tailored to the
                   verification task at hand. We moreover show that a large
                   variety of specification monads can be obtained by applying
                   monad transformers to various base specification monads,
                   including predicate transformers and Hoare-style pre- and
                   postconditions. For defining correct monad transformers, we
                   propose a language inspired by Moggi's monadic metalanguage
                   that is parameterized by a dependent type theory. We also
                   develop a notion of algebraic operations for Dijkstra monads,
                   and start to investigate two ways of also accommodating
                   effect handlers. We implement our framework in both Coq and
                   F*, and illustrate that it supports a wide variety of
                   verification styles for effects such as exceptions,
                   nondeterminism, state, input-output, and general recursion.",
  month         =  mar,
  year          =  2019,
  url           = "http://arxiv.org/abs/1903.01237",
  archivePrefix = "arXiv",
  primaryClass  = "cs.PL"
}

@ARTICLE{Apt2019-oh,
  title         = "Fifty years of Hoare's Logic",
  author        = "Apt, Krzysztof R and Olderog, Ernst-Ruediger",
  journal       = "arXiv [cs.LO]",
  abstract      = "We present a history of Hoare's logic.",
  month         =  apr,
  year          =  2019,
  url           = "http://arxiv.org/abs/1904.03917",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LO"
}

@ARTICLE{Maillard2020-cr,
  title     = "The next 700 relational program logics",
  author    = "Maillard, Kenji and Hriţcu, Cătălin and Rivas, Exequiel and Van
               Muylder, Antoine",
  journal   = "Proc. ACM Program. Lang.",
  publisher = "Association for Computing Machinery (ACM)",
  volume    =  4,
  number    = "POPL",
  pages     = "1--33",
  abstract  = "We propose the first framework for defining relational program
               logics for arbitrary monadic effects. The framework is embedded
               within a relational dependent type theory and is highly
               expressive. At the semantic level, we provide an algebraic
               presentation of relational specifications as a class of relative
               monads, and link computations and specifications by introducing
               relational effect observations, which map pairs of monadic
               computations to relational specifications in a way that respects
               the algebraic structure. For an arbitrary relational effect
               observation, we generically define the core of a sound relational
               program logic, and explain how to complete it to a full-fledged
               logic for the monadic effect at hand. We show that this generic
               framework can be used to define relational program logics for
               effects as diverse as state, input-output, nondeterminism, and
               discrete probabilities. We, moreover, show that by instantiating
               our framework with state and unbounded iteration we can embed a
               variant of Benton's Relational Hoare Logic, and also sketch how
               to reconstruct Relational Hoare Type Theory. Finally, we identify
               and overcome conceptual challenges that prevented previous
               relational program logics from properly dealing with control
               effects, and are the first to provide a relational program logic
               for exceptions.",
  month     =  jan,
  year      =  2020,
  url       = "http://dx.doi.org/10.1145/3371072",
  language  = "en"
}

@INCOLLECTION{Brauer2011-wt,
  title     = "Existential quantification as incremental {SAT}",
  author    = "Brauer, Jörg and King, Andy and Kriener, Jael",
  booktitle = "Computer Aided Verification",
  publisher = "Springer Berlin Heidelberg",
  address   = "Berlin, Heidelberg",
  pages     = "191--207",
  series    = "Lecture notes in computer science",
  year      =  2011,
  url       = "http://dx.doi.org/10.1007/978-3-642-22110-1_17"
}

@ARTICLE{Dagand2012-aw,
  title         = "A Categorical Treatment of Ornaments",
  author        = "Dagand, Pierre-Evariste and McBride, Conor",
  journal       = "arXiv [cs.PL]",
  abstract      = "Ornaments aim at taming the multiplication of special-purpose
                   datatype in dependently-typed theory. In its original form,
                   the definition of ornaments is tied to a particular universe
                   of datatypes. Being a type theoretic object, constructions on
                   ornaments are typically explained through an operational
                   narrative. This overbearing concreteness calls for an
                   abstract model of ornaments. In this paper, we give a
                   categorical model of ornaments. As a necessary first step, we
                   abstract the universe of datatypes using the theory of
                   polynomial functors. We are then able to characterize
                   ornaments as cartesian morphisms between polynomial functors.
                   We thus gain access to powerful mathematical tools that shall
                   help us understand and develop ornaments. We shall also
                   illustrate the adequacy of our model. Firstly, we rephrase
                   the standard ornamental constructions into our framework.
                   Thanks to its conciseness, this process gives us a deeper
                   understanding of the structures at play. Secondly, we develop
                   new ornamental constructions, by translating categorical
                   structures into type theoretic artifacts.",
  month         =  dec,
  year          =  2012,
  url           = "http://arxiv.org/abs/1212.3806",
  archivePrefix = "arXiv",
  primaryClass  = "cs.PL"
}

@ARTICLE{Atkey2012-nq,
  title         = "Refining Inductive Types",
  author        = "Atkey, Robert and Johann, Patricia and Ghani, Neil",
  journal       = "arXiv [cs.LO]",
  abstract      = "Dependently typed programming languages allow sophisticated
                   properties of data to be expressed within the type system. Of
                   particular use in dependently typed programming are indexed
                   types that refine data by computationally useful information.
                   For example, the N-indexed type of vectors refines lists by
                   their lengths. Other data types may be refined in similar
                   ways, but programmers must produce purpose-specific
                   refinements on an ad hoc basis, developers must anticipate
                   which refinements to include in libraries, and
                   implementations must often store redundant information about
                   data and their refinements. In this paper we show how to
                   generically derive inductive characterisations of refinements
                   of inductive types, and argue that these characterisations
                   can alleviate some of the aforementioned difficulties
                   associated with ad hoc refinements. Our characterisations
                   also ensure that standard techniques for programming with and
                   reasoning about inductive types are applicable to
                   refinements, and that refinements can themselves be further
                   refined.",
  month         =  may,
  year          =  2012,
  url           = "http://arxiv.org/abs/1205.2492",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LO"
}

@INPROCEEDINGS{Gaboardi2021-li,
  title     = "Graded Hoare Logic and its Categorical Semantics",
  author    = "Gaboardi, Marco and Katsumata, Shin-Ya and Orchard, Dominic and
               Sato, Tetsuya",
  booktitle = "Programming Languages and Systems",
  publisher = "Springer International Publishing",
  pages     = "234--263",
  abstract  = "Deductive verification techniques based on program logics (i.e.,
               the family of Floyd-Hoare logics) are a powerful approach for
               program reasoning. Recently, there has been a trend of increasing
               the expressive power of such logics by augmenting their rules
               with additional information to reason about program side-effects.
               For example, general program logics have been augmented with cost
               analyses, logics for probabilistic computations have been
               augmented with estimate measures, and logics for differential
               privacy with indistinguishability bounds. In this work, we unify
               these various approaches via the paradigm of grading, adapted
               from the world of functional calculi and semantics. We propose
               Graded Hoare Logic (GHL), a parameterisable framework for
               augmenting program logics with a preordered monoidal analysis. We
               develop a semantic framework for modelling GHL such that grading,
               logical assertions (pre- and post-conditions) and the underlying
               effectful semantics of an imperative language can be integrated
               together. Central to our framework is the notion of a graded
               category which we extend here, introducing graded Freyd
               categories which provide a semantics that can interpret many
               examples of augmented program logics from the literature. We
               leverage coherent fibrations to model the base assertion
               language, and thus the overall setting is also fibrational.",
  year      =  2021,
  url       = "http://dx.doi.org/10.1007/978-3-030-72019-3_9"
}

@ARTICLE{Marshall2022-zq,
  title         = "Replicate, Reuse, Repeat: Capturing Non-Linear Communication
                   via Session Types and Graded Modal Types",
  author        = "Marshall, Daniel and Orchard, Dominic",
  journal       = "arXiv [cs.PL]",
  abstract      = "Session types provide guarantees about concurrent behaviour
                   and can be understood through their correspondence with
                   linear logic, with propositions as sessions and proofs as
                   processes. However, a strictly linear setting is somewhat
                   limiting, since there exist various useful patterns of
                   communication that rely on non-linear behaviours. For
                   example, shared channels provide a way to repeatedly spawn a
                   process with binary communication along a fresh linear
                   session-typed channel. Non-linearity can be introduced in a
                   controlled way in programming through the general concept of
                   graded modal types, which are a framework encompassing
                   various kinds of coeffect typing (describing how computations
                   make demands on their context). This paper shows how graded
                   modal types can be leveraged alongside session types to
                   enable various non-linear concurrency behaviours to be
                   re-introduced in a precise manner in a type system with a
                   linear basis. The ideas here are demonstrated using Granule,
                   a functional programming language with linear, indexed, and
                   graded modal types.",
  month         =  mar,
  year          =  2022,
  url           = "http://arxiv.org/abs/2203.12875",
  archivePrefix = "arXiv",
  primaryClass  = "cs.PL"
}

@ARTICLE{Reynaud2021-gs,
  title     = "A practical mode system for recursive definitions",
  author    = "Reynaud, Alban and Scherer, Gabriel and Yallop, Jeremy",
  journal   = "Proc. ACM Program. Lang.",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  volume    =  5,
  number    = "POPL",
  pages     = "1--29",
  abstract  = "In call-by-value languages, some mutually-recursive definitions
               can be safely evaluated to build recursive functions or cyclic
               data structures, but some definitions (let rec x = x + 1) contain
               vicious circles and their evaluation fails at runtime. We propose
               a new static analysis to check the absence of such runtime
               failures. We present a set of declarative inference rules, prove
               its soundness with respect to the reference source-level
               semantics of Nordlander, Carlsson, and Gill [2008], and show that
               it can be directed into an algorithmic backwards analysis check
               in a surprisingly simple way. Our implementation of this new
               check replaced the existing check used by the OCaml programming
               language, a fragile syntactic criterion which let several subtle
               bugs slip through as the language kept evolving. We document some
               issues that arise when advanced features of a real-world
               functional language (exceptions in first-class modules, GADTs,
               etc.) interact with safety checking for recursive definitions.",
  month     =  jan,
  year      =  2021,
  url       = "https://doi.org/10.1145/3434326",
  keywords  = "functional programming, call-by-value, ML, recursion, semantics,
               types"
}

@ARTICLE{Abel2023-ey,
  title     = "A Graded Modal Dependent Type Theory with a Universe and Erasure,
               Formalized",
  author    = "Abel, Andreas and Danielsson, Nils Anders and Eriksson, Oskar",
  journal   = "Proc. ACM Program. Lang.",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  volume    =  7,
  number    = "ICFP",
  pages     = "920--954",
  abstract  = "We present a graded modal type theory, a dependent type theory
               with grades that can be used to enforce various properties of the
               code. The theory has Π-types, weak and strong Σ-types, natural
               numbers, an empty type, and a universe, and we also extend the
               theory with a unit type and graded Σ-types. The theory is
               parameterized by a modality, a kind of partially ordered
               semiring, whose elements (grades) are used to track the usage of
               variables in terms and types. Different modalities are possible.
               We focus mainly on quantitative properties, in particular
               erasure: with the erasure modality one can mark function
               arguments as erasable.The theory is fully formalized in Agda. The
               formalization, which uses a syntactic Kripke logical relation at
               its core and is based on earlier work, establishes major
               meta-theoretic properties such as subject reduction, consistency,
               normalization, and decidability of definitional equality. We also
               prove a substitution theorem for grade assignment, and
               preservation of grades under reduction. Furthermore we study an
               extraction function that translates terms to an untyped
               λ-calculus and removes erasable content, in particular function
               arguments with the “erasable” grade. For a certain class of
               modalities we prove that extraction is sound, in the sense that
               programs of natural number type have the same value before and
               after extraction. Soundness of extraction holds also for open
               programs, as long as all variables in the context are erasable,
               the context is consistent, and erased matches are not allowed for
               weak Σ-types.",
  month     =  aug,
  year      =  2023,
  url       = "https://doi.org/10.1145/3607862",
  keywords  = "linearity, dependent types, modalities, erasure, formalization,
               graded modal type theory"
}

@INPROCEEDINGS{Moon2021-eb,
  title     = "Graded Modal Dependent Type Theory",
  author    = "Moon, Benjamin and Eades, III, Harley and Orchard, Dominic",
  booktitle = "Programming Languages and Systems",
  publisher = "Springer International Publishing",
  pages     = "462--490",
  abstract  = "Graded type theories are an emerging paradigm for augmenting the
               reasoning power of types with parameterizable, fine-grained
               analyses of program properties. There have been many such
               theories in recent years which equip a type theory with
               quantitative dataflow tracking, usually via a semiring-like
               structure which provides analysis on variables (often called
               ‘quantitative’ or ‘coeffect’ theories). We present Graded Modal
               Dependent Type Theory (Grtt for short), which equips a dependent
               type theory with a general, parameterizable analysis of the flow
               of data, both in and between computational terms and types. In
               this theory, it is possible to study, restrict, and reason about
               data use in programs and types, enabling, for example, parametric
               quantifiers and linearity to be captured in a dependent setting.
               We propose Grtt, study its metatheory, and explore various case
               studies of its use in reasoning about programs and studying other
               type theories. We have implemented the theory and highlight the
               interesting details, including showing an application of grading
               to optimising the type checking procedure itself.",
  year      =  2021,
  url       = "http://dx.doi.org/10.1007/978-3-030-72019-3_17"
}

@ARTICLE{Choudhury2020-hq,
  title         = "A graded dependent type system with a usage-aware semantics
                   (extended version)",
  author        = "Choudhury, Pritam and Eades, III, Harley and Eisenberg,
                   Richard A and Weirich, Stephanie C",
  journal       = "arXiv [cs.PL]",
  abstract      = "Graded Type Theory provides a mechanism to track and reason
                   about resource usage in type systems. In this paper, we
                   develop GraD, a novel version of such a graded dependent type
                   system that includes functions, tensor products, additive
                   sums, and a unit type. Since standard operational semantics
                   is resource-agnostic, we develop a heap-based operational
                   semantics and prove a soundness theorem that shows correct
                   accounting of resource usage. Several useful properties,
                   including the standard type soundness theorem,
                   non-interference of irrelevant resources in computation and
                   single pointer property for linear resources, can be derived
                   from this theorem. We hope that our work will provide a base
                   for integrating linearity, irrelevance and dependent types in
                   practical programming languages like Haskell.",
  month         =  nov,
  year          =  2020,
  url           = "http://arxiv.org/abs/2011.04070",
  archivePrefix = "arXiv",
  primaryClass  = "cs.PL"
}

@ARTICLE{McBride2014-qo,
  title    = "Ornamental Algebras, Algebraic Ornaments",
  author   = "McBride, Conor",
  abstract = "This paper re-examines the presentation of datatypes in
              dependently typed languages, addressing in particular the issue of
              what it means for one datatype to be in various ways more
              informative than another. This paper re-examines the presentation
              of datatypes in dependently typed languages, addressing in
              particular the issue of what it means for one datatype to be in
              various ways more informative than another. Informal human
              observations like ‘lists are natural numbers with extra
              decoration’ and ‘vectors are lists indexed by length’ are
              expressed in a first class language of ornaments — presentations
              of fancy new types based on plain old ones — encompassing both
              decoration and, in the sense of Tim Freeman and Frank Pfenning
              (1991), refinement. Each ornament adds information, so it comes
              with a forgetful function from fancy data back to plain,
              expressible as the fold of its ornamental algebra: lists built
              from numbers acquire the ‘length’ algebra. Conversely, each
              algebra for a datatype induces a way to index it — an algebraic
              ornament. The length algebra for lists induces the construction of
              the paradigmatic dependent vector types. Dependent types thus
              provide not only a new ‘axis of diversity’ — indexing — for data
              structures, but also new abstractions to manage and exploit that
              diversity. In the spirit of ‘the new programming’ (McBride \&
              McKinna, 2004), the engineering of coincidence is replaced by the
              propagation of consequence.",
  year     =  2014,
  url      = "https://www.semanticscholar.org/paper/e16519d681e959d3919c4c256a5cd36853075697",
  language = "en"
}

@INPROCEEDINGS{Ringer2018-vo,
  title     = "Adapting proof automation to adapt proofs",
  author    = "Ringer, Talia and Yazdani, Nathaniel and Leo, John and Grossman,
               Dan",
  booktitle = "Proceedings of the 7th ACM SIGPLAN International Conference on
               Certified Programs and Proofs",
  publisher = "ACM",
  address   = "New York, NY, USA",
  abstract  = "We extend proof automation in an interactive theorem prover to
               analyze changes in specifications and proofs. Our approach
               leverages the history of changes to specifications and proofs to
               search for a patch that can be applied to other specifications
               and proofs that need to change in analogous ways. We identify and
               implement five core components that are key to searching for a
               patch. We build a patch finding procedure from these components,
               which we configure for various classes of changes. We implement
               this procedure in a Coq plugin as a proof-of-concept and use it
               on real Coq code to change specifications, port definitions of a
               type, and update the Coq standard library. We show how our
               findings help drive a future that moves the burden of dealing
               with the brittleness of small changes in an interactive theorem
               prover away from the programmer and into automated tooling.",
  month     =  jan,
  year      =  2018,
  url       = "https://dl.acm.org/doi/10.1145/3167094",
  language  = "en"
}

@INPROCEEDINGS{Dagand2016-xi,
  title     = "Partial type equivalences for verified dependent interoperability",
  author    = "Dagand, Pierre-Evariste and Tabareau, Nicolas and Tanter, Éric",
  booktitle = "Proceedings of the 21st ACM SIGPLAN International Conference on
               Functional Programming",
  publisher = "ACM",
  address   = "New York, NY, USA",
  abstract  = "Full-spectrum dependent types promise to enable the development
               of correct-by-construction software. However, even certified
               software needs to interact with simply-typed or untyped programs,
               be it to perform system calls, or to use legacy libraries.
               Trading static guarantees for runtime checks, the dependent
               interoperability framework provides a mechanism by which
               simply-typed values can safely be coerced to dependent types and,
               conversely, dependently-typed programs can defensively be
               exported to a simply-typed application. In this paper, we give a
               semantic account of dependent interoperability. Our presentation
               relies on and is guided by a pervading notion of type
               equivalence, whose importance has been emphasized in recent work
               on homotopy type theory. Specifically, we develop the notion of
               partial type equivalences as a key foundation for dependent
               interoperability. Our framework is developed in Coq; it is thus
               constructive and verified in the strictest sense of the terms.
               Using our library, users can specify domain-specific partial
               equivalences between data structures. Our library then takes care
               of the (sometimes, heavy) lifting that leads to interoperable
               programs. It thus becomes possible, as we shall illustrate, to
               internalize and hand-tune the extraction of dependently-typed
               programs to interoperable OCaml programs within Coq itself.",
  month     =  sep,
  year      =  2016,
  url       = "https://dl.acm.org/doi/10.1145/2951913.2951933",
  language  = "en"
}

@ARTICLE{Allais2023-rg,
  title     = "Frex: dependently-typed algebraic simplification",
  author    = "Allais, Guillaume and Brady, Edwin and Corbyn, Nathan and Kammar,
               Ohad and Yallop, Jeremy",
  journal   = "arXiv.org",
  publisher = "arXiv",
  abstract  = "We present an extensible, mathematically-structured algebraic
               simplification library design. We structure the library using
               universal algebraic concepts: a free algebra -- fral -- and a
               free extension -- frex -- of an algebra by a set of variables.
               The library's dependently-typed API guarantees simplification
               modules, even user-defined ones, are terminating, sound, and
               complete with respect to a well-specified class of equations.
               Completeness offers intangible benefits in practice -- our main
               contribution is the novel design. Cleanly separating between the
               interface and implementation of simplification modules provides
               two new modularity axes. First, simplification modules share
               thousands of lines of infrastructure code dealing with
               term-representation, pretty-printing, certification, and
               macros/reflection. Second, new simplification modules can reuse
               existing ones. We demonstrate this design by developing
               simplification modules for monoid varieties: ordinary,
               commutative, and involutive. We implemented this design in the
               new Idris2 dependently-typed programming language, and in Agda.",
  year      =  2023,
  url       = "http://dx.doi.org/10.48550/ARXIV.2306.15375",
  language  = "en"
}

@ARTICLE{Williams2018-to,
  title     = "A principled approach to ornamentation in {ML}",
  author    = "Williams, Thomas and Rémy, Didier",
  journal   = "Proc. ACM Program. Lang.",
  publisher = "Association for Computing Machinery (ACM)",
  volume    =  2,
  number    = "POPL",
  pages     = "1--30",
  abstract  = "Ornaments are a way to describe changes in datatype definitions
               reorganizing, adding, or dropping some pieces of data so that
               functions operating on the bare definition can be partially and
               sometimes totally lifted into functions operating on the
               ornamented structure. We propose an extension of ML with
               higher-order ornaments, demonstrate its expressiveness with a few
               typical examples, including code refactoring, study the
               metatheoretical properties of ornaments, and describe their
               elaboration process. We formalize ornamentation via an a
               posteriori abstraction of the bare code, returning a generic
               term, which lives in a meta-language above ML. The lifted code is
               obtained by application of the generic term to well-chosen
               arguments, followed by staged reduction, and some remaining
               simplifications. We use logical relations to closely relate the
               lifted code to the bare code.",
  month     =  jan,
  year      =  2018,
  url       = "https://dl.acm.org/doi/10.1145/3158109",
  language  = "en"
}

@ARTICLE{Williams2017-hq,
  title    = "A {MetaLanguage} for ornamentation in {ML}",
  author   = "Williams, Thomas and Rémy, Didier",
  abstract = "A meta-language above ML is introduced in which a most generic
              lifting of bare code is elaborate, so that ornamented code can
              then be obtained by instantiation of the generic lifting, followed
              by staged reduction and some remaining simplifications. Ornaments
              are a way to describe changes in datatype definitions that
              preserve their recursive structure, reorganizing, adding, or
              dropping some pieces of data so that functions operating on the
              bare definition can be partially and sometimes totally lifted into
              functions operating on the ornamented structure. We propose an
              extension of ML with higherorder ornaments. We introduce a
              meta-language above ML in which we can elaborate a most generic
              lifting of bare code, so that ornamented code can then be obtained
              by instantiation of the generic lifting, followed by staged
              reduction and some remaining simplifications. We use logical
              relations to closely relate the ornamented code to the bare code.",
  year     =  2017,
  url      = "https://www.semanticscholar.org/paper/16d8594dd9e24bf67c5200b02c9661435465617f",
  language = "en"
}

@INPROCEEDINGS{Ringer2019-zq,
  title     = "Ornaments for Proof Reuse in Coq",
  author    = "Ringer, Talia and Yazdani, Nathaniel and Leo, John and Grossman,
               Dan",
  booktitle = "10th International Conference on Interactive Theorem Proving (ITP
               2019)",
  publisher = "Schloss Dagstuhl - Leibniz-Zentrum fuer Informatik",
  pages     =  19,
  year      =  2019,
  url       = "https://drops.dagstuhl.de/opus/volltexte/2019/11081/pdf/LIPIcs-ITP-2019-26.pdf"
}

@INPROCEEDINGS{Lumsdaine2009-ls,
  title     = "Weak ω-Categories from Intensional Type Theory",
  author    = "Lumsdaine, Peter Lefanu",
  booktitle = "Typed Lambda Calculi and Applications",
  publisher = "Springer Berlin Heidelberg",
  pages     = "172--187",
  abstract  = "Higher-dimensional categories have recently emerged as a natural
               context for modelling intensional type theories; this raises the
               question of what higher-categorical structures the syntax of type
               theory naturally forms. We show that for any type in Martin-Löf
               Intensional Type Theory, the system of terms of that type and its
               higher identity types forms a weak ω-category in the sense of
               Leinster. Precisely, we construct a contractible globular operad
               ${P_{\mathit{ML}^{\mathrm{Id}}}}$of type-theoretically definable
               composition laws, and give an action of this operad on any type
               and its identity types.",
  year      =  2009,
  url       = "http://dx.doi.org/10.1007/978-3-642-02273-9_14"
}

@ARTICLE{Wu2017-cf,
  title         = "Dependent Session Types",
  author        = "Wu, Hanwen and Xi, Hongwei",
  journal       = "arXiv [cs.PL]",
  abstract      = "Session types offer a type-based discipline for enforcing
                   communication protocols in distributed programming. We have
                   previously formalized simple session types in the setting of
                   multi-threaded $\lambda$-calculus with linear types. In this
                   work, we build upon our earlier work by presenting a form of
                   dependent session types (of DML-style). The type system we
                   formulate provides linearity and duality guarantees with no
                   need for any runtime checks or special encodings. Our
                   formulation of dependent session types is the first of its
                   kind, and it is particularly suitable for practical
                   implementation. As an example, we describe one implementation
                   written in ATS that compiles to an Erlang/Elixir back-end.",
  month         =  apr,
  year          =  2017,
  url           = "http://arxiv.org/abs/1704.07004",
  archivePrefix = "arXiv",
  primaryClass  = "cs.PL"
}

@ARTICLE{Barwell2022-ed,
  title         = "Generalised Multiparty Session Types with Crash-Stop Failures
                   (Technical Report)",
  author        = "Barwell, Adam D and Scalas, Alceste and Yoshida, Nobuko and
                   Zhou, Fangyi",
  journal       = "arXiv [cs.PL]",
  abstract      = "Session types enable the specification and verification of
                   communicating systems. However, their theory often assumes
                   that processes never fail. To address this limitation, we
                   present a generalised multiparty session type (MPST) theory
                   with crash-stop failures, where processes can crash
                   arbitrarily. Our new theory validates more protocols and
                   processes w.r.t. previous work. We apply minimal syntactic
                   changes to standard session {\pi}-calculus and types: we
                   model crashes and their handling semantically, with a
                   generalised MPST typing system parametric on a behavioural
                   safety property. We cover the spectrum between fully reliable
                   and fully unreliable sessions, via optional reliability
                   assumptions, and prove type safety and protocol conformance
                   in the presence of crash-stop failures. Introducing
                   crash-stop failures has non-trivial consequences: writing
                   correct processes that handle all crash scenarios can be
                   difficult. Yet, our generalised MPST theory allows us to tame
                   this complexity, via model checking, to validate whether a
                   multiparty session satisfies desired behavioural properties,
                   e.g. deadlock-freedom or liveness, even in presence of
                   crashes. We implement our approach using the mCRL2 model
                   checker, and evaluate it with examples extended from the
                   literature.",
  month         =  jul,
  year          =  2022,
  url           = "http://arxiv.org/abs/2207.02015",
  archivePrefix = "arXiv",
  primaryClass  = "cs.PL"
}

@INPROCEEDINGS{Yoshida2020-pz,
  title     = "A Very Gentle Introduction to Multiparty Session Types",
  author    = "Yoshida, Nobuko and Gheri, Lorenzo",
  booktitle = "Distributed Computing and Internet Technology: 16th International
               Conference, ICDCIT 2020, Bhubaneswar, India, January 9–12, 2020,
               Proceedings",
  publisher = "Springer-Verlag",
  address   = "Berlin, Heidelberg",
  pages     = "73--93",
  abstract  = "Multiparty session types (MPST) are a formal specification and
               verification framework for message-passing protocols without
               central control: the desired interactions at the scale of the
               network itself are specified into a session (called global type).
               Global types are then projected onto local types (one for each
               participant), which describe the protocol from a local point of
               view. These local types are used to validate an application
               through type-checking, monitoring, and code generation. Theory of
               session types guarantees that local conformance of all
               participants induces global conformance of the network to the
               initial global type. This paper provides a very gentle
               introduction of the simplest version of multiparty session types
               for readers who are not familiar with session types nor process
               calculi.",
  month     =  jan,
  year      =  2020,
  url       = "https://doi.org/10.1007/978-3-030-36987-3_5",
  keywords  = "Distributed systems, Multiparty session types, Process calculi,
               Progress, Type safety"
}

@ARTICLE{Mellies2015-ky,
  title     = "Functors are Type Refinement Systems",
  author    = "Melliès, Paul-André and Zeilberger, Noam",
  journal   = "SIGPLAN Not.",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  volume    =  50,
  number    =  1,
  pages     = "3--16",
  abstract  = "The standard reading of type theory through the lens of category
               theory is based on the idea of viewing a type system as a
               category of well-typed terms. We propose a basic revision of this
               reading: rather than interpreting type systems as categories, we
               describe them as functors from a category of typing derivations
               to a category of underlying terms. Then, turning this around, we
               explain how in fact any functor gives rise to a generalized type
               system, with an abstract notion of typing judgment, typing
               derivations and typing rules. This leads to a purely categorical
               reformulation of various natural classes of type systems as
               natural classes of functors.The main purpose of this paper is to
               describe the general framework (which can also be seen as
               providing a categorical analysis of refinement types), and to
               present a few applications. As a larger case study, we revisit
               Reynolds' paper on ``The Meaning of Types'' (2000), showing how
               the paper's main results may be reconstructed along these lines.",
  month     =  jan,
  year      =  2015,
  url       = "https://doi.org/10.1145/2775051.2676970",
  keywords  = "category theory, type theory, refinement types"
}

@ARTICLE{noauthor_undated-vt,
  title = "The n-Category Café: A 2-Categorical Approach to the Pi Calculus",
  url   = "https://golem.ph.utexas.edu/category/2015/05/a_2categorical_approach_to_the.html"
}

@MISC{noauthor_undated-ge,
  title        = "Freyd category in {nLab}",
  howpublished = "\url{https://ncatlab.org/nlab/show/Freyd+category}",
  note         = "Accessed: 2023-10-9"
}

@ARTICLE{Stay2015-rd,
  title         = "Higher category models of the pi-calculus",
  author        = "Stay, Mike and Meredith, Lucius Gregory",
  journal       = "arXiv [cs.LO]",
  abstract      = "We present an approach to modeling computational calculi
                   using higher category theory. Specifically we present a fully
                   abstract semantics for the pi-calculus. The interpretation is
                   consistent with Curry-Howard, interpreting terms as typed
                   morphisms, while simultaneously providing an explicit
                   interpretation of the rewrite rules of standard operational
                   presentations as 2-morphisms. One of the key contributions,
                   inspired by catalysis in chemical reactions, is a method of
                   restricting the application of 2-morphisms interpreting
                   rewrites to specific contexts.",
  month         =  apr,
  year          =  2015,
  url           = "http://arxiv.org/abs/1504.04311",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LO"
}

@INPROCEEDINGS{Sakayori2019-eo,
  title     = "A Categorical Model of an i/o-typed \pi -calculus",
  author    = "Sakayori, Ken and Tsukada, Takeshi",
  booktitle = "Programming Languages and Systems",
  publisher = "Springer International Publishing",
  pages     = "640--667",
  abstract  = "This paper introduces a new categorical structure that is a model
               of a variant of the $$ \mathbf {i/o} $$-typed $$ \pi $$-calculus,
               in the same way that a cartesian closed category is a model of
               the $$ \lambda $$-calculus. To the best of our knowledge, no
               categorical model has been given for the $$ \mathbf {i/o}
               $$-typed $$ \pi $$-calculus, in contrast to session-typed
               calculi, to which corresponding logic and categorical structure
               were given. The categorical structure introduced in this paper
               has a simple definition, combining two well-known structures,
               namely, closed Freyd category and compact closed category. The
               former is a model of effectful computation in a general setting,
               and the latter describes connections via channels, which cause
               the effect we focus on in this paper. To demonstrate the
               relevance of the categorical model, we show by a semantic
               consideration that the $$ \pi $$-calculus is equivalent to a core
               calculus of Concurrent ML.",
  year      =  2019,
  url       = "http://dx.doi.org/10.1007/978-3-030-17184-1_23"
}

@ARTICLE{Hirschkoff2003-cb,
  title    = "A brief survey of the theory of the Pi-calculus",
  author   = "Hirschkoff, Daniel",
  pages    = "2+15p",
  year     =  2003,
  url      = "https://hal-lara.archives-ouvertes.fr/hal-02101985",
  keywords = "Behavioural Equivalence;Lambda-calculus;Mobility;Name
              Passing;Pi-calculus;Process Algebra;Type System; Algèbre de
              processus;mobilité;passage de noms;π-calcul;λ-calcul;système de
              type;équivalence comportementale",
  language = "en"
}

@ARTICLE{Dardha2017-nu,
  title    = "Session types revisited",
  author   = "Dardha, Ornela and Giachino, Elena and Sangiorgi, Davide",
  journal  = "Inform. and Comput.",
  volume   =  256,
  pages    = "253--286",
  abstract = "Session types are a formalism used to model structured
              communication-based programming. A binary session type describes
              communication by specifying the type and direction of data
              exchanged between two parties. When session types and session
              processes are added to the syntax of standard π-calculus they give
              rise to additional separate syntactic categories. As a
              consequence, when new type features are added, there is
              duplication of effort in the theory: the proofs of properties must
              be checked both on standard types and on session types. We show
              that session types are encodable into standard π-types, relying on
              linear and variant types. Besides being an expressivity result,
              the encoding (i) removes the above redundancies in the syntax, and
              (ii) the properties of session types are derived as
              straightforward corollaries, exploiting the corresponding
              properties of standard π-types. The robustness of the encoding is
              tested on a few extensions of session types, including subtyping,
              polymorphism and higher-order communications.",
  month    =  oct,
  year     =  2017,
  url      = "https://www.sciencedirect.com/science/article/pii/S0890540117300962",
  keywords = "Session types; -Calculus; Linear types; Variant types; Encoding"
}

@ARTICLE{Dagand2018-lu,
  title     = "Foundations of dependent interoperability",
  author    = "Dagand, Pierre-Évariste and Tabareau, Nicolas and Tanter, Éric",
  journal   = "J. Funct. Programming",
  publisher = "Cambridge University Press (CUP)",
  volume    =  28,
  number    = "e9",
  abstract  = "AbstractFull-spectrum dependent types promise to enable the
               development of correct-by-construction software. However, even
               certified software needs to interact with simply-typed or untyped
               programs, be it to perform system calls, or to use legacy
               libraries. Trading static guarantees for runtime checks,
               thedependent interoperabilityframework provides a mechanism by
               which simply-typed values can safely be coerced to dependent
               types and, conversely, dependently-typed programs can defensively
               be exported to a simply-typed application. In this article, we
               give a semantic account of dependent interoperability. Our
               presentation relies on and is guided by a pervading notion of
               type equivalence, whose importance has been emphasized in recent
               work on homotopy type theory. Specifically, we develop the
               notions oftype-theoretic partial Galois connectionsas a key
               foundation for dependent interoperability, which accounts for the
               partiality of the coercions between types. We explore the
               applicability of both type-theoretic Galois connections and
               anticonnections in the setting of dependent interoperability. A
               partial Galois connection enforces a translation of dependent
               types to runtime checks that are both sound and complete with
               respect to the invariants encoded by dependent types. Conversely,
               picking an anticonnection instead lets us induce weaker, sound
               conditions that can amount to more efficient runtime checks.Our
               framework is developed in Coq; it is thus constructive and
               verified in the strictest sense of the terms. Using our library,
               users can specify domain-specific partial connections between
               data structures. Our library then takes care of the (sometimes,
               heavy) lifting that leads to interoperable programs. It thus
               becomes possible, as we shall illustrate, to internalize and
               hand-tune the extraction of dependently-typed programs to
               interoperable OCaml programs within Coq itself.",
  month     =  mar,
  year      =  2018,
  url       = "https://inria.hal.science/hal-01629909",
  language  = "en"
}

@ARTICLE{Dybjer2000-ex,
  title     = "A general formulation of simultaneous inductive-recursive
               definitions in type theory",
  author    = "Dybjer, Peter",
  journal   = "J. Symbolic Logic",
  publisher = "Cambridge University Press",
  volume    =  65,
  number    =  2,
  pages     = "525--549",
  abstract  = "A general formulation of simultaneous inductive-recursive
               definitions in type theory - Volume 65 Issue 2",
  month     =  jun,
  year      =  2000,
  url       = "https://www.cambridge.org/core/services/aop-cambridge-core/content/view/22936CE8026FB72BD3D722EE2BF58809/S0022481200012044a.pdf/div-class-title-a-general-formulation-of-simultaneous-inductive-recursive-definitions-in-type-theory-div.pdf"
}

@ARTICLE{Palmgren2019-oc,
  title    = "Categories with families and first-order logic with dependent
              sorts",
  author   = "Palmgren, Erik",
  journal  = "Ann. Pure Appl. Logic",
  volume   =  170,
  number   =  12,
  pages    =  102715,
  abstract = "First-order logic with dependent sorts, such as Makkai's
              first-order logic with dependent sorts (FOLDS), or Aczel's and
              Belo's dependently typed (intuitionistic) first-order logic
              (DFOL), may be regarded as logic enriched dependent type theories.
              Categories with families (cwfs) is an established semantical
              structure for dependent type theories, such as Martin-Löf type
              theory. We introduce in this article a notion of hyperdoctrine
              over a cwf, and show how FOLDS and DFOL fit in this semantical
              framework. A soundness and completeness theorem is proved for
              DFOL. The semantics is functorial in the sense of Lawvere, and
              uses a dependent version of the Lindenbaum-Tarski algebra for a
              DFOL theory. Agreement with standard first-order semantics is
              established. Applications of DFOL to constructive mathematics and
              categorical foundations are given. A key feature is a local
              propositions-as-types principle.",
  month    =  dec,
  year     =  2019,
  url      = "https://www.sciencedirect.com/science/article/pii/S0168007219300727",
  keywords = "Intuitionistic first-order logic; Dependent types; Categorical
              logic; Models of type theory"
}

@ARTICLE{Pujet2022-ep,
  title     = "Observational Equality: Now for Good; Observational Equality: Now
               for Good",
  author    = "Pujet, Loïc and Tabareau, Nicolas",
  journal   = "Proceedings of the ACM on Programming Languages",
  publisher = "ACM",
  volume    =  2022,
  pages     = "1--29",
  abstract  = "Building on the recent extension of dependent type theory with a
               universe of definitionally proof-irrelevant types, we introduce
               TT obs , a new type theory based on the setoidal interpretation
               of dependent type theory. TT obs equips every type with an
               identity relation that satisfies function extensionality,
               propositional exten-sionality, and definitional uniqueness of
               identity proofs (UIP). Compared to other existing proposals to
               enrich dependent type theory with these principles, our theory
               features a notion of reduction that is normalizing and provides
               an algorithmic canonicity result, which we formally prove in Agda
               using the logical relation framework of Abel et al. Our paper
               thoroughly develops the meta-theoretical properties of TT obs ,
               such as the decidability of the conversion and of the type
               checking, as well as consistency. We also explain how to extend
               our theory with quotient types, and we introduce a setoidal
               version of Swan's Id types that turn it into a proper extension
               of MLTT with inductive equality.",
  year      =  2022,
  url       = "https://doi.org/10.1145/3498693",
  keywords  = "confluence; dependent types; rewriting theory; termination; type
               theory"
}

@ARTICLE{Altenkirch_undated-xu,
  title    = "Observational Equality, Now!",
  author   = "Altenkirch, Thorsten and Mcbride, Conor and Swierstra, Wouter",
  abstract = "This paper has something new and positive to say about
              propo-sitional equality in programming and proof systems based on
              the Curry-Howard correspondence between propositions and types. We
              have found a way to present a propositional equality type • which
              is substitutive, allowing us to reason by replacing equal for
              equal in propositions; • which reflects the observable behaviour
              of values rather than their construction: in particular, we have
              extensionality-functions are equal if they take equal inputs to
              equal outputs; • which retains strong normalisation, decidable
              typechecking and canonicity-the property that closed normal forms
              inhabiting datatypes have canonical constructors; • which allows
              inductive data structures to be expressed in terms of a standard
              characterisation of well-founded trees; • which is presented
              syntactically-you can implement it directly , and we are doing
              so-this approach stands at the core of Epigram 2; • which you can
              play with now: we have simulated our system by a shallow embedding
              in Agda 2, shipping as part of the standard examples package for
              that system [21]. Until now, it has always been necessary to
              sacrifice some of these aspects. The closest attempt in the
              literature is Al-tenkirch's construction of a setoid-model for a
              system with canon-icity and extensionality on top of an
              intensional type theory with proof-irrelevant propositions [4].
              Our new proposal simplifies Al-tenkirch's construction by adopting
              McBride's heterogeneous approach to equality [19].",
  keywords = "Equality; F41 [Mathematical Logic]: Lambda calculus and related
              systems; D31 [Formal Definitions and Theory]: Semantics General
              Terms Languages; Theory Keywords Type Theory"
}

@ARTICLE{Campos2015-gl,
  title     = "Imperative objects with dependent types",
  author    = "Campos, Joana and Vasconcelos, Vasco T",
  journal   = "Proceedings for the 17th Workshop on Formal Techniques for
               Java-like Programs, FTfJP 2015: co-located with ECOOP 2015",
  publisher = "Association for Computing Machinery, Inc",
  abstract  = "Index refinements (or dependent types over a restricted domain)
               enable the expression of many desirable invariants that can be
               verified at compile time. We propose to incorporate a system of
               index refinements in a small, class-based, imperative,
               object-oriented language. While rooted in techniques formulated
               for dependently-typed functional languages, our type system is
               able to capture more than just value properties and pure
               computations. Index refinements, combined with a notion of pre-
               and post-type to track state, give programmers the ability to
               reason about effectful computations. Our type system
               distinguishes between two classes of objects, imposing an affine
               discipline to objects whose types are governed by indices, as
               opposed to conventional objects which can be freely shared. We
               have designed and implemented an expressive and decidable type
               system, which we illustrate through a number of examples.",
  month     =  jul,
  year      =  2015,
  url       = "https://dl.acm.org/doi/10.1145/2786536.2786538",
  keywords  = "Dependent types; Index refinements; Mutable objects"
}

@ARTICLE{Xi2000-bq,
  title     = "Imperative programming with dependent types",
  author    = "Xi, Hongwei",
  journal   = "Proceedings - Symposium on Logic in Computer Science",
  publisher = "IEEE",
  pages     = "375--387",
  abstract  = "In this paper, we enrich imperative programming with a form of
               dependent types. We start with explaining some motivations for
               this enrichment and mentioning some major obstacles that need to
               be overcome. We then present the design of a source level
               dependently typed imperative programming language Xanadu, forming
               both static and dynamic semantics and then establishing the type
               soundness theorem. We also present realistic examples, which have
               all been verified in a prototype implementation, in support of
               the practicality of Xanadu. We claim that the language design of
               Xanadu is novel and it serves as an informative example that
               demonstrates a means to combine imperative programming with
               dependent types.",
  year      =  2000,
  url       = "http://dx.doi.org/10.1109/LICS.2000.855785"
}

@ARTICLE{Brady2010-pd,
  title     = "Correct-by-Construction Concurrency: Using Dependent Types to
               Verify Implementations of Effectful Resource Usage Protocols",
  author    = "Brady, Edwin and Hammond, Kevin",
  journal   = "Fund. Inform.",
  publisher = "IOS Press PUB827 Amsterdam, The Netherlands, The Netherlands",
  abstract  = "In the modern, multi-threaded, multi-core programming
               environment, correctly managing system resources, including locks
               and shared variables, can be especially difficult and errorprone.
               A simple mi...",
  month     =  apr,
  year      =  2010,
  url       = "https://dl.acm.org/doi/10.5555/1883634.1883636"
}

@ARTICLE{Jhala2020-kp,
  title     = "Refinement Types: A Tutorial",
  author    = "Jhala, Ranjit and Vazou, Niki",
  journal   = "Foundations and Trends in Programming Languages",
  publisher = "Now Publishers Inc",
  volume    =  6,
  number    = "3-4",
  pages     = "159--317",
  abstract  = "Refinement types enrich a language's type system with logical
               predicates that circumscribe the set of values described by the
               type, thereby providing software developers a tunable knob with
               which to inform the type system about what invariants and
               correctness properties should be checked on their code. In this
               article, we distill the ideas developed in the substantial
               literature on refinement types into a unified tutorial that
               explains the key ingredients of modern refinement type systems.
               In particular, we show how to implement a refinement type checker
               via a progression of languages that incrementally add features to
               the language or type system.",
  month     =  oct,
  year      =  2020,
  url       = "http://dx.doi.org/10.48550/arxiv.2010.07763"
}

@ARTICLE{Brady2013-ff,
  title   = "Idris, a general-purpose dependently typed programming language:
             Design and implementation",
  author  = "Brady, Edwin C",
  journal = "J. Funct. Programming",
  volume  =  23,
  pages   = "552--593",
  year    =  2013
}

@MISC{Angiuli_undated-zt,
  title  = "The red* family of proof assistants",
  author = "Angiuli, Carlo and Cavallo, Evan and Hou, Kuen-Bang and {Reed
            Mullanix} and Sterling, Jon",
  url    = "http://www.redpl.com/"
}

@ARTICLE{Cohen2016-hq,
  title   = "Cubical Type Theory: a constructive interpretation of the
             univalence axiom",
  author  = "Cohen, Cyril and Coquand, Thierry and Huber, Simon and Mörtberg,
             Anders",
  journal = "CoRR",
  volume  = "abs/1611.02108",
  year    =  2016,
  url     = "http://arxiv.org/abs/1611.02108"
}

@MISC{Microsoft_Research2020-lf,
  title        = "Proof-oriented Programming in F*",
  author       = "{Microsoft Research}",
  year         =  2020,
  howpublished = "\url{https://www.fstar-lang.org/tutorial/book/index.html\#}"
}

@ARTICLE{Lubin2021-ps,
  title     = "How statically-typed functional programmers write code",
  author    = "Lubin, Justin and Chasins, Sarah E",
  journal   = "Proceedings of the ACM on Programming Languages",
  publisher = "ACM PUB27 New York, NY, USA",
  volume    =  5,
  number    = "OOPSLA",
  pages     =  30,
  abstract  = "How working statically-typed functional programmers write code is
               largely understudied. And yet, a better understanding of
               developer practices could pave the way for the design of more
               useful and u...",
  month     =  oct,
  year      =  2021,
  url       = "https://dl.acm.org/doi/10.1145/3485532",
  keywords  = "functional programming; grounded theory; interviews; mixed
               methods; need-finding; qualitative; quantitative; randomized
               controlled trial; static types"
}

@ARTICLE{Nanevski2007-ny,
  title     = "Abstract Predicates and Mutable {ADTs} in Hoare Type Theory",
  author    = "Nanevski, Aleksandar and Ahmed, Amal and Morrisett, Greg and
               Birkedal, Lars",
  journal   = "Lect. Notes Comput. Sci.",
  publisher = "Springer, Berlin, Heidelberg",
  volume    = "4421 LNCS",
  pages     = "189--204",
  abstract  = "Hoare Type Theory (HTT) combines a dependently typed,
               higher-order language with monadically-encapsulated, stateful
               computations. The type system incorporates pre- and
               post-conditions, in a fashion similar to Hoare and Separation
               Logic, so that...",
  year      =  2007,
  url       = "https://link.springer.com/chapter/10.1007/978-3-540-71316-6_14"
}

@ARTICLE{Shen2021-ah,
  title    = "Toward {SMT}-Based Refinement Types in Agda",
  author   = "Shen, Gan and Kuper, Lindsey",
  abstract = "Dependent types offer great versatility and power, but developing
              proofs with them can be tedious and requires considerable human
              guidance. We propose to integrate Satisfiability Modulo Theories
              (SMT)-based refinement types into the dependently-typed language
              Agda in an effort to ease some of the burden of programming with
              dependent types and combine the strengths of the two approaches to
              mechanized theorem proving.",
  month    =  oct,
  year     =  2021,
  url      = "http://dx.doi.org/10.48550/arxiv.2110.05771"
}

@ARTICLE{Pena2017-kz,
  title     = "An Introduction to Liquid Haskell",
  author    = "Peña, Ricardo",
  journal   = "Electronic Proceedings in Theoretical Computer Science, EPTCS",
  publisher = "Open Publishing Association",
  volume    =  237,
  pages     = "68--80",
  abstract  = "This paper is a tutorial introducing the underlying technology
               and the use of the tool Liquid Haskell, a type-checker for the
               functional language Haskell that can help programmers to verify
               non-trivial properties of their programs with a low effort. The
               first sections introduce the technology of Liquid Types by
               explaining its principles and summarizing how its type inference
               algorithm manages to prove properties. The remaining sections
               present a selection of Haskell examples and show the kind of
               properties that can be proved with the system.",
  month     =  jan,
  year      =  2017,
  url       = "http://dx.doi.org/10.4204/EPTCS.237.5"
}

@ARTICLE{Jung2018-uj,
  title     = "Iris from the ground up: A modular foundation for higher-order
               concurrent separation logic",
  author    = "Jung, Ralf and Krebbers, Robbert and Jourdan, Jacques Henri and
               Bizjak, Aleš and Birkedal, Lars and Dreyer, Derek",
  journal   = "J. Funct. Programming",
  publisher = "Cambridge University Press",
  volume    =  28,
  pages     = "e20",
  abstract  = "Iris is a framework for higher-order concurrent separation logic,
               which has been implemented in the Coq proof assistant and
               deployed very effectively in a wide variety of verification
               projects. Iris was designed with the express goal of simplifying
               and consolidating the foundations of modern separation logics,
               but it has evolved over time, and the design and semantic
               foundations of Iris itself have yet to be fully written down and
               explained together properly in one place. Here, we attempt to
               fill this gap, presenting a reasonably complete picture of the
               latest version of Iris (version 3.1), from first principles and
               in one coherent narrative.",
  year      =  2018,
  url       = "https://www.cambridge.org/core/journals/journal-of-functional-programming/article/iris-from-the-ground-up-a-modular-foundation-for-higherorder-concurrent-separation-logic/26301B518CE2C52796BFA12B8BAB5B5F"
}

@ARTICLE{Petersen2008-ga,
  title     = "A realizability model for impredicative Hoare Type Theory",
  author    = "Petersen, Rasmus Lerchedahl and Birkedal, Lars and Nanevski,
               Aleksandar and Morrisett, Greg",
  journal   = "Lect. Notes Comput. Sci.",
  publisher = "Springer, Berlin, Heidelberg",
  volume    = "4960 LNCS",
  pages     = "337--352",
  abstract  = "We present a denotational model of impredicative Hoare Type
               Theory, a very expressive dependent type theory in which one can
               specify and reason about mutable abstract data types. The model
               ensures soundness of the extension of Hoare Type Theory with
               impredicative polymorphism; makes the connections to separation
               logic clear, and provides a basis for investigation of further
               sound extensions of the theory, in particular equations between
               computations and types. © 2008 Springer-Verlag Berlin Heidelberg.",
  year      =  2008,
  url       = "https://link.springer.com/chapter/10.1007/978-3-540-78739-6_26"
}

@ARTICLE{Brady2021-fh,
  title     = "Idris 2: Quantitative Type Theory in Practice",
  author    = "Brady, Edwin",
  journal   = "Leibniz International Proceedings in Informatics, LIPIcs",
  publisher = "Schloss Dagstuhl- Leibniz-Zentrum fur Informatik GmbH, Dagstuhl
               Publishing",
  volume    =  194,
  abstract  = "Dependent types allow us to express precisely what a function is
               intended to do. Recent work on Quantitative Type Theory (QTT)
               extends dependent type systems with linearity, also allowing
               precision in expressing when a function can run. This is
               promising, because it suggests the ability to design and reason
               about resource usage protocols, such as we might find in
               distributed and concurrent programming, where the state of a
               communication channel changes throughout program execution. As
               yet, however, there has not been a full-scale programming
               language with which to experiment with these ideas. Idris 2 is a
               new version of the dependently typed language Idris, with a new
               core language based on QTT, supporting linear and dependent
               types. In this paper, we introduce Idris 2, and describe how QTT
               has influenced its design. We give examples of the benefits of
               QTT in practice including: expressing which data is erased at run
               time, at the type level; and, resource tracking in the type
               system leading to type-safe concurrent programming with session
               types.",
  month     =  apr,
  year      =  2021,
  url       = "http://dx.doi.org/10.48550/arxiv.2104.00480",
  keywords  = "Concurrency; Dependent types; Linear types"
}

@ARTICLE{Brown2022-bd,
  title    = "Towards a refactoring tool for dependently-typed programs
              (Extended abstract)",
  author   = "Brown, Christopher Mark and Barwell, Adam and Thompson, Simon and
              Sarkar, Susmit and Brady, Edwin Charles",
  abstract = "Funding: This work was generously supported by UK EPSRC Energise,
              grant number EP/V006290/1 (CMB).",
  month    =  jul,
  year     =  2022,
  url      = "https://research-repository.st-andrews.ac.uk/handle/10023/25612",
  keywords = "Dependent; QA75; QA75 Electronic computers. Computer science;
              Refactoring; types"
}

@ARTICLE{Barrett2018-xv,
  title     = "Satisfiability modulo theories",
  author    = "Barrett, Clark and Tinelli, Cesare",
  journal   = "Handbook of Model Checking",
  publisher = "Springer International Publishing",
  pages     = "305--343",
  abstract  = "Satisfiability Modulo Theories (SMT) refers to the problem of
               determining whether a first-order formula is satisfiable with
               respect to some logical theory. Solvers based on SMT are used as
               back-end engines in model-checking applications such as bounded,
               interpolation-based, and predicate-abstraction-based model
               checking. After a brief illustration of these uses, we survey the
               predominant techniques for solving SMT problems with an emphasis
               on the lazy approach, in which a propositional satisfiability
               (SAT) solver is combined with one or more theory solvers. We
               discuss the architecture of a lazy SMT solver, give examples of
               theory solvers, show how to combine such solvers modularly, and
               mention several extensions of the lazy approach.We also briefly
               describe the eager approach in which the SMT problem is reduced
               to a SAT problem. Finally, we discuss how the basic framework for
               determining satisfiability can be extended with additional
               functionality such as producing models, proofs, unsatisfiable
               cores, and interpolants.",
  month     =  may,
  year      =  2018,
  url       = "https://link.springer.com/chapter/10.1007/978-3-319-10575-8_11"
}

@ARTICLE{Benton2002-bi,
  title     = "Monads and effects",
  author    = "Benton, Nick and Hughes, John and Moggi, Eugenio",
  journal   = "Lect. Notes Comput. Sci.",
  publisher = "Springer Verlag",
  volume    = "2395 LNCS",
  pages     = "42--122",
  abstract  = "A tension in language design has been between simple semantics on
               the one hand, and rich possibilities for side-effects, exception
               handling and so on on the other. The introduction of monads has
               made a large step towards reconciling these alternatives. First
               proposed by Moggi as a way of structuring semantic descriptions,
               they were adopted by Wadler to structure Haskell programs. Monads
               have been used to solve long-standing problems such as adding
               pointers and assignment, inter-language working, and exception
               handling to Haskell, without compromising its purely functional
               semantics. The course introduces monads, effects, and exemplifies
               their applications in programming (Haskell) and in compilation
               (MLj). The course presents typed metalanguages for monads and
               related categorical notions, and then describes how they can be
               further refined by introducing effects. © Springer-Verlag Berlin
               Heidelberg 2002.",
  year      =  2002,
  url       = "https://www.researchgate.net/publication/2806790_Monads_and_Effects"
}

@BOOK{Hutton2016-ko,
  title     = "Programming in Haskell",
  author    = "Hutton, Graham",
  publisher = "Cambridge University Press",
  edition   =  2,
  pages     =  304,
  abstract  = "Second edition. ``First published 2007.'' ``Haskell is a purely
               functional language that allows programmers to rapidly develop
               clear, concise, and correct software. The language has grown in
               popularity in recent years, both in teaching and in industry.
               This book is based on the author's experience of teaching Haskell
               for more than twenty years. All concepts are explained from first
               principles and no programming experience is required, making this
               book accessible to a broad spectrum of readers. While Part I
               focuses on basic concepts, Part II introduces the reader to more
               advanced topics. This new edition has been extensively updated
               and expanded to include recent and more advanced features of
               Haskell, new examples and exercises, selected solutions, and
               freely downloadable lecture slides and example code. The
               presentation is clean and simple, while also being fully
               compliant with the latest version of the language, including
               recent changes concerning applicative, monadic, foldable, and
               traversable types''-- Introduction -- First steps -- Types and
               classes -- Defining functions -- List comprehensions -- Recursive
               functions -- Higher-order functions -- Declaring types and
               classes -- The countdown problem -- Interactive programming --
               Unbeatable tic-tac-toe -- Monads and more -- Monadic parsing --
               Foldables and friends -- Lazy evaluation -- Reasoning about
               programs -- Calculating compilers.",
  year      =  2016
}

@MISC{noauthor_undated-ya,
  title        = "The Agda Wiki",
  howpublished = "\url{https://wiki.portal.chalmers.se/agda/pmwiki.php}"
}

@MISC{noauthor_undated-yj,
  title        = "Stack Overflow Developer Survey 2022",
  howpublished = "\url{https://survey.stackoverflow.co/2022/\#technology}"
}

@ARTICLE{Rustan2013-jc,
  title    = "Co-induction Simply: Automatic Co-inductive Proofs in a Program
              Verifier",
  author   = "Rustan, K and Leino, M and Moskal, Michał",
  abstract = "Program verification relies heavily on induction, which has
              received decades of attention in mechanical verification tools.
              When program correctness is best described by infinite structures,
              program verification is usefully aided also by co-induction, which
              has not benefited from the same degree of tool support.
              Co-induction is complicated to work with in interactive proof
              assistants and has had no previous support in dedicated program
              verifiers. This paper shows that an SMT-based program verifier can
              support reasoning about co-induction-handling infinite data
              structures , lazy function calls, and user-defined properties
              defined as greatest fix-points, as well as letting users write
              co-inductive proofs. Moreover, the support can be packaged to
              provide a simple user experience. The paper describes the features
              for co-induction in the language and verifier Dafny, defines their
              translation into input for a first-order SMT solver, and reports
              on some encouraging initial experience.",
  year     =  2013,
  url      = "http://rise4fun.com/Dafny/id",
  keywords = "Co-induction Keywords verification; D24 [SOFTWARE ENGI-NEERING]:
              Software/Program Verification General Terms Verification; SMT;
              co-induction; lazy data structures"
}

@ARTICLE{Gleirscher2020-xi,
  title     = "Formal methods in dependable systems engineering: a survey of
               professionals from Europe and North America",
  author    = "Gleirscher, Mario and Marmsoler, Diego",
  journal   = "Empirical Software Engineering",
  publisher = "Springer",
  volume    =  25,
  number    =  6,
  pages     = "4473--4546",
  abstract  = "Context: Formal methods (FMs) have been around for a while, still
               being unclear how to leverage their benefits, overcome their
               challenges, and set new directions for their improvement towards
               a more successful transfer into practice. Objective: We study the
               use of formal methods in mission-critical software domains,
               examining industrial and academic views. Method: We perform a
               cross-sectional on-line survey. Results: Our results indicate an
               increased intent to apply FMs in industry, suggesting a
               positively perceived usefulness. But the results also indicate a
               negatively perceived ease of use. Scalability, skills, and
               education seem to be among the key challenges to support this
               intent. Conclusions: We present the largest study of this kind so
               far (N = 216), and our observations provide valuable insights,
               highlighting directions for future theoretical and empirical
               research of formal methods. Our findings are strongly coherent
               with earlier observations by Austin and Graeme (1993).",
  month     =  nov,
  year      =  2020,
  url       = "https://link.springer.com/article/10.1007/s10664-020-09836-5",
  keywords  = "Empirical research; Formal methods; On-line survey; Practical
               challenges; Research transfer; Software engineering education \&
               training; Usage; Usefulness"
}

@ARTICLE{Garavel2020-uj,
  title     = "The 2020 Expert Survey on Formal Methods",
  author    = "Garavel, Hubert and Ter Beek, Maurice H and Van De Pol, Jaco",
  journal   = "Lect. Notes Comput. Sci.",
  publisher = "Springer Science and Business Media Deutschland GmbH",
  volume    = "12327 LNCS",
  pages     = "3--69",
  abstract  = "Organised to celebrate the 25th anniversary of the FMICS
               international conference, the present survey addresses 30
               questions on the past, present, and future of formal methods in
               research, industry, and education. Not less than 130 high-profile
               experts in formal methods (among whom three Turing award winners
               and many recipients of other prizes and distinctions) accepted to
               participate in this survey. We analyse their answers and
               comments, and present a collection of 111 position statements
               provided by these experts. The survey is both an exercise in
               collective thinking and a family picture of key actors in formal
               methods.",
  year      =  2020,
  url       = "https://link.springer.com/chapter/10.1007/978-3-030-58298-2_1",
  keywords  = "Cybersecurity; Education; Formal method; Modelling; Safety;
               Software engineering; Software tool; Specification; Survey;
               Technology transfer; Verification"
}

@ARTICLE{Oliveira2015-lh,
  title     = "Use case analysis based on formal methods: An empirical study",
  author    = "Oliveira, Marcos and Ribeiro, Leila and Cota, Érika and Duarte,
               Lucio Mauro and Nunes, Ingrid and Reis, Filipe",
  journal   = "Lect. Notes Comput. Sci.",
  publisher = "Springer Verlag",
  volume    =  9463,
  pages     = "110--130",
  abstract  = "Use Cases (UC) are a popular way of describing system behavior
               and represent important artifacts for system design, analysis,
               and evolution. Hence, UC quality impacts the overall system
               quality and defect rates. However, they are presented in natural
               language, which is usually the cause of issues related to
               imprecision, ambiguity, and incompleteness. We present the
               results of an empirical study on the formalization of UCs as
               Graph Transformation models (GTs) with the goal of running
               tool-supported analyses on them and revealing possible errors
               (treated as open issues). We describe initial steps for a
               translation from a UC to a GT, how to use an existing tool to
               analyze the produced GT, and present some diagnostic feedback
               based on the results of these analyses and the possible level of
               severity of the detected problems. To evaluate the effectiveness
               of the translation and of the analyses in identifying problems in
               UCs, we applied our approach on a set of real UC descriptions
               obtained from a software developer company and measured the
               results using a well-known metric. The final results demonstrate
               that this approach can reveal real problems that could otherwise
               go undetected and, thus, help improve the quality of the UCs.",
  year      =  2015,
  url       = "https://link.springer.com/chapter/10.1007/978-3-319-28114-8_7",
  keywords  = "Empirical study; Graph transformation; Model analysis; Use cases"
}

@ARTICLE{Hall2005-zs,
  title     = "Realising the benefits of formal methods",
  author    = "Hall, Anthony",
  journal   = "Lect. Notes Comput. Sci.",
  publisher = "Springer Verlag",
  volume    = "3785 LNCS",
  pages     = "1--4",
  abstract  = "The web site for this conference states that: ``The challenge now
               is to achieve general acceptance of formal methods as a part of
               industrial development of high quality systems, particularly
               trusted systems.'' We are all going to be discussing How to
               achieve this, but before that we should maybe ask the other
               questions: What are the real benefits of formal methods and Why
               should we care about them? When and Where should we expect to use
               them, and Who should be involved? I will suggest some answers to
               those questions and then describe some ways that the benefits are
               being realised in practice, and what I think needs to happen for
               them to become more widespread. © Springer-Verlag Berlin
               Heidelberg 2005.",
  year      =  2005,
  url       = "https://link.springer.com/chapter/10.1007/11576280_1"
}

@ARTICLE{Kelley_Sobel2002-uj,
  title    = "Formal methods application: An empirical tale of software
              development",
  author   = "Kelley Sobel, Ann E and Clarkson, Michael R",
  journal  = "IEEE Trans. Software Eng.",
  volume   =  28,
  number   =  3,
  pages    = "308--320",
  abstract = "The development of an elevator scheduling system by undergraduate
              students is presented. The development was performed by 20 teams
              of undergraduate students, divided into two groups. One group
              produced specifications by employing a formal method that involves
              only first-order logic. The other group used no formal analysis.
              The solutions of the groups are compared using the metrics of code
              correctness, conciseness, and complexity. Particular attention is
              paid to a subset of the formal methods group which provided a full
              verification of their implementation. Their results are compared
              to other published formal solutions. The formal methods group's
              solutions are found to be far more correct than the nonformal
              solutions.",
  month    =  mar,
  year     =  2002,
  url      = "http://dx.doi.org/10.1109/32.991322",
  keywords = "Formal methods; Software engineering curriculum; Software
              specifications"
}

@ARTICLE{Wijbrans2008-bp,
  title    = "Software engineering with formal methods: Experiences with the
              development of a storm surge barrier control system",
  author   = "Wijbrans, Klaas and Buve, Franc and Rijkers, Robin and Geurts,
              Wouter",
  journal  = "Lect. Notes Comput. Sci.",
  volume   = "5014 LNCS",
  pages    = "419--424",
  abstract = "This paper revisits the experiences with the use of formal methods
              in the development of the control system for the Maeslant Kering.
              The Maeslant Kering is the movable barrier which has to protect
              Rotterdam from floodings while, at almost the same time, not
              restricting shipping traffic to the port of Rotterdam. The control
              system, called BOS, completely autonomously decides about closing
              and opening of the barrier and, when necessary, also performs
              these tasks without human intervention. BOS is a safety-critical
              software system of the highest Safety Integrity Level according to
              the IEC 61508 standard. One of the reliability increasing
              techniques used during its development is formal methods. This
              paper revisits the earlier published experiences with the project
              after the system is in operation for ten years and has performed
              its first autonomous barrier operation on November 11th, 2007. ©
              2008 Springer-Verlag Berlin Heidelberg.",
  year     =  2008,
  url      = "https://www.researchgate.net/publication/221267750_Software_Engineering_with_Formal_Methods_Experiences_with_the_Development_of_a_Storm_Surge_Barrier_Control_System"
}

@ARTICLE{Cofer2012-vy,
  title     = "Formal Methods in the Aerospace Industry: Follow the Money",
  author    = "Cofer, Darren and Collins, Rockwell",
  journal   = "LNCS",
  publisher = "Springer, Berlin, Heidelberg",
  volume    =  7635,
  pages     = "2--3",
  abstract  = "Modern aircraft contain millions of lines of complex software,
               much of it performing functions that are critical to safe flight.
               This software must be verified to function correctly with the
               highest levels of assurance, and aircraft manufacturers must
               demonstrate...",
  year      =  2012,
  url       = "https://link.springer.com/chapter/10.1007/978-3-642-34281-3_2"
}

@ARTICLE{Vezzosi2019-cn,
  title     = "Cubical agda: a dependently typed programming language with
               univalence and higher inductive types",
  author    = "Vezzosi, Andrea and Mörtberg, Anders and Abel, Andreas",
  journal   = "Proceedings of the ACM on Programming Languages",
  publisher = "ACM PUB27 New York, NY, USA",
  volume    =  3,
  number    = "ICFP",
  pages     =  29,
  abstract  = "Proof assistants based on dependent type theory provide
               expressive languages for both programming and proving within the
               same system. However, all of the major implementations lack
               powerful extensi...",
  month     =  jul,
  year      =  2019,
  url       = "https://dl.acm.org/doi/10.1145/3341691",
  keywords  = "Cubical Type Theory; Dependent Pattern Matching; Higher Inductive
               Types; Univalence"
}

@ARTICLE{Pujet2022-py,
  title     = "Observational equality: now for good",
  author    = "Pujet, Loic and Tabareau, Nicolas",
  journal   = "Proceedings of the ACM on Programming Languages",
  publisher = "ACM PUB27 New York, NY, USA",
  volume    =  6,
  number    = "POPL",
  abstract  = "Building on the recent extension of dependent type theory with a
               universe of definitionally proof-irrelevant types, we introduce
               TTobs, a new type theory based on the setoidal interpretation of
               dep...",
  month     =  jan,
  year      =  2022,
  url       = "https://dl.acm.org/doi/10.1145/3498693",
  keywords  = "confluence; dependent types; rewriting theory; termination; type
               theory"
}

@ARTICLE{Barbosa2019-bi,
  title     = "Extending {SMT} Solvers to Higher-Order Logic",
  author    = "Barbosa, Haniel and Reynolds, Andrew and El Ouraoui, Daniel and
               Tinelli, Cesare and Barrett, Clark",
  journal   = "Lect. Notes Comput. Sci.",
  publisher = "Springer",
  volume    =  11716,
  pages     = "35--54",
  abstract  = "SMT solvers have throughout the years been able to cope with
               increasingly expressive formulas, from ground logics to full
               first-order logic (FOL). In contrast, the extension of SMT
               solvers to higher-order logic (HOL) is mostly un-explored. We
               propose a pragmatic extension for SMT solvers to support HOL
               reasoning natively without compromising performance on FOL
               reasoning, thus leveraging the extensive research and
               implementation efforts dedicated to efficient SMT solving. We
               show how to generalize data structures and the ground decision
               procedure to support partial applications and extensionality, as
               well as how to reconcile quantifier instantiation techniques with
               higher-order variables. We also discuss a separate approach for
               redesigning an HOL SMT solver from the ground up via new data
               structures and algorithms. We apply our pragmatic extension to
               the CVC4 SMT solver and discuss a redesign of the veriT SMT
               solver. Our evaluation shows they are competitive with
               state-of-the-art HOL provers and often outperform the traditional
               encoding into FOL.",
  month     =  aug,
  year      =  2019,
  url       = "https://hal.science/hal-02300986"
}

@ARTICLE{Moura2021-qx,
  title     = "The Lean 4 Theorem Prover and Programming Language",
  author    = "Moura, Leonardo de and Ullrich, Sebastian",
  journal   = "Lect. Notes Comput. Sci.",
  publisher = "Springer Science and Business Media Deutschland GmbH",
  volume    = "12699 LNAI",
  pages     = "625--635",
  abstract  = "Lean 4 is a reimplementation of the Lean interactive theorem
               prover (ITP) in Lean itself. It addresses many shortcomings of
               the previous versions and contains many new features. Lean 4 is
               fully extensible: users can modify and extend the parser,
               elaborator, tactics, decision procedures, pretty printer, and
               code generator. The new system has a hygienic macro system
               custom-built for ITPs. It contains a new typeclass resolution
               procedure based on tabled resolution, addressing significant
               performance problems reported by the growing user base. Lean 4 is
               also an efficient functional programming language based on a
               novel programming paradigm called functional but in-place.
               Efficient code generation is crucial for Lean users because many
               write custom proof automation procedures in Lean itself.",
  year      =  2021,
  url       = "https://link.springer.com/chapter/10.1007/978-3-030-79876-5_37"
}

@MISC{noauthor_undated-vg,
  title        = "Patterns and Matching - The Rust Programming Language",
  howpublished = "\url{https://doc.rust-lang.org/book/ch18-00-patterns.html}"
}

@MISC{noauthor_undated-cg,
  title        = "The {OCaml} language - Patterns",
  howpublished = "\url{https://v2.ocaml.org/manual/patterns.html}"
}

@MISC{noauthor_undated-yn,
  title        = "5.3. Optimisation (code improvement) — Glasgow Haskell
                  Compiler 9.6.1 User's Guide",
  howpublished = "\url{https://downloads.haskell.org/ghc/latest/docs/users\_guide/using-optimisation.html}"
}

@ARTICLE{Maranget_undated-lm,
  title    = "Warnings for pattern matching",
  author   = "Maranget, Luc",
  abstract = "We examine the ML pattern-matching anomalies of useless clauses
              and non-exhaustive matches. We state the definition of these
              anomalies, building upon pattern matching semantics , and propose
              a simple algorithm to detect them. We have integrated the
              algorithm in the Objective Caml compiler, but we show that the
              same algorithm is also usable in a non-strict language such as
              Haskell. Or-patterns are considered for both strict and non-strict
              languages.",
  url      = "http://moscova.inria.fr/~maranget/papers/warn/warn.pdf"
}

@MISC{noauthor_undated-ib,
  title        = "hash-org/lang: The Hash programming language compiler",
  howpublished = "\url{https://github.com/hash-org/lang}"
}

@MISC{noauthor_undated-os,
  title        = "Races - The Rustonomicon",
  howpublished = "\url{https://doc.rust-lang.org/nomicon/races.html}"
}

@MISC{noauthor_undated-vq,
  title        = "Official page for Language Server Protocol",
  howpublished = "\url{https://microsoft.github.io/language-server-protocol/}"
}

@ARTICLE{Coquand1996-cl,
  title     = "An algorithm for type-checking dependent types",
  author    = "Coquand, Thierry",
  journal   = "Science of Computer Programming",
  publisher = "Elsevier",
  volume    =  26,
  number    = "1-3",
  pages     = "167--177",
  abstract  = "We present a simple type-checker for a language with dependent
               types and let expressions, with a simple proof of correctness.",
  month     =  may,
  year      =  1996,
  url       = "http://dx.doi.org/10.1016/0167-6423(95)00021-6"
}

@ARTICLE{De_Bruijn1991-ok,
  title     = "Telescopic mappings in typed lambda calculus",
  author    = "de Bruijn, N G",
  journal   = "Inform. and Comput.",
  publisher = "Academic Press",
  volume    =  91,
  number    =  2,
  pages     = "189--204",
  abstract  = "The paper develops notation for strings of abstractors in typed
               lambda calculus, and shows how to treat them more or less as
               single abstractors. © 1991.",
  month     =  apr,
  year      =  1991,
  url       = "http://dx.doi.org/10.1016/0890-5401(91)90066-B"
}

@ARTICLE{Weisstein_undated-om,
  title     = "Bounded Lattice",
  author    = "Weisstein, Eric W",
  publisher = "Wolfram Research, Inc.",
  abstract  = "A bounded lattice is an algebraic structure L=(L,
               \textasciicircum , v ,0,1), such that (L, \textasciicircum , v )
               is a lattice, and the constants 0,1 in L satisfy the following:
               1. for all x in L, x \textasciicircum 1=x and x v 1=1, 2. for all
               x in L, x \textasciicircum 0=0 and x v 0=x. The element 1 is
               called the upper bound, or top of L and the element 0 is called
               the lower bound or bottom of L. There is a natural relationship
               between bounded lattices and bounded lattice-ordered sets. In
               particular, given a bounded lattice, (L, \textasciicircum , v
               ,0,1), the...",
  url       = "https://mathworld.wolfram.com/",
  keywords  = "03G10; 06B; Mathematics:Foundations of Mathematics:Set
               Theory:Lattice Theory; Mathematics:MathWorld Contributors:Insall"
}

@ARTICLE{Abel2011-sx,
  title     = "Higher-order dynamic pattern unification for dependent types and
               records",
  author    = "Abel, Andreas and Pientka, Brigitte",
  journal   = "Lect. Notes Comput. Sci.",
  publisher = "Springer, Berlin, Heidelberg",
  volume    = "6690 LNCS",
  pages     = "10--26",
  abstract  = "While higher-order pattern unification for the λ calculus is
               decidable and unique unifiers exists, we face several challenges
               in practice: 1) the pattern fragment itself is too restrictive
               for many applications; this is typically addressed by solving
               sub-problems which satisfy the pattern restriction eagerly but
               delay solving sub-problems which are non-patterns until we have
               accumulated more information. This leads to a dynamic pattern
               unification algorithm. 2) Many systems implement λ calculus and
               hence the known pattern unification algorithms for λ are too
               restrictive. In this paper, we present a constraint-based
               unification algorithm for λ -calculus which solves a richer class
               of patterns than currently possible; in particular it takes into
               account type isomorphisms to translate unification problems
               containing ∑-types into problems only involving Π-types. We prove
               correctness of our algorithm and discuss its application. © 2011
               Springer-Verlag Berlin Heidelberg.",
  year      =  2011,
  url       = "https://link.springer.com/chapter/10.1007/978-3-642-21691-6_5"
}

@MISC{noauthor_undated-fm,
  title        = "{IEEE} {SA} - {IEEE} 754-2019",
  howpublished = "\url{https://standards.ieee.org/ieee/754/6210/}"
}

@ARTICLE{Flanagan_undated-lx,
  title    = "Hybrid Type Checking",
  author   = "Flanagan, Cormac",
  journal  = "In contrast, dynamic contract checking",
  volume   =  24,
  pages    =  27,
  abstract = "Traditional static type systems are very effective for verifying
              basic interface specifications, but are somewhat limited in the
              kinds specifications they support. Dynamically-checked contracts
              can enforce more precise specifications, but these are not checked
              until run time, resulting in incomplete detection of defects.
              Hybrid type checking is a synthesis of these two approaches that
              enforces precise interface specifications, via static analysis
              where possible, but also via dynamic checks where necessary. This
              paper explores the key ideas and implications of hybrid type
              checking, in the context of the simply-typed λ-calculus with
              arbitrary refinements of base types. The construction of reliable
              software is extremely difficult. For large systems, it requires a
              modular development strategy that, ideally, is based on precise
              and trusted interface specifications. In practice, however,
              programmers typically work in the context of a large collection of
              APIs whose behavior is only informally and imprecisely specified
              and understood. Practical mechanisms for specifying and verifying
              precise, behavioral aspects of interfaces are clearly needed.
              Static type systems have proven to be extremely effective and
              practical tools for specifying and verifying basic interface
              specifications , and are widely adopted by most software
              engineering projects. However, traditional type systems are
              somewhat limited in the kinds of specifications they support.
              Ongoing research on more powerful type systems (e.g., [45, 44, 17,
              29, 11]) attempts to overcome some of these restrictions, via
              advanced features such as dependent and refinement types. Yet
              these systems are designed to be statically type safe, and so the
              specification language is intentionally restricted to ensure that
              specifications can always be checked statically., 36, 25] provides
              a simple method for checking more expressive specifications.
              Dynamic checking can easily support precise specifications , such
              as:-Subrange types, e.g., the function printDigit requires an
              integer in the range [0,9].-Aliasing restrictions, e.g., swap
              requires that its arguments are distinct reference cells.-Ordering
              restrictions, e.g., binarySearch requires that its argument is a
              sorted array.-Size specifications, e.g., the function
              serializeMatrix takes as input a matrix of size n by m, and
              returns a one-dimensional array of size n × m.-Arbitrary
              predicates: an interpreter (or code generator) for a typed
              language (or intermediate representation [39]) might naturally
              require that its input be well-typed, i.e., that it satisfies the
              predicate wellTyped : Expr → Bool. However, dynamic checking
              suffers from two limitations. First, it consumes cycles that could
              otherwise perform useful computation. More seriously, dynamic
              checking provides only limited coverage specifications are only
              checked on data values and code paths of actual executions. Thus,
              dynamic checking often results in incomplete and late (possibly
              post-deployment) detection of defects. Thus, the twin goals of
              complete checking and expressive specifications appear to be
              incompatible in practice. 1 Static type checking focuses on
              complete checking of restricted specifications. Dynamic checking
              focuses on incomplete checking of expressive specifications.
              Neither approach in isolation provides an entirely satisfactory
              solution for checking precise interface specifications. In this
              paper, we describe an approach for validating precise interface
              specifications using a synthesis of static and dynamic techniques.
              By checking correctness properties and detecting defects
              statically (whenever possible) and dynamically (only when
              necessary), this approach of hybrid type checking provides a
              potential solution to the limitations of purely-static and
              purely-dynamic approaches. We illustrate the key idea of hybrid
              type checking by considering the type rule for function
              application: E t1 : T → T E t2 : S E S <: T E (t1 t2) : T 1
              Complete checking of expressive specifications could be achieved
              by requiring that each program be accompanied by a proof (perhaps
              expressed as type annotations) that the program satisfies its
              specification, but this approach is too heavyweight for widespread
              use.",
  keywords = "D31 [Programming Lan-guages: Formal Definitions and Theory]:
              specification and verifi-cation General Terms Languages; Theory;
              Verification Keywords Type systems; contracts; dynamic checking 1
              Motivation; static checking"
}
