\section{Introduction}\label{sec:intro}

Functional languages offer a high degree of expressiveness using a very small
set of primitives: most employ a variant of the typed lambda calculus, commonly
with schemas for defining recursive or inductive types. Such constructs allow
programmers to define data and logic in a natural, algebraic way that is
amenable to abstraction. Data structures such as lists, trees, and even natural
numbers are the archetypical examples of inductively defined data, which offer
pattern-matching as a principled way to introduce `complete' branching points
in a program. Despite these advantages, functional languages generally do not
provide programmers with many tools to specify how their programs should be
represented on physical computers. For inductively-defined data, there is a
fixed representation that the majority of functional languages utilise, which
is to represent each constructor as a heap cell, and link chains of
constructors together using pointer indirection. As a result, a list as an
inductively defined data structure is stored as a linked list, and a natural
number is stored as a unary number where each digit is an `empty' heap cell. To
get around this issue, most real-world implementations expose underlying
machine primitive types such as contiguous arrays and bitvectors, and
programmers are able to utilise these instead of the `algebraic'
inductively-defined types to make their functional programs more performant. In
Haskell, the \texttt{array} package comes to mind, as well as the \hs{Integer}
and \hs{Natural} primitives which utilise a GMP-style big-integer
implementation internally.

\subsection{The `Nat-hack'}

In functional languages with dependent types, such as Idris or Lean, the issue
of type representation is further complicated by the fact that inductive proofs
come into the mix. Inductively defined types enjoy induction principles which
can prove facts about them on a case-by-case basis. The standard example is the
induction over the natural numbers \idr{data Nat = Z | S Nat} given by
\begin{bminted}{idris}
  induction : (P : Nat -> Type) -> P Z -> ((m : Nat) -> P m -> P (S m)) -> (n : Nat) -> P n
\end{bminted}
This can be proven trivially by well-founded recursion and case analysis over
\idr{Z} and \idr{S}. However, if
natural numbers are not defined inductively, but rather opaquely as an
intricate data structure like Haskell's \idr{Natural}, induction is no longer
given `for free', and must be manually proven internally in the language. On
the other hand, if natural numbers are defined inductively, then many
operations become inexcusably slow, such as multiplication
\begin{bminted}{idris}
  mul : Nat -> Nat -> Nat
  mul Z b = Z
  mul (S a) b = add (mul a b) b
\end{bminted}
taking $a \times b$ steps to compute for input numbers $a$ and $b$. To solve
this problem, Idris, Agda and Lean `short-circuit' the default inductive type
representation for natural numbers, to use arbitrarily-sized big integers for
calculations whose digits are bitvectors, rather than unary numbers.

To describe how this trick works, assume we have access to a type \idr{BigUInt}
for arbitrarily-sized big integers, along with some primitive operations
\begin{bminted}{idris}
  bigZero, bigOne : BigUint
  bigAdd, bigMul, bigSub : BigUInt -> BigUInt -> BigUInt
  bigIsZero : BigUInt -> Bool
\end{bminted}
In a well-typed input program, all occurences of the zero constructor \idr|Z : Nat|
should be replaced with the constant \idr{bigZero}, and all occurences of the
successor constructor \idr|S : Nat -> Nat| should be replaced with the expression
\idr|\x -> bigAdd x bigOne|. For case analysis, each pattern matching expression
\idr+case x of Z -> b | S n -> r n+ should be replaced with the conditional
expression \idr|if bigIsZero x then b else r (bigSub x bigOne)|. Additionally,
some basic functions on natural numbers should be replaced with more performant
variants. The recursively-defined addition function \idr|add : Nat -> Nat -> Nat|
should be replaced with \idr{bigAdd} and similarly \idr{mul} should be replaced with
\idr{bigMul}.
This way, the high-level program still appears to be using the inductive definition
\idr{Nat}, but upon compilation it uses \idr{BigUInt} for efficient execution.

\subsection{Beyond natural numbers}

There are arguably many inductive types which could admit a more optimised
representation than the default linked tree. The first which comes to mind is
the type of lists \idr{data List t = Nil | Cons t (List t)}. There exist many
representations of lists in memory, including flattened contiguous heap-backed
arrays with dynamic resizing, singly-linked lists, doubly-linked lists, their
circular variants, tree-based representations like binary search trees, their
balanced variants, B and B+ trees, and segment trees. Each representation
offers different performance characteristics for common operations such as
appending, insertion, deletion, splicing, concatenating, and so on.
Nevertheless, all of them are \idr{List} in spirit; there is a `canonical'
bijection between a list and any of the afforementioned data structures. A
functional program using the algebraic \idr{List} type could potentially
benefit from a different representation depending on the exact operations it
performs and in what proportion. Not only lists, but structures such as trees,
finite sets, as well as refinement predicates on types such as element proofs
or parity proofs, could all be subject to more optimal representations. Since,
at first glance, such an alteration could be done purely mechanically in a
similar way to the `Nat-hack', it is natural to wonder if this technique
readily generalises to user-defined inductive types such that the
transformation itself is specified in the same language.

\subsection{Contributions}

This paper develops an extension of dependently-typed lambda calculus with
inductive constructions, in order to support the definition of custom
representations of the inductive constructions, along with the specialisation
of functions for fine tuning to the chosen representations. This system
features correct-by-construction representations, awarding the programmer with
a bijection proof between an inductive type and its representation, as well as
an elaboration into a lower language with a guarantee that no inductive
constructors or eliminators remain. The standard linked tree representation of
inductive types that is the hardcoded default in most implementations is
incarnated as `just another representation' in this system, special only
because it can apply to \emph{any} inductive type. The order of these
developments in the paper are as follows:

\begin{itemize}
  \item The language $\lambdaprim$ is introduced, which is a staged language with
        dependent types, whose object-level fragment contains some machine primitives
        that act as building blocks of data representations.
  \item The language $\lambdaind$ is introduced as an extension of $\lambdaprim$, which
        allows the familiar definition of inductive types, living in a universe of
        `codes' for object-level types.
  \item The language $\lambdarep$ is introduced as the completion of $\lambdaind$,
        containing a `representation' construct that assigns concrete object-level
        codes to inductive types. An elaboration procedure $\Crepr{} : \lambdarep \to
          \lambdaprim$ is formulated which eliminates inductive constructs through the
        defined representations, yielding the final program that can be staged and
        compiled.
  \item The `Nat-hack' is defined internally in $\lambdarep$ and shown to be coherent
        up to some notable assumptions.
  \item The standard linked tree representation is recovered in $\lambdarep$ for any
        inductive type, and shown to be coherent.
  \item The system is further explored, showcasing representations for other inductive
        types. Some desirable extensions as part of future work are discussed.
\end{itemize}
