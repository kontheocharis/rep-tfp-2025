\section{A tour of data representations}\label{sec:examples}

A common optimisation done by programming languages with dependent types such as
Idris 2 and Lean is to represent natural numbers as GMP-style big integers. The
definition of natural numbers looks like
\begin{equation}\label{eq:nat-def}
  \Sdata{Nat}{}{} \Sbody{
    \ctorlab{0} & : \datalab{Nat} \\
    \ctorlab{1+} & : \datalab{Nat} \to \datalab{Nat}
  }
\end{equation}
and generates a Peano-style induction principle $\elimlab{Nat}$ of type\footnote{Recursive parameters
like $\overline{P\ n}$ are lazy, which makes the eliminator more efficient when they
are not used.}
\[
   (P : \datalab{Nat} \to \univ)
  \to P\ \ctorlab 0 \to ((n : \datalab{Nat}) \to \overline{P\ n} \to P\ (\ctorlab{1+}\ n))
  \to (s : \datalab{Nat}) \to P\ s \,.
\]
Without further intervention, the $\datalab{Nat}$ type is represented in unary
form, where each digit becomes an empty heap cell at runtime. This is
inefficient for a lot of the basic operations on natural numbers, especially
since computers are particularly well-equipped to deal with numbers natively, so
many real-world implementations will treat \datalab{Nat} specially, swapping the
default inductive type representation with one based on GMP integers. This is
done by performing the replacements
\begin{align}
  |\ctorlab{0}| &= \primlab{0} \\ \label{eq:nat-repr-untyped}
  |\ctorlab{1+}| &= \primlab{1}\ \primlab{+} \\
  |\elimlab{Nat}\ P\ m_{\ctorlab 0}\ m_{\ctorlab{$1+$}}\ s| &=\primlab{ubig-elim}\ |s|\ |m_{\ctorlab 0}|\ |m_{\ctorlab{$1+$}}|
\end{align}
where $|\cdot|$ denotes a source translation into a compilation target
language with primitives $\primlab{ubig-$\ast$}$.\footnote{
Idris 2 will in fact look for any `Nat-like' types and apply this optimisation. A Nat-like type
is any type with two constructors, one with arity zero and the other with arity one.
A similar optimisation is also done with list-like and boolean-like types because
they have a canonical representation in the target runtime, Chez Scheme.}

In addition to the constructors and eliminators, the compiler might also define
translations for commonly used definitions which have a more efficient
counterpart in the target, such as recursively-defined addition, multiplication,
etc. The recursively-defined functions are well-suited to proofs and reasoning,
while the GMP primitives are more efficient for computation.

The issue with this approach is that it only works for the data types which the
compiler can recognise as special. Particularly in the presence of dependent
types, other data types might end up being equivalent to \datalab{Nat} or
another `nicely-representable' type, but in a non-trivial way that the compiler
cannot recognise. Hence, one of our goals is to extend this optimisation to work
for any data type. To achieve this this, our framework requires that
representations are fully typed in a way that ensures the behaviour of the
representation of a data type matches the behaviour of the data type itself.

\subsection{The well-typed Nat-hack}

A representation definition looks like
\[
  \Sreprvar{\datalab{Nat}}{\primlab{UBig}} \Sbody{
    \ctorlab{0}\ \as&\ \primlab{0} \\
    \ctorlab{1+}\ n\ \as&\ \primlab{1}\,\primlab{$+$}\,n \\
    \elimlab{Nat}\ \as&\ \primlab{ubig-elim}\\ \by&\ \primlab{ubig-elim-zero-id}, \\ &\ \primlab{ubig-elim-add-one-id}
  }
\]
$\datalab{Nat}$ is represented as the type $\primlab{UBig}$ of GMP-style
unlimited-size unsigned integers, with translations for the constructors
$\ctorlab{0}$ and $\ctorlab{1+}$, and the eliminator $\elimlab{Nat}$.
Additionally, the eliminator satisfies the expected computation rules of the
$\datalab{Nat}$ eliminator, which are postulated as propositional equalities.
This representation is valid in a context containing the primitives
\begin{gather*}
  \primlab{0}, \primlab{1} : \primlab{UBig} \qquad
  \primlab{$+$},\primlab{$\times$} : \primlab{UBig} \to \primlab{UBig} \to \primlab{UBig} \\
  \begin{aligned}
  \primlab{ubig-elim} :\ &(P : \primlab{UBig} \to \univ) \to P\ \primlab{0}
  \to ((n : \primlab{UBig}) \to \overline{P\ n} \to P\ (\primlab{1}\,\primlab{$+$}\,n)) \\
   &\to (s : \primlab{UBig}) \to P\ s
  \end{aligned}
\end{gather*}
and propositional equalities
\begin{align*}
  &\primlab{ubig-elim-zero-id} : _{\forall Pbr}  \primlab{ubig-elim}\ P\ b\ r\ \primlab{0} = b \\
  &\primlab{ubig-elim-add-one-id} : _{\forall Pbrn}  \primlab{ubig-elim}\ P\ b\ r\ (\primlab{1}\,\primlab{$+$}\,n)
  = r\ n\ (\lambda \wildp. \ \primlab{ubig-elim}\ P\ b\ r\ n)  \,.
\end{align*}
Representations can also be defined for functions on $\datalab{Nat}$, such as
addition, multiplication, and other numeric operations, in terms of
$\primlab{UBig}$ primitives.
\begin{align*}
  \Sreprvar{\datalab{add}}{\primlab{$+$}\ \by\ \primlab{$+$-id}}
  \qquad \Sreprvar{\datalab{mul}}{\primlab{$\times$}\ \by\ \primlab{$\times$-id}}
\end{align*}
 These will be replaced during a translation process
back to $\lambdaind$, like rewriting rules \cite{Cockx2021-pw}, given
that we have the appropriate lemmas to justify them in the context.

This will effectively erase the \datalab{Nat} type from the compiled program,
replacing all occurrences with the $\primlab{UBig}$ type and its primitives. In
a way, the hard work is done by the postulates above; we expect that the
underlying implementation of $\primlab{UBig}$ indeed satisfies them, which is a
separate concern from the correctness of the representation itself. However,
postulates are only needed when the representation target is a primitive; the
next examples use defined types as targets, so that the coherence of the target
eliminator follows from the coherence of other eliminators used in its
implementation.

\subsection{Vectors are just certain lists}

In addition to representing inductive types as primitives, we can use
representations to share the underlying data when converting between indexed
views of the same data. For example, we can define a representation of
$\datalab{Vec}$ in terms of $\datalab{List}$, so that the conversion from one to
the other is `compiled away'. We can do this by representing the indexed type as
a refinement of the unindexed type by an appropriate relation. For the case of
$\datalab{Vec}$, we know intuitively that
\[
  \datalab{Vec}\ T\ n \simeq \{ l : \datalab{List}\ T \mid \lab{length}\ l = n \} := \lab{List'}\ T\ n
\]
so we can start by choosing $\lab{List'}\ T\ n$ as the representation of
$\datalab{Vec}\ T\ n$.\footnote{We will take the subset $\{ x : A \mid P\ x \}$ to mean a
$\Sigma$-type $(x : A) \times P\ x$ where the right component is irrelevant and
erased at runtime.} We are then tasked with providing terms that correspond to
the constructors of $\datalab{Vec}$ but for $\lab{List'}$. These can be defined
as
\[
  \begin{aligned}
  & \lab{nil} : \lab{List'}\ T\ 0 \\
  & \lab{nil} = (\ctorlab{nil}, \ctorlab{refl})
  \end{aligned}\qquad
  \begin{aligned}
  & \lab{cons} : T \to \lab{List'}\ T\ n \to \lab{List'}\ T\ (\ctorlab{1+}\ n) \\
  & \lab{cons}\ x\ (\mathit{xs}, p) = (\ctorlab{cons}\ x\ \mathit{xs}, \lab{cong}\ (\ctorlab{1+})\ p)
  \end{aligned}
\]
Next we need to define the eliminator for $\lab{List'}$, which should have the form
\begin{align*}
  & \lab{elim-List'} : (E : (n : \datalab{Nat}) \to \lab{List'}\ T\ n \to \lab{Type}) \\
  & \quad \to E\ 0\ \lab{nil} \\
  & \quad \to ((x : T) \to (n : \datalab{Nat}) \to (\mathit{xs} : \lab{List'}\ T\ n) \to \overline{E\ n\ xs} \to E\ (\ctorlab{1+}\ n)\ (\lab{cons}\ x\ \mathit{xs})) \\
  & \quad \to (n : \datalab{Nat}) \to (v : \lab{List'}\ T\ n) \to E\ n\ v
\end{align*}
Dependent pattern matching does a lot of the heavy lifting by refining the
length index and equality proof by matching on the underlying list. However we still need to
substitute the lemma $\lab{cong}\ (\ctorlab{1+})\ (\lab{\ctorlab{1+}-inj}\ p) = p$ in the recursive case.
\begin{align*}
  &\lab{elim-List'}\ P\ b\ r\ \ctorlab{0}\ (\ctorlab{nil}, \ctorlab{refl}) = b \\
  &\lab{elim-List'}\ P\ b\ r\ (\ctorlab{1+}\ m)\ (\ctorlab{cons}\ x\ \mathit{xs}, e) = \lab{subst}\
  \begin{aligned}[t]
  & (\lambda p .\ P\ (\ctorlab{1+}\ m)\ (\ctorlab{cons}\ x\ \mathit{xs}, p))\ \\
  & (\lab{\ctorlab{1+}-cong-id}\ e)\ (r\ x\ ({\mathit{xs}}, \lab{\ctorlab{1+}-inj}\ e) \\
  & (\lambda \wildp . \ \lab{elim-List'}\ P\ b\ r\ m\ ({\mathit{xs}}, \lab{\ctorlab{1+}-inj}\ e)))
  \end{aligned}
\end{align*}
Finally, we need to prove that the eliminator satisfies the expected computation
rules propositionally. These are
\begin{align*}
  \lab{elim-List'-nil-id} : &\ \lab{elim-List'}\ P\ b\ r\ \ctorlab 0\ (\ctorlab{nil}, \ctorlab{refl}) = b \\[0.5em]
  \lab{elim-List'-cons-id} :&\  \lab{elim-List'}\ P\ b\ r\ (\ctorlab{1+}\ m)\ (\ctorlab{cons}\ x\ \mathit{xs}, \lab{cong}\ (\ctorlab{1+})\ p) \\
  & = r\ x\ (\mathit{xs}, p)\ (\lambda \wildp .\ \lab{elim-List'}\ P\ b\ r\ m\ (\mathit{xs}, p))
\end{align*}
which we leave as an exercise, though they are evident from the definition of
$\lab{elim-List'}$. This completes the definition of the representation of
$\datalab{Vec}$ as $\lab{List'}$, which would be written as
\[
  \Sreprvar{\datalab{Vec}\ T\ n}{\lab{List'}\ T\ n} \Sbody{
    \ctorlab{nil}\ \as&\ \lab{nil} \\
    \ctorlab{cons}\ \as&\ \lab{cons} \\
    \elimlab{Vec}\ \as&\ \lab{elim-List'} \\
     \by&\ \lab{elim-List'-nil-id}, \\ &\ \lab{elim-List'-cons-id}
  }
\]
Now the hard work is done; Every time we are working with a $v : \datalab{Vec}\
T\ n$, its form will be $(l, p)$ at runtime, where $l$ is the underlying list
and $p$ is the proof that the length of $l$ is $n$. Under the assumption that
the $\Sigma$-type's right component is irrelevant and erased at runtime, every
vector is simply a list at runtime, where the length proof has been erased. In
practice, this erasure is achieved in \superfluid using quantitative type theory
\cite{Atkey2018-pj}. In \cref{sub:irr} we show how to formally identify
computationally irrelevant conversion functions.

We can utilise this representation to convert between $\datalab{Vec}$ and
$\datalab{List}$ at zero runtime cost, by using the $\repr{}$ and $\unrepr{}$
operators of the language (defined in \cref{sec:type-system}). Specifically, we
can define the functions
\begin{align*}
  &\lab{forget-length} : \datalab{Vec}\ T\ n \to \datalab{List}\ T \\
  &\lab{forget-length}\ v = \letin{(l, \wildp)}{\repr{v}}{l} \\[1em]
  &\lab{recall-length} : (l : \datalab{List}\ T) \to \datalab{Vec}\ T\ (\lab{length}\ l) \\
  &\lab{recall-length}\ l = \unrepr{(l, \ctorlab{refl})}
\end{align*}
and in \cref{sub:irr} we will show that it holds by reflexivity that
such functions are inverses of one another.

\subsection{General reindexing}

The idea from the previous example can be generalised to any data type. In
general, suppose that we have two inductive families
\[
 \datalab{F} : P \to \univ \qquad \datalab{G} : P \to X\ p \to \univ
\]
for some index family $X : P \to \univ$. If we hope to represent $\datalab{G}$
as some refinement of $\datalab{F}$ then we must be able to provide a way to
compute $\datalab{G}$'s extra indices $X$ from $\datalab{F}$, like we computed
$\datalab{Vec}$'s extra $\datalab{Nat}$ index from $\datalab{List}$ with
$\lab{length}$ in the previous example. This means that we need to provide a
function $\lab{comp} :_{\forall p} \datalab{F}\ p \to X\ p$ which can then be
used to form the family
\[
  \datalab{F}^{\lab{comp}}\ p\ x :=  \{ f : \datalab{F}\ p \mid \lab{comp}\ f = x \} .
\]
If $\datalab{G}$ is `equivalent' to the algebraic ornament of $\datalab{F}$ by
the algebra defining $\lab{comp}$ (given by an isomorphism between the
underlying polynomial functors), then it is also equivalent to the $\Sigma$-type
above. The `recomputation lemma' of algebraic ornaments \cite{Dagand2012-aw}
then arises from its projections. Our system allows us to \emph{set} the
representation of $\datalab{G}$ as $\datalab{F}^{\lab{comp}}$, so that the
forgetful map from $\datalab{G}$ to $\datalab{F}$ is the identity at runtime.

\subsection{Zero-copy deserialisation}

The machinery of representations can be used to implement zero-copy deserialisation
of data formats into inductive types. For example, consider the following
record for a player in a game:
\[
  \Sdata{Player}{}{} \Sbody{
    \ctorlab{player} : &\ (\fieldlab{position} : \datalab{Position}) \\
    \to &\ (\fieldlab{direction} : \datalab{Direction}) \\
    \to &\ (\fieldlab{items} : \datalab{Fin}\ \lab{MAX\_INVENTORY}) \\
    \to &\ (\fieldlab{inventory} : \datalab{Inventory}\ (\lab{fin-to-nat}\ \fieldlab{items})) \to \datalab{Player}
  }
\]
We can use the $\datalab{Fin}$ type to maintain the invariant that the inventory
has a maximum size. Additionally, we can index the $\datalab{Inventory}$ type by
the number of items it contains, which might be defined similarly to $\datalab{Vec}$:
\[
  \Sdata{Inventory}{(n : \datalab{Nat})}{} \Sbody{
    \ctorlab{empty} : &\ \datalab{Inventory}\ \ctorlab 0 \\
    \ctorlab{add} : &\ \datalab{Item} \to \datalab{Inventory}\ n \to  \datalab{Inventory}\ (\ctorlab{1+}\ n)
  }
\]
We can use the full power of inductive families to model the domain of our
problem in the way that is most convenient for us. If we were writing this in a
lower-level language, we might choose to use the serialised format directly when
manipulating the data, relying on the appropriate pointer arithmetic to access
the fields of the serialised data, to avoid copying overhead. Representations
allow us to do this while still being able to work with the high-level inductive
type.

We can define a representation for $\datalab{Player}$
as a pair of a byte buffer and a proof that the byte buffer contents correspond to
a player record. Similarly, we can define a representation for $\datalab{Inventory}$
as a pair of a byte buffer and a proof that the byte buffer contents correspond to
an inventory record of a certain size. By the implementation of the eliminator
in the representation, the projection $\fieldlab{inventory} : (p :
\datalab{Player}) \to \datalab{Inventory}\ p.\fieldlab{items}$ is compiled into
some code to slice into the inventory part of the player's byte buffer. We
assume that the standard library already represents $\datalab{Fin}$ in the same
way as $\datalab{Nat}$, so that reading the $\fieldlab{items}$ field is a
constant-time operation (we do not need to build a unary numeral). We can thus
define the representation
of $\datalab{Player}$ as
\[
  \Sreprvar{\datalab{Player}}{\{ \datalab{Buf} \mid \datalab{IsPlayer} \}} \Sbody{
    \ctorlab{player}\ \as&\ \lab{buf-is-player} \\
    \elimlab{Player}\ \as&\ \lab{elim-buf-is-player} \\
                      \by&\ \lab{elim-buf-is-player-id}
  }
\]
with an appropriate definition of $\datalab{IsPlayer}$ which refines a byte
buffer. The refinement would have to match the expected structure of the byte
buffer, so that all the required fields can be extracted. Allais
\cite{Allais2023-zq} explores how data descriptions that index into a flat
buffer could be defined.


\subsection{Transitivity}

Representations are transitive, so in the previous example, the `terminal'
representation of $\datalab{Vec}$ also depends on the representation of
$\datalab{List}$. It is possible to define a custom representation for
$\datalab{List}$ itself, for example a heap-backed array or a finger tree, and
$\datalab{Vec}$ would inherit this representation. However it will still be the
case that $\Repr{(\datalab{Vec}\ T\ n)} \equiv \datalab{List}\ T$, which means the
$\repr{}/\Repr{}$ operators only look at the immediate representation of a
term, not its terminal representation. Regardless, we can construct predicates that
find types which satisfy a certain `eventual' representation. For example, given a
\datalab{Buf} type of byte buffers, we can consider the set of all types which are eventually
represented as a \datalab{Buf}:
\[
  \Sdata {ReprBuf}{(T : \univ)}{} \Sbody{
    \ctorlab{buf} &: \datalab{ReprBuf}\ \datalab{Buf} \\
    \ctorlab{from} &: \datalab{ReprBuf}\ (\Repr{T}) \to \datalab{ReprBuf}\ T \\
    \ctorlab{refined} &: \datalab{ReprBuf}\ T \to \datalab{ReprBuf}\ \{ t : T \mid  P\ t \}
  }
\]
Every such type comes with a projection function to the \datalab{Buf} type
\begin{align*}
  &\lab{as-buf} :\{r : \datalab{ReprBuf}\ T\} \to T \to \datalab{Buf} \\
  &\lab{as-buf}\ \{r = \ctorlab{buf}\}\ x = x \\
  &\lab{as-buf}\ \{r = \ctorlab{from}\ t\}\ x = \lab{as-buf}\ t\ (\repr{x}) \\
  &\lab{as-buf}\ \{r = \ctorlab{refined}\ t\}\ (x, \wildp) = \lab{as-buf}\ t\ x
\end{align*}
which eventually computes to the identity function after applying $\repr{}$ the
appropriate amount of times. Upon compilation, every type is converted to its
terminal representation, and all $\repr{}$ calls are erased, so the
$\lab{as-buf}$ function is effectively the identity function at
runtime.\footnote{Given that the $r$ argument is known at compile-time and monomorphised.}
