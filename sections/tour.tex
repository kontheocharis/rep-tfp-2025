\section{A tour of data representations}\label{sec:examples}

A common optimisation done by programming languages with dependent types such as
Idris 2 and Lean is to represent natural numbers as GMP-style big integers. The core
definition of natural numbers looks like
\begin{equation}\label{eq:nat-def}
  \Sdata{Nat}{}{} \Sbody{
    \ctorlab{0} & : \datalab{Nat} \\
    \ctorlab{1+} & : \datalab{Nat} \to \datalab{Nat}
  }
\end{equation}
This definition is convenient because it generates a Peano-style induction principle
\begin{align*}
  \elimlab{Nat} :\ &(P : \datalab{Nat} \to \univ) \\
  & \to (m_{\ctorlab{0}} : P\ \ctorlab 0) \to (m_{\ctorlab{1+}} : (n : \datalab{Nat}) \to P\ n \to P\ (\ctorlab{1+}\ n)) \\
  & \to (s : \datalab{Nat}) \to P\ s \,.
\end{align*}
Moreover, dependent pattern matching with structural recursion on
$\datalab{Nat}$ can be elaborated into invocations of $\elimlab{Nat}$
\cite{Goguen2006-sy,Cockx2018-bv,Cockx2018-fk}, and lemmas such as constructor
injectivity, disjointness and acyclicity \cite{McBride2006-tz} can be
substituted automatically to simplify the context and goal. This way, if
addition is defined using recursion and pattern matching, proofs like the
commutativity of addition or the additive identity of $\ctorlab 0$ are easy to
write.

When it comes to compilation, without further intervention, the $\datalab{Nat}$
type is represented in unary form, where each digit is an empty heap cell. This
is inefficient for a lot of the basic operations on natural numbers; addition is
a linear-time operation and multiplication is quadratic. For this reason, most
real-world implementations will treat \datalab{Nat} specially, swapping the
default inductive type representation with one based on GMP integers. The basic
idea is to perform the replacements
\begin{align}
  |\ctorlab{0}| &= \primlab{ubig-zero} \\ \label{eq:nat-repr-untyped}
  |\ctorlab{1+}| &= \primlab{ubig-add}\ \primlab{ubig-one} \\
  |\elimlab{Nat}\ P\ m_{\ctorlab 0}\ m_{\ctorlab{$1+$}}\ s| &=\primlab{ubig-if-zero}\ |s|\ |m_{\ctorlab 0}|\ |m_{\ctorlab{$1+$}}|
\end{align}
where $|\cdot|$ denotes a source translation into an untyped compilation target
language with primitives $\lab{ubig-$\ast$}$.

In addition to the constructors and eliminators, the compiler might also define translations
for commonly used definitions which have a more efficient counterpart in the untyped target, such as
$|+| = \primlab{ubig-add}$, $|\times| = \primlab{ubig-mul}$, etc.
Idris 2 will in fact look for any `Nat-like' types and apply this optimisation. A Nat-like type
is any type with two constructors, one with arity zero and the other with arity one.
A similar optimisation is also done with list-like and boolean-like types.

The issue with this approach is that it only works for the data types which the
compiler can recognise as special. Particularly in the presence of dependent
types, other data types might end up being equivalent to \datalab{Nat} or another
`nicely-representable' type, but in a non-trivial way that the compiler cannot recognise.
Hence, our first goal is to extend this optimisation to work for any data type.
Additionally, the optimisation is defined as a compilation step, which means
that if non-trivial representations are provided by the user, their correctness
is not guaranteed. To address this, our framework requires that representations
are fully typed, ensuring that the behaviour of the representation of a data
type matches the behaviour of the data type itself.

\subsection{Main idea of representations}

TODO

\subsection{Natural numbers as big unsigned integers}

To build up data out of something other than just inductive types, we need
access to some primitive types which are `provided by the runtime environment'.
It would be reasonable to assume access to types corresponding to machine words
of different widths, but for brevity we will go a step further and assume access
to an unlimited-size unsigned integer type $\primlab{UBig}$, along with:
\begin{align*}
  &\primlab{0} : \primlab{UBig} \\
  &\primlab{1} : \primlab{UBig} \\
  &\primlab{$+$} : \primlab{UBig} \to \primlab{UBig} \to \primlab{UBig} \\
  &\primlab{$\times$} : \primlab{UBig} \to \primlab{UBig} \to \primlab{UBig} \\
  &\primlab{ubig-elim} : (P : \primlab{UBig} \to \univ) \to P\ \primlab{0} \\
  &  \to ((n : \primlab{UBig}) \to P\ (\primlab{1}\,\primlab{$+$}\,n)) \\
  & \to (s : \primlab{UBig}) \to P\ s
\end{align*}
and propositional equalities
\begin{align*}
  &\primlab{ubig-elim-zero-id} : _{\forall Pbr}  \primlab{ubig-elim}\ P\ b\ r\ \primlab{0} = b \\
  &\primlab{ubig-elim-add-one-id} : _{\forall Pbrn}  \primlab{ubig-elim}\ P\ b\ r\ (\primlab{1}\,\primlab{$+$}\,n)
  = r\ n\ (\primlab{ubig-elim}\ P\ b\ r\ n)  \,.
\end{align*}
The equalities above postulate that $\primlab{UBig}$'s eliminator satisfies the
expected computation rules of the $\datalab{Nat}$ eliminator, albeit
propositionally rather than definitionally.
In contrast to the earlier example \eqref{eq:nat-repr-untyped}, this interface is fully \emph{typed}.

Having access to $\primlab{UBig}$, we can define a representation for the $\datalab{Nat}$ data type
which maps the constructors and eliminators of $\datalab{Nat}$ to the primitives of $\primlab{UBig}$:
\[
  \Sreprvar{\datalab{Nat}}{\primlab{UBig}} \Sbody{
    \ctorlab{0}\ \as&\ \primlab{0} \\
    \ctorlab{1+}\ n\ \as&\ \primlab{1}\,\primlab{$+$}\,n \\
    \elimlab{Nat}\ \as&\ \primlab{ubig-elim}\\ \by&\ \primlab{ubig-elim-zero-id}, \\ &\ \primlab{ubig-elim-add-one-id}
  }
\]

This will effectively erase the \datalab{Nat} type from the compiled program,
replacing all occurrences with the $\primlab{UBig}$ type and its primitives.
This is an example of a \emph{representation definition}, which is part of the
global context of a program, which can access top-level symbols.

% The
% requirements for representing an inductive type family $\datalab{D}$,
% with parameters $\Delta$ and indices $\Xi$ are to provide:

% \subsection{Encoding datatypes}

% A datatype $\datalab{D}$ is defined by the following data:
% \begin{itemize}
%   \item A telescope of parameters $\Delta$, used uniformly across constructors.
%   \item A telescope of indices $\Delta \vdash \Xi$, which can vary between constructors.
%   \item A list of constructors, and for each constructor $\ctorlab{c}_i$,
%   \begin{itemize}
%     \item A telescope of data arguments $\Delta \vdash \Pi_i$. These cannot refer to $\datalab{D}$.
%     \item A telescope of arities $\Delta,\Pi \vdash \Omega_i$.
%     \item For each spine $\omega : \Omega_i$,
%       a spine of indices $\Delta,\Pi,\Omega_i \vdash \zeta_i : \Xi$
%       for each recursive argument $\rho_\omega : \datalab{D}\ \delta\ (\zeta_i[\omega])$.
%     \item A telescope of indices $\Delta \vdash \Xi$, which can vary between constructors.
%     \item A spine of indices for the constructor $\Delta,\Pi \vdash \xi : \Xi$.
%   \end{itemize}
% \end{itemize}
% \begin{itemize}
%   \item A type family $R_{\datalab{D}} : \Delta \to \Xi \to \univ$.
%   \item For each constructor $\ctorlab{c}$ of $\datalab{D}$, with data arguments $\pi$ and recursive arguments $\rho : ((\omega : \Omega) \to \datalab{D}\ \delta\ (\zeta \omega))$
%         a term $r_{\ctorlab{c}} : \Pi \Delta \Pi \Xi \Pi \pi R_{\datalab{D}} \to R_{\datalab{D}} \rho$.
% \end{itemize}


\subsection{Vectors are just certain lists}

We can use representations to share the underlying data when converting between
$\datalab{Vec}$ and $\datalab{List}$, so that the conversion is `compiled away'.
We can do this by representing the more indexed type as a refinement of the less
indexed type by an appropriate relation. For the case of $\datalab{Vec}$, we know
intuitively that
\[
  \datalab{Vec}\ T\ n \simeq \{ l : \datalab{List}\ T \mid \lab{length}\ l = n \}
\]
so we can start by choosing the type
\[
  \lab{List'}\ T\ n := \{ l : \datalab{List}\ T \mid \lab{length}\ l = n \}
\]
as the representation of $\datalab{Vec}\ T\ n$. We will take the subset $\{ x : A
\mid P\ x \}$ to mean a $\Sigma$-type $(x : A) \times P\ x$ where the right
component is irrelevant and erased at runtime. We are then tasked with providing
terms that correspond to the constructors of $\datalab{Vec}$ but for
$\lab{List'}$. These can be defined as
\begin{align*}
  & \lab{nil} : \lab{List'}\ T\ 0 \\
  & \lab{nil} = (\ctorlab{nil}, \ctorlab{refl}) \\[1em]
  & \lab{cons} : T \to \lab{List'}\ T\ n \to \lab{List'}\ T\ (\ctorlab{1+}\ n) \\
  & \lab{cons}\ x\ (\mathit{xs}, p) = (\ctorlab{cons}\ x\ \mathit{xs}, \lab{cong}\ (\ctorlab{1+})\ p)
\end{align*}
Next we need to define the eliminator for $\lab{List'}$, which should have the form
\begin{align*}
  & \lab{elim-List'} : (E : (n : \datalab{Nat}) \to \lab{List'}\ T\ n \to \lab{Type}) \\
  & \quad \to E\ 0\ \lab{nil} \\
  & \quad \to ((x : T) \to (n : \datalab{Nat}) \to (\mathit{xs} : \lab{List'}\ T\ n) \to E\ (\ctorlab{1+}\ n)\ (\lab{cons}\ x\ \mathit{xs})) \\
  & \quad \to (n : \datalab{Nat}) \to (v : \lab{List'}\ T\ n) \to E\ n\ v
\end{align*}
Some heavy lifting can be done by dependent pattern matching, where we refine the
length index and equality proof by matching on the underlying list. However we still need to
substitute the lemma $\lab{cong}\ (\ctorlab{1+})\ (\lab{\ctorlab{1+}-inj}\ p) = p$ in the recursive case.
\begin{align*}
  &\lab{elim-List'}\ P\ b\ r\ \ctorlab{0}\ (\ctorlab{nil}, \ctorlab{refl}) = b \\
  &\lab{elim-List'}\ P\ b\ r\ (\ctorlab{1+}\ m)\ (\ctorlab{cons}\ x\ \mathit{xs}, e) = \lab{subst}\
  \begin{aligned}[t]
  & (\lambda p .\ P\ (\ctorlab{1+}\ m)\ (\ctorlab{cons}\ x\ \mathit{xs}, p))\ \\
  & (\lab{\ctorlab{1+}-cong-id}\ e)\ (r\ x\ ({\mathit{xs}}, \lab{\ctorlab{1+}-inj}\ e))
  \end{aligned}
\end{align*}
Finally, we need to prove that the eliminator satisfies the expected computation
rules propositionally. These are
\begin{align*}
  & \lab{elim-List'-nil-id} : \lab{elim-List'}\ P\ b\ r\ \ctorlab 0\ (\ctorlab{nil}, \ctorlab{refl}) = b \\
  & \lab{elim-List'-cons-id} : \lab{elim-List'}\ P\ b\ r\ (\ctorlab{1+}\ m)\ (\ctorlab{cons}\ x\ \mathit{xs}, \lab{cong}\ (\ctorlab{1+})\ p) = r\ x\ (\mathit{xs}, p)
\end{align*}
which we leave as an exercise, though they are evident from the definition of
$\lab{elim-List'}$. This completes the definition of the representation of
$\datalab{Vec}$ as $\lab{List'}$, which would be written as
\[
  \Sreprvar{\datalab{Vec}\ T\ n}{\lab{List'}\ T\ n} \Sbody{
    \ctorlab{nil}\ \as&\ \lab{nil} \\
    \ctorlab{cons}\ \as&\ \lab{cons} \\
    \elimlab{Vec}\ \as&\ \lab{elim-List'} \\
     \by&\ \lab{elim-List'-nil-id}, \\ &\ \lab{elim-List'-cons-id}
  }
\]
Now the hard work is done; Every time we are working with a $v : \datalab{Vec}\
T\ n$, its real form will be $(l, p) : \lab{List'}\ T\ n$ at runtime, where $l$
is the underlying list and $p$ is the proof that the length of $l$ is $n$. Under
the assumption that the $\Sigma$-type's right component is irrelevant and erased
at runtime, every vector is simply a list at runtime, where the length proof has
been erased. In \ref{sec:implementation} we show how this erasure is achieved in
practice in \superfluid using quantitative type theory \cite{Atkey2018-pj}.

We can utilise this representation to convert between
$\datalab{Vec}$ and $\datalab{List}$ at zero runtime cost, by using the
$\repr{}$ and $\unrepr{}$ operators of the language. Specifically, we can define
the functions
\begin{align*}
  &\lab{forget-length} : \datalab{Vec}\ T\ n \to \datalab{List}\ T \\
  &\lab{forget-length}\ v = \letin{(l, \wildp)}{\repr{v}}{l} \\[1em]
  &\lab{recall-length} : (l : \datalab{List}\ T) \to \datalab{Vec}\ T\ (\lab{length}\ l) \\
  &\lab{recall-length}\ l = \unrepr{(l, \ctorlab{refl})}
\end{align*}
and it holds by reflexivity that $\lab{forget-length}$ is a left inverse of $\lab{recall-length}$.

\subsection{General reindexing}

The idea from the previous example can be generalised to any data type. Given
a simple data type $\datalab{F} : \univ$ and an algebra
\[
  \lab{$\alpha$} : \tilde{\datalab{F}}\ J \to J
\]
which computes some index $J$ from $\datalab{F}$, we can construct
a new type $\datalab{F}^\lab{$\alpha$} : J \to \univ$ which has the same constructors and arities,
but with with new data arguments for the indices and a call to $\lab{$\alpha$}$ in the
recursive arguments and return type. For this new type $\datalab{F}^\lab{$\alpha$}$, we can
define a representation as
\[
  \Sreprvar{\datalab{F}^{\lab{$\alpha$}}\ j}{\{ x : \datalab{F} \mid \lab{fold}\ \lab{$\alpha$}\ x = j \}} \Sbody{
    \ctorlab{c}_i\ \as&\ \ctorlab{c}_i \\
    \elimlab{F}\ \as&\ \elimlab{G} \\
    \by&\ \lab{elim-iso}
  }
\]
$\datalab{F}$,

If the interpretation of the algebraic ornament defined by $\alpha$ is the same
as the type $\datalab{G}\ (f\ i)$, then we can represent $\datalab{F}\ i$ as
$\{ g : \datalab{G}\ (f\ i) \mid \alpha\ g = \lab{im}\ f\ i \}$.


one can construct a data type $\datalab{G} : J \to \univ$ which is


\subsection{Transitivity}

Representations are transitive, so in the previous example, the `terminal'
representation of $\datalab{Vec}$ also depends on the representation of
$\datalab{List}$. It is possible to define a custom representation for
$\datalab{List}$ itself, for example a heap-backed array or a finger tree, and
$\datalab{Vec}$ would inherit this representation. However it will still be the
case that $\Repr{(\datalab{Vec}\ T\ n)} \equiv \datalab{List}\ T$, which means the
$\repr{}/\Repr{}$ operators only look at the immediate representation of a
term, not its terminal representation. Regardless, we can construct predicates that
find terms which satisfy a certain `eventual' representation. For example, given a
\datalab{Buf} type of byte buffers, we can consider the set of all types which are eventually
represented as a \datalab{Buf}:
\[
  \Sdata {ReprBuf}{(T : \univ)}{} \Sbody{
    \ctorlab{buf} &: \datalab{ReprBuf}\ \datalab{Buf} \\
    \ctorlab{from} &: \datalab{ReprBuf}\ (\Repr{T}) \to \datalab{ReprBuf}\ T \\
    \ctorlab{refined} &: \datalab{ReprBuf}\ T \to \datalab{ReprBuf}\ \{ t : T \mid  P\ t \}
  }
\]
Every such type comes with a projection function to the \datalab{Buf} type
\begin{align*}
  &\lab{as-buf} :_{\forall T . \datalab{ReprBuf}\ T} T \to \datalab{Buf} \\
  &\lab{as-buf}\ \ctorlab{buf}\ x = x \\
  &\lab{as-buf}\ (\ctorlab{from}\ t)\ x = \lab{as-buf}\ t\ (\repr{x}) \\
  &\lab{as-buf}\ (\ctorlab{refined}\ t)\ (x, \wildp) = \lab{as-buf}\ t\ x
\end{align*}
which eventually computes to the identity function after applying $\repr{}$ the
appropriate amount of times. Upon compilation, every type is converted to its
terminal representation, and all $\repr{}$ calls are erased, so the
$\lab{as-buf}$ function is effectively the identity function at
runtime. We do not guarrantee that an invocation of $\lab{as-buf}$ will
be entirely erased, but rather that any invocation will eventually produce the
identity function without having to perform a case analysis on its $T$ subject.


\subsection{Views on arrays}

In functional languages, lists are automatically represented as linked lists by
virtue of their inductive definition. This representation is particularly
suitable for the recursion pattern where the head of the list is processed
first, and the tail is processed recursively. However, it is not always the most
efficient for all operations. For example, accessing the $n$-th element of a
list requires $O(n)$ steps. In contrast, contiguous, homogenous arrays provide
$O(1)$ access to elements, but cannot be defined inductively in any nice way.
There is clearly a bijection between lists and arrays, which makes lists a good
candidate for a custom representation.

We assume access to a primitive type $\primlab{Array}\ T$ representing
contiguous heap arrays of elements of type $T$, along with primitives
\begin{align*}
  &\primlab{array} : (n : \datalab{Nat}) \to (\datalab{Fin}\ n \to T) \to \primlab{Array}\ T \\
  &\primlab{array-length} : \primlab{Array}\ T \to \datalab{Nat} \\
  &\primlab{array-index} : (a : \primlab{Array}\ T) \to \datalab{Fin}\ (\primlab{array-length}\ a) \to T \\[1em]
  &\primlab{array-elim} : (P : \primlab{Array}\ T \to \univ) \\
  & \to (m_1 : P\ \lab{array-nil}) \\
  & \to (m_2 : (x : T) \to (xs : \primlab{Array}\ T) \to P\ xs \to P\ (\lab{array-cons}\ x\ xs)) \\
  & \to (s : \primlab{Array}\ T) \to P\ s \,.
\end{align*}
using the auxilliary definitions
\begin{align*}
  &\lab{array-nil} : \primlab{Array}\ T \\
  &\lab{array-nil} = \primlab{array}\ \ctorlab 0\ (\lab{fin-zero-void}\ T) \\[1em]
  &\lab{array-cons} : T \to \primlab{Array}\ T \to \primlab{Array}\ T \\[-1em]
  &\lab{array-cons}\ x\ xs = \primlab{array}\ (\ctorlab{1+}\ \primlab{array-length}\ n) \Sbody{
     \ctorlab{0f} &\mapsto x \\
     \ctorlab{1f+}\ m &\mapsto \primlab{array-index}\ xs\ m
  }
\end{align*}

In addition, we require that the following equalities hold definitionally:
\begin{align*}
  \primlab{array-elim}\ P\ a\ b\ \lab{array-nil} &\equiv a : P\ \lab{array-nil} \\
  \primlab{array-elim}\ P\ a\ b\ (\lab{array-cons}\ x\ xs) &\equiv b\ x\ xs : P\ (\lab{array-cons}\ x\ xs)
\end{align*}

The symbols $\lab{array-nil}$, $\lab{array-cons}$, and $\lab{array-elim}$ along
with the above definitional equalities form an inductive interface, which can be
used as a representation for the $\datalab{List}$ data type:
\[
  \Sreprvar{\datalab{List}\ T}{\primlab{Array}\ T} \Sbody{
    \ctorlab{[]}\ &\as\ \lab{array-nil} \\
    \ctorlab{(}x\ctorlab{::}xs\ctorlab{)}\ &\as\ \lab{array-cons}\ x\ xs \\
    \elimlab{List}\ &\as\ \primlab{array-elim}
  }
\]

Now the indexing function for lists can be represented as the indexing function
for arrays, and the length function for lists can be represented as the length
function for arrays. To do this, it is required that some further definitional
equalities hold:
\begin{gather*}
  \primlab{array-elim}\ (\kappa\ \datalab{Nat})\ \ctorlab{0}\ (\kappa\ \kappa\ \ctorlab{1+}) \equiv \primlab{array-length} : \datalab{Array}\ T \to \datalab{Nat} \\[1em]
  \primlab{array-elim}\ (\lambda a .\ \datalab{Fin}\ (\primlab{array-length}\ a) \to T)\
  (\lab{fin-zero-void}\ T)\ (\kappa\ .\ \elimlab{Fin}\ T)\\
  \equiv \ \primlab{array-index} : \primlab{Array}\ T \to \datalab{Fin}\ (\primlab{array-length}\ a) \to T
\end{gather*}
Then we can set
\begin{align*}
  \repr{} \lab{index}\ &\as\ \primlab{array-index} \\
  \repr{} \lab{length}\ &\as\ \primlab{array-length} \,.
\end{align*}
This works because the definition of $\lab{length}$ elaborates to the algebra above
which is equated to $\primlab{array-length}$, satisfying the definitional equality requirements.


\subsection{Reindexing and forgetful maps for free}

Let us assume that in our language, for some data type
\[
  \Sdata{D}{\Delta}{\Xi}
\]
we have access to an internal description $\lab{desc}_{\datalab{D}} : \Delta \to \datalab{Desc}\ \Xi$, along
with an interpretation function
\[
  \lab{$\llbracket \_ \rrbracket$} : \datalab{Desc}\ \Xi \to (\Xi \to \univ) \to (\Xi \to \univ)
\]
and a fixpoint operator
\[
  \Sdata{$\mu$}{(D : \datalab{Desc}\ \Xi ) \  (\xi : \Xi)}{} \Sbody {
    \ctorlab{fix} : \llbracket D \rrbracket \ (\datalab{$\mu$}\ D) \ \xi \to \datalab{$\mu$}\ D \ \xi
  }
\]
such that $\datalab{$\mu$} \ (\lab{desc}_{\datalab{D}} \ \delta) \simeq
\datalab{D} \ \delta$ is a strong bijection.
Then, we might choose to define the representation of $D$ as $\datalab{$\mu$} \
(\lab{desc}_{\datalab{D}} \ \delta)$; in this way, we represent the `primitive'
datatype $\datalab{D}$ as the interpretation of its code
$\lab{desc}_{\datalab{D}}$.

Now consider that we have an indexed datatype
\[
  \Sdata{C}{\Delta}{(\xi : \Xi) \to \Phi [\xi]}
\]
which is a refined version of $\datalab D$. If we can construct an algebra
\[
  \lab{phi}\ \delta : \llbracket \lab{desc}_{\datalab{D}} \rrbracket \ \Phi \ \xi \to \Phi \ \xi
\]
that computes the index $\Phi$, then we can form the description
\[
  \lab{alg-orn}\ (\lab{phi}\ \delta) : \datalab{Desc}\ (\datalab{$\Sigma$} \ \Xi \ \Phi)
\]
which is the ornament induced by the algebra $\lab{phi}$. If a strong bijection
can be established between $\datalab{$\mu$} \ (\lab{alg-orn}\ (\lab{phi} \ \delta))$ and
$\datalab{C} \ \delta$, then the former can be used as the representation of the
latter. Finally, this allows us to define a zero-cost forgetful conversion function
from $\datalab{C}$ and $\datalab{D}$, which is erased at compile-time, as
\begin{align}
  &\lab{forget-}\Phi : \datalab C\ \delta\ \xi\ \phi \to \datalab D\ \delta\ \xi \label{eq:forget} \\
  &\lab{forget-}\Phi\ c\ d = \unrepr{(\lab{alg-orn-forget}\ (\repr c))}
\end{align}

\begin{example}
  Let $\Delta = (T : \univ)$, $\Xi = \cdot$ and $\Phi = (n : \datalab{Nat})$.
  Let $\datalab D\ T = \datalab{List}\ T$, and $\datalab C\ T\ n = \datalab{Vec}\ T\ n$.
  Then let $\lab{desc}_{\datalab{List}}$ be the description for lists
  \[
    \ctorlab{$\sigma$} \ [\ctorlab{nil}, \ctorlab{cons}] \Sbody{
      \ctorlab{nil} &\mapsto \ctorlab{end} \\
      \ctorlab{cons} &\mapsto \ctorlab{$\sigma$}\ T\ (\lambda \wildp .\  \ctorlab{node$\times$}\ \ctorlab{end})
    }
  \]
  and construct the algebra $\lab{phi} = \lab{length-alg}$ mirroring the length function $\lab{length}
  : \datalab{List}\ T \to \datalab{Nat}$. It is easy to construct a strong
  bijection $\datalab{$\mu$} \ (\lab{alg-orn}\ (\lab{length-alg} \ T))\ n \simeq
  \datalab{Vec}\ T\ n$, so we can represent the latter as the former, and
  define a zero-cost function to forget the length of a vector as $\lab{forget-}\datalab{Nat}$.
\end{example}


\subsection{Binary data}
