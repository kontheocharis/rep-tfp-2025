\section{A tour of data representations}\label{sec:examples}

A common optimisation done by programming languages with dependent types such as
Idris 2, Agda, Rocq and Lean is to represent natural numbers as GMP-style big integers. The
definition of natural numbers looks like
\begin{equation}\label{eq:nat-def}
  \Sdata{Nat}{}{} \Sbody{
    \ctorlab{zero} & : \datalab{Nat} \\
    \ctorlab{succ} & : \datalab{Nat} \to \datalab{Nat}
  }
\end{equation}
and generates a case analysis principle $\caselab{Nat}$ of type
\[
   (P : \datalab{Nat} \to \univ)
  \to P\ \ctorlab{zero} \to ((n : \datalab{Nat}) \to P\ (\ctorlab{succ}\ n))
  \to (s : \datalab{Nat}) \to P\ s \,.
\]
This is a special case of the induction principle $\elimlab{Nat}$, where the inductive hypotheses
are given in each method.
Without further intervention, $\datalab{Nat}$ is represented in unary,
where each digit becomes a heap cell at runtime. This is
inefficient for many basic operations on natural numbers, especially
since computers are well-equipped to deal with numbers natively, so
many real-world implementations will treat \datalab{Nat} specially, swapping the
default inductive type representation with one based on GMP integers. This is
done with the replacements
\begin{align*} \label{eq:nat-repr-untyped}
  &|\ctorlab{zero}| = \texttt{0}, \\
  &|\ctorlab{succ}| = \texttt{\textbackslash x => x + 1}, \\
  &|\caselab{Nat}\ P\ m_{\ctorlab{zero}}\ m_{\ctorlab{succ}}\ s| \\
  & \qquad =\ \texttt{let s = $|s|$ in if s == 0 } \\
  &\qquad \qquad \texttt{then $|m_{\ctorlab{zero}}|$} \\
  &\qquad \qquad \texttt{else $|m_{\ctorlab{succ}}|$\ (s - 1)},
\end{align*}
where $|\cdot|$ denotes a source translation into a compilation target
language with appropriate big unsigned integer primitives. This is the `Nat-hack'.\footnote{
Idris 2 will in fact look for any \datalab{Nat}-like types and apply this optimisation.
A similar optimisation is also done with list-like and boolean-like types because
they have a canonical representation in the target runtime, Chez Scheme.}

In addition to the constructors and case analysis, the compiler might define
translations for commonly used definitions which have a more efficient
counterpart in the target, such as addition, multiplication,
etc. The recursively-defined functions are well-suited to proofs and reasoning,
while the primitives are more efficient for computation. This way, the surface
program can take advantage of the structural properties of induction,
while still benefiting from an efficient representation at runtime.

However, this approach only works for the data types which the
compiler recognises as `special'. Particularly in the presence of dependent
types, other data types might end up being equivalent to \datalab{Nat} or
another `nicely-representable' type, but in a non-trivial way that the compiler
cannot recognise. It is also hard to know when such optimisations will fire
and performance can become brittle upon refactoring.
Hence, one of our goals is to extend this optimisation to work
for any data type. To achieve this, our framework requires that
representations are fully typed in a way that ensures the behaviour of the
representation of a data type matches the behaviour of the data type itself.

\subsection{The well-typed Nat-hack}

A representation definition looks like
\[
  \Sreprvar{\datalab{Nat}}{\primlab{UBig}} \Sbody{
    \ctorlab{zero}\ \as&\ \primlab{0} \\
    \ctorlab{succ}\ n\ \as&\ \primlab{1}\,\primlab{$+$}\,n \\
    \elimlab{Nat}\ \as&\ \primlab{ubig-elim}\\ \by&\ \primlab{ubig-elim-zero-id}, \\ &\ \primlab{ubig-elim-add-one-id}
  }
\]
The inductive type $\datalab{Nat}$ is represented as the type $\primlab{UBig}$ of big
unsigned integers, with translations for the constructors
$\ctorlab{zero}$ and $\ctorlab{succ}$, and the eliminator $\elimlab{Nat}$ (now with the inductive hypotheses).
Additionally, the eliminator must satisfy the computation rules of the
$\datalab{Nat}$ eliminator, which are postulated as propositional equalities.
This representation is valid in a context containing the symbols
\begin{gather*}
  \primlab{0}, \primlab{1} : \primlab{UBig} \qquad
  \primlab{$+$} : \primlab{UBig} \to \primlab{UBig} \to \primlab{UBig} \\
  \begin{aligned}
  \primlab{ubig-elim} :\ &(P : \primlab{UBig} \to \univ) \to P\ \primlab{0}
  \to ((n : \primlab{UBig}) \to \overline{P\ n} \to P\ (\primlab{1}\,\primlab{$+$}\,n)) \\
   &\to (s : \primlab{UBig}) \to P\ s
  \end{aligned}
\end{gather*}
and propositional equalities
\begin{align*}
  &\primlab{ubig-elim-zero-id} : _{\forall Pbr}  \primlab{ubig-elim}\ P\ b\ r\ \primlab{0} = b \\
  &\primlab{ubig-elim-add-one-id} : _{\forall Pbrn}  \primlab{ubig-elim}\ P\ b\ r\ (\primlab{1}\,\primlab{$+$}\,n)
  = r\ n\ (\lambda \wildp. \ \primlab{ubig-elim}\ P\ b\ r\ n)  \,.
\end{align*}
For the remainder of the paper we will work with eliminators rather than case
analysis but the approach can be specialised to the latter if the language has
general recursion. Assuming call-by-value semantics, inductive hypotheses are labelled
$\overline{P}$ which denotes lazy values, that is, functions $\datalab{Unit} \to
P$.
% Representations can also be defined for functions on $\datalab{Nat}$, such as
% addition, multiplication, and other numeric operations, in terms of
% $\primlab{UBig}$ primitives.
% \begin{align*}
%   \Sreprvar{\datalab{add}}{\primlab{$+$}\ \by\ \primlab{$+$-id}}
%   \qquad \Sreprvar{\datalab{mul}}{\primlab{$\times$}\ \by\ \primlab{$\times$-id}}
% \end{align*}
% These are replaced during a translation process
% back to $\lambdadata$, like rewriting rules \cite{Cockx2021-pw}, given
% that we have the appropriate lemmas to justify them in the context.

The compiler knows how to perform pattern matching on $\datalab{Nat}$, and
produces invocations of $\elimlab{Nat}$ as a result. On the other hand, it does
not know how to pattern match on $\primlab{UBig}$. With this representation, we
get the best of both worlds: \datalab{Nat} is used for pattern matching, but is
then replaced with $\primlab{UBig}$ during compilation, generating more efficient code. We
expect that the underlying implementation of $\primlab{UBig}$ indeed satisfies
these postulated properties, which is a separate concern from the correctness of
the representation itself. However, such postulates are only needed when the
representation target is a primitive; the next examples use defined types as
targets, so that the coherence of the target eliminator follows from the
coherence of other eliminators used in its implementation.

\subsection{Vectors are just certain lists}

In addition to representing inductive types as primitives, we can use
representations to share the underlying data when converting between indexed
views of the same data. For example, we can define a representation of
$\datalab{Vec}$ in terms of $\datalab{List}$, so that the conversion from one to
the other is `compiled away'. We can do this by representing the indexed type as
a refinement of the unindexed type by an appropriate relation. For the case of
$\datalab{Vec}$, we know intuitively that
\[
  \datalab{Vec}\ T\ n \simeq \{ l : \datalab{List}\ T \mid \lab{length}\ l = n \}
\]
which we shorthand as $\lab{List'}\ T\ n := \{ l : \datalab{List}\ T \mid
\lab{length}\ l = n \}$. We will take the subset $\{ x : A \mid P\ x \}$ to mean
a $\Sigma$-type $(x : A) \times P\ x$ where the right component is irrelevant
and erased at runtime.
We can thus choose $\lab{List'}\ T\ n$ as the representation of
$\datalab{Vec}\ T\ n$. We are then tasked with providing terms that correspond to
the constructors of $\datalab{Vec}$ but for $\lab{List'}$. These can be defined
as
\[
  \begin{aligned}
  & \lab{nil} : \lab{List'}\ T\ \ctorlab{zero} \\
  & \lab{nil} = (\ctorlab{nil}, \ctorlab{refl})
  \end{aligned}\qquad
  \begin{aligned}
  & \lab{cons} : T \to \lab{List'}\ T\ n \to \lab{List'}\ T\ (\ctorlab{succ}\ n) \\
  & \lab{cons}\ x\ (\mathit{xs}, p) = (\ctorlab{cons}\ x\ \mathit{xs}, \lab{cong}\ (\ctorlab{succ})\ p)
  \end{aligned}
\]
Next we need to define the eliminator for $\lab{List'}$, which should have the form
\begin{align*}
  & \lab{elim-List'} : (E : (n : \datalab{Nat}) \to \lab{List'}\ T\ n \to \lab{Type}) \\
  & \quad \to E\ \ctorlab{zero}\ \lab{nil} \\
  & \quad \to ((x : T) \to (n : \datalab{Nat}) \to (\mathit{xs} : \lab{List'}\ T\ n) \to \overline{E\ n\ xs} \to E\ (\ctorlab{succ}\ n)\ (\lab{cons}\ x\ \mathit{xs})) \\
  & \quad \to (n : \datalab{Nat}) \to (v : \lab{List'}\ T\ n) \to E\ n\ v
\end{align*}
Dependent pattern matching does a lot of the heavy lifting by refining the
length index and equality proof by matching on the underlying list. However we still need to
substitute the lemma $\lab{cong}\ (\ctorlab{succ})\ (\lab{\ctorlab{succ}-inj}\ p) = p$ in the recursive case.
\begin{align*}
  &\lab{elim-List'}\ P\ b\ r\ \ctorlab{zero}\ (\ctorlab{nil}, \ctorlab{refl}) = b \\
  &\lab{elim-List'}\ P\ b\ r\ (\ctorlab{succ}\ m)\ (\ctorlab{cons}\ x\ \mathit{xs}, e) = \lab{subst}\
  \begin{aligned}[t]
  &(\lambda p .\ P\ (\ctorlab{succ}\ m)\ (\ctorlab{cons}\ x\ \mathit{xs}, p))\ \\
  &\hspace{-3em} (\lab{\ctorlab{succ}-cong-id}\ e)\ (r\ x\ ({\mathit{xs}}, \lab{\ctorlab{succ}-inj}\ e) \\
  &\hspace{-3em}  (\lambda \wildp . \ \lab{elim-List'}\ P\ b\ r\ m\ ({\mathit{xs}}, \lab{\ctorlab{succ}-inj}\ e)))
  \end{aligned}
\end{align*}
Finally, we need to prove that the eliminator satisfies the expected computation
rules propositionally. These are
\begin{align*}
  \lab{elim-List'-nil-id} : &\ \lab{elim-List'}\ P\ b\ r\ \ctorlab{zero}\ (\ctorlab{nil}, \ctorlab{refl}) = b \\[0.5em]
  \lab{elim-List'-cons-id} :&\  \lab{elim-List'}\ P\ b\ r\ (\ctorlab{succ}\ m)\ (\ctorlab{cons}\ x\ \mathit{xs}, \lab{cong}\ \ctorlab{succ}\ p) \\
  & = r\ x\ (\mathit{xs}, p)\ (\lambda \wildp .\ \lab{elim-List'}\ P\ b\ r\ m\ (\mathit{xs}, p))
\end{align*}
The first holds definitionally, and the second requires a small amount of
equality reasoning if the theory does not have definitional uniqueness of
identity.
This completes the definition of the representation of
$\datalab{Vec}$ as $\lab{List'}$, which would be written as
\[
  \Sreprvar{\datalab{Vec}\ T\ n}{\lab{List'}\ T\ n} \Sbody{
    \ctorlab{nil}\ \as&\ \lab{nil} \\
    \ctorlab{cons}\ \as&\ \lab{cons} \\
    \elimlab{Vec}\ \as&\ \lab{elim-List'} \\
     \by&\ \lab{elim-List'-nil-id}, \\ &\ \lab{elim-List'-cons-id}
  }
\]
Now the hard work is done; Every time we are working with a $v : \datalab{Vec}\
T\ n$, its form will be $(l, p)$ at runtime, where $l$ is the underlying list
and $p$ is the proof that the length of $l$ is $n$. Under the assumption that
the $\Sigma$-type's right component is irrelevant and erased at runtime, every
vector is simply a list at runtime, where the length proof has been erased. In
practice, this erasure is achieved in \superfluid using quantitative type theory
\cite{Atkey2018-pj}. In \cref{sub:irr} we show how to formally identify
computationally irrelevant conversion functions.

We can utilise this representation to convert between $\datalab{Vec}$ and
$\datalab{List}$ at zero runtime cost, by using the $\repr{}$ and $\unrepr{}$
operators of the language (defined in \cref{sec:type-system}). Specifically, we
can define the functions
\begin{align*}
  &\lab{forget-length} : \datalab{Vec}\ T\ n \to \datalab{List}\ T \\
  &\lab{forget-length}\ v = \letin{(l, \wildp)}{\repr{v}}{l} \\[1em]
  &\lab{remember-length} : (l : \datalab{List}\ T) \to \datalab{Vec}\ T\ (\lab{length}\ l) \\
  &\lab{remember-length}\ l = \unrepr{(l, \ctorlab{refl})}
\end{align*}
and in \cref{sub:irr} we will show that it holds by reflexivity that
such functions are inverses of one another.

\subsection{General reindexing}

The idea from the previous example can be generalised to any data type. In
general, suppose that we have two inductive families
\[
 \datalab{F} : P \to \univ \qquad \datalab{G} : (p : P) \to X\ p \to \univ
\]
for some index family $X : P \to \univ$. If we hope to represent $\datalab{G}$
as some refinement of $\datalab{F}$ then we must be able to provide a way to
compute $\datalab{G}$'s extra indices $X$ from $\datalab{F}$, like we computed
$\datalab{Vec}$'s extra $\datalab{Nat}$ index from $\datalab{List}$ with
$\lab{length}$ in the previous example. This means that we need to provide a
function $\lab{comp} : {\forall p.}\ \datalab{F}\ p \to X\ p$ which can then be
used to form the family
\[
  \datalab{F}^{\lab{comp}}\ p\ x :=  \{ f : \datalab{F}\ p \mid \lab{comp}\ f = x \} .
\]
If $\datalab{G}$ is `equivalent' to the algebraic ornament of $\datalab{F}$ by
the algebra defining $\lab{comp}$ (given by an isomorphism between the
underlying polynomial functors), then it is also equivalent to the $\Sigma$-type
above. The `recomputation lemma' of algebraic ornaments \cite{Dagand2012-aw}
then arises from its projections. Our system allows us to \emph{set} the
representation of $\datalab{G}$ as $\datalab{F}^{\lab{comp}}$, so that the
forgetful map from $\datalab{G}$ to $\datalab{F}$ is the identity at runtime.

\subsection{Zero-copy deserialisation}

The machinery of representations can be used to implement zero-copy deserialisation
of data formats into inductive types. Here we sketch how this could work. Consider the following
record for a player in a game:
\[
  \Sdata{Player}{}{} \Sbody{
    \ctorlab{player} : &\ (\fieldlab{position} : \datalab{Position}) \\
    \to &\ (\fieldlab{direction} : \datalab{Direction}) \\
    \to &\ (\fieldlab{items} : \datalab{Fin}\ \lab{MAX\_INVENTORY}) \\
    \to &\ (\fieldlab{inventory} : \datalab{Inventory}\ (\lab{fin-to-nat}\ \fieldlab{items})) \to \datalab{Player}
  }
\]
We can use the $\datalab{Fin}$ type to maintain the invariant that the inventory
has a maximum size. Additionally, we can index the $\datalab{Inventory}$ type by
the number of items it contains, which might be defined similarly to $\datalab{Vec}$:
\[
  \Sdata{Inventory}{(n : \datalab{Nat})}{} \Sbody{
    \ctorlab{empty} : &\ \datalab{Inventory}\ \ctorlab 0 \\
    \ctorlab{add} : &\ \datalab{Item} \to \datalab{Inventory}\ n \to  \datalab{Inventory}\ (\ctorlab{1+}\ n)
  }
\]
We can use the full power of inductive families to model the domain of our
problem in the way that is most convenient for us. If we were writing this in a
lower-level language, we might choose to use the serialised format directly when
manipulating the data, relying on the appropriate pointer arithmetic to access
the fields of the serialised data, to avoid copying overhead. Representations
allow us to do this while still being able to work with the high-level inductive
type.

We can define a representation for $\datalab{Player}$
as a pair of a byte buffer and a proof that the byte buffer contents correspond to
a player record. Similarly, we can define a representation for $\datalab{Inventory}$
as a pair of a byte buffer and a proof that the byte buffer contents correspond to
an inventory record of a certain size. By the implementation of the eliminator
in the representation, the projection $\fieldlab{inventory} : (p :
\datalab{Player}) \to \datalab{Inventory}\ p.\fieldlab{items}$ is compiled into
some code to slice into the inventory part of the player's byte buffer. We
assume that the standard library already represents $\datalab{Fin}$ in the same
way as $\datalab{Nat}$, so that reading the $\fieldlab{items}$ field is a
constant-time operation (we do not need to build a unary numeral). We can thus
define the representation
of $\datalab{Player}$ as
\[
  \Sreprvar{\datalab{Player}}{\{ \datalab{Buf} \mid \datalab{IsPlayer} \}} \Sbody{
    \ctorlab{player}\ \as&\ \lab{buf-is-player} \\
    \elimlab{Player}\ \as&\ \lab{elim-buf-is-player} \\
                      \by&\ \lab{elim-buf-is-player-id}
  }
\]
with an appropriate definition of $\datalab{IsPlayer}$ which refines a byte
buffer. The refinement would have to match the expected structure of the byte
buffer, so that all the required fields can be extracted. Allais
\cite{Allais2023-zq} explores how data descriptions that index into a flat
buffer can be defined.


\subsection{Transitivity}\label{sub:transitivity}

Representations are transitive, so in the previous example, the eventual
representation of $\datalab{Vec}$ at runtime is determined by the representation
of $\datalab{List}$. It is possible to define a custom representation for
$\datalab{List}$ itself, for example a heap-backed array or a finger tree, and
$\datalab{Vec}$ would inherit this representation. However it will still be the
case that $\Repr{(\datalab{Vec}\ T\ n)} \equiv \datalab{List}\ T$, which means
the $\repr{}/\Repr{}$ operators only look at the immediate defined
representation of a term. Regardless, we can construct predicates that find
types which satisfy a certain eventual representation. For example, given a
\datalab{Buf} type of byte buffers, we can consider the set of all types which
are eventually
represented as a \datalab{Buf}:
\[
  \Sdata {ReprBuf}{(T : \univ)}{} \Sbody{
    \ctorlab{buf} &: \datalab{ReprBuf}\ \datalab{Buf} \\
    \ctorlab{from} &: \datalab{ReprBuf}\ (\Repr{T}) \to \datalab{ReprBuf}\ T \\
    \ctorlab{refined} &: \datalab{ReprBuf}\ T \to \datalab{ReprBuf}\ \{ t : T \mid  P\ t \}
  }
\]
Every such type comes with a projection function to the \datalab{Buf} type
\begin{align*}
  &\lab{as-buf} :\{r : \datalab{ReprBuf}\ T\} \to T \to \datalab{Buf} \\
  &\lab{as-buf}\ \{r = \ctorlab{buf}\}\ x = x \\
  &\lab{as-buf}\ \{r = \ctorlab{from}\ t\}\ x = \lab{as-buf}\ t\ (\repr{x}) \\
  &\lab{as-buf}\ \{r = \ctorlab{refined}\ t\}\ (x, \wildp) = \lab{as-buf}\ t\ x
\end{align*}
which eventually computes to the identity function after applying $\repr{}$ the
appropriate amount of times. Upon compilation, every type is converted to its
eventual representation, and all $\repr{}$ calls are erased, so the
$\lab{as-buf}$ function becomes the identity function at
runtime, given that the $r$ argument is known at compile-time and monomorphised.
